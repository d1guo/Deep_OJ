diff --git a/CHANGELOG.md b/CHANGELOG.md
index fb22c34..95b9210 100644
--- a/CHANGELOG.md
+++ b/CHANGELOG.md
@@ -12,9 +12,9 @@
 - 默认禁用宿主机 checker，需显式开启。
 
 ### 可靠性
-- 结果流写入加入可配置重试与退避，保证 SetNX+XAdd 一致性。
-- ACK 仅在 DB 同步成功后确认，失败消息保持 pending 可重放。
-- Scheduler 无可用 Worker 时立即回滚任务到 pending。
+- Worker 执行结果使用 DB fencing finalize + XACK，保证“先落库后确认”。
+- reclaim 链路基于 `XAUTOCLAIM` 与 DB 租约 CAS，支持崩溃恢复。
+- API outbox 投递支持退避重试，降低短暂故障下的丢单风险。
 - 缓存反序列化失败时回退到 DB。
 
 ### API 行为
@@ -24,9 +24,9 @@
 - 题目包上传限制大小，采用流式 hash 避免 OOM。
 
 ### Worker & Scheduler
-- `ETCD_ENDPOINTS` 支持逗号分隔；新增 dial 超时与 lease TTL 配置。
+- Scheduler 收敛为控制面（发现与指标），不再承担派单数据面。
+- Worker 仅保留 Streams 消费执行链路。
 - 指标端口与采样频率可配置。
-- 队列超时、watchdog、slow-path 周期、重试 TTL 可配置。
 - checker/cleanup 超时可配置。
 
 ### Sandbox/Core
diff --git a/Dockerfile b/Dockerfile
index f7a516f..67155fc 100644
--- a/Dockerfile
+++ b/Dockerfile
@@ -98,6 +98,6 @@ RUN mkdir -p /data/testcases && \
     chown root:root /app/config.yaml && \
     chmod 644 /app/config.yaml
 
-EXPOSE 8080 18080 50051 50052
+EXPOSE 8080 18080
 
 CMD ["/bin/bash"]
diff --git a/FAILURE_MODES.md b/FAILURE_MODES.md
index f9090b7..387e205 100644
--- a/FAILURE_MODES.md
+++ b/FAILURE_MODES.md
@@ -1,45 +1,45 @@
-# Deep-OJ Failure Modes（Baseline）
+# Deep-OJ Failure Modes（Streams-only）
 
-本文件先记录 A1 阶段已确认的关键故障模式，后续按任务队列持续扩展。
+本文件记录当前 Streams-only 数据面（API outbox -> Redis Stream -> Worker -> DB finalize -> XACK -> reclaim）的关键故障模式。
 
-## 1. Redis 不可用（提交路径）
+## 1. Redis 不可用（提交或出队阶段）
 
-- 触发点：`src/go/internal/api/handler.go` 提交阶段 `LPush queue:pending` 失败
-- 现象：`/submit` 返回 `QUEUE_ERROR`
-- 定位：API 日志关键字 `Redis 推送错误`
-- 恢复：恢复 Redis 后重提；后续建议 Outbox（D1）避免丢单
+- 触发点：API 写 outbox/入流或 Worker `XREADGROUP` 失败
+- 现象：提交后状态长期 pending，或 worker 日志持续报 Redis 错误
+- 定位：`docker compose logs --tail=200 api worker | rg -i "redis|xreadgroup|xadd"`
+- 恢复：恢复 Redis 连通后重试；outbox 会继续补投未投递事件
 
-## 2. Scheduler 无可用 Worker
+## 2. Worker 无法 claim 任务
 
-- 触发点：`src/go/cmd/scheduler/main.go` `selectWorker` 失败
-- 现象：任务被 `RequeueTask(queue:processing -> queue:pending)` 循环重试
-- 定位：日志关键字 `No workers available`
-- 恢复：恢复 worker 可达（`WORKER_ADDR/WORKER_ADDRS`），观察 pending 回落
+- 触发点：`ClaimSubmissionForRun` 返回拒绝或 DB 异常
+- 现象：消息被读到但无法进入执行，状态不推进
+- 定位：worker 日志关键字 `DB claim`、`db_claim_reject`
+- 恢复：检查 DB 可用性与 submissions 行状态，恢复后由后续消费/reclaim 继续推进
 
-## 3. Worker 执行完成但结果未入 Stream
+## 3. Worker 执行完成但 finalize 失败
 
-- 触发点：`src/go/internal/worker/judge.go` `XADD stream:results` 连续失败
-- 现象：状态卡在 processing
-- 定位：日志关键字 `Redis stream add failed`
-- 恢复：修复 Redis 后重试；当前会删除 `result:{job_id}` 以避免脏缓存
+- 触发点：`FinalizeSubmissionWithFence` 失败
+- 现象：日志出现 `db_finalize_error`，消息未成功收敛
+- 定位：`docker compose logs --tail=300 worker | rg -i "db_finalize"`
+- 恢复：优先修复 DB 问题；消息仍在流中，后续可由 reclaim 继续处理
 
-## 4. ACK 消费失败
+## 4. XACK 失败导致 PEL 堆积
 
-- 触发点：`src/go/internal/scheduler/ack_listener.go` `processResult` 失败
-- 现象：消息留在 pending entries list，不 `XACK`
-- 定位：日志关键字 `Result processing failed, keep pending`
-- 恢复：修复 DB/JSON 解析问题后，listener 重启可继续消费 pending
+- 触发点：执行或拒绝分支中 `XACK` 失败
+- 现象：`XPENDING` 持续增长，lag 上升
+- 定位：`redis-cli XPENDING deepoj:jobs deepoj:workers` 与 worker 日志 `xack_error`
+- 恢复：恢复 Redis 后，worker reclaim loop 会继续回收并确认
 
-## 5. Worker 崩溃导致 processing 残留
+## 5. Worker 崩溃或租约丢失
 
-- 触发点：任务在 `queue:processing`，worker 心跳丢失
-- 现象：任务长期不完成
-- 定位：`task:processing:zset` 超时项、Watchdog 日志 `Worker dead, requeuing task`
-- 恢复：Watchdog/SlowPath 会回收并重试；需检查是否触发重复执行风险
+- 触发点：worker 进程崩溃、心跳中断、lease owner 变化
+- 现象：任务进入 pending entries list，原执行中断
+- 定位：日志关键字 `lease_lost`、`reclaim_claimed`，以及 `XAUTOCLAIM` 相关指标
+- 恢复：重启 worker；reclaim 机制按 `min-idle-ms` 规则重新接管
 
-## 6. DB 已 done，重复结果回写
+## 6. 重复执行与陈旧结果
 
-- 触发点：旧结果重复投递
-- 现象：当前 SQL 条件 `state != 'done'` 下被忽略
-- 定位：日志关键字 `Duplicate result ignored (already done)`
-- 恢复：无需人工处理；后续需升级到 attempt_id fencing（C5）
+- 触发点：网络抖动或重启导致同一 job 被多次尝试
+- 现象：旧 attempt finalize 被拒绝（stale/already_finished）
+- 定位：日志关键字 `db_finalize_rejected`、`db_finalize_stale`
+- 恢复：无需人工干预；fencing 机制保证最终态单写入
diff --git a/README.md b/README.md
index 400071d..0b468fb 100644
--- a/README.md
+++ b/README.md
@@ -2,7 +2,7 @@
 
 Deep-OJ 是一个面向在线评测场景的分布式判题系统，采用 Go + C++ 混合实现。
 
-当前代码状态（以仓库现状为准，2026-02-18）已经从“Scheduler 主导数据面”演进到“Worker 直连 Streams 数据面”。
+当前代码状态（以仓库现状为准，2026-02-21）为 Streams-only 数据面。
 
 ## 架构变化概览
 
@@ -11,21 +11,18 @@ Deep-OJ 是一个面向在线评测场景的分布式判题系统，采用 Go +
 | 维度 | 旧链路（历史） | 新链路（当前主线） |
 | --- | --- | --- |
 | 入队 | API 直接写 Redis List | API 写 DB（`submissions + outbox_events`），由 outbox dispatcher 投递 Redis Stream |
-| 消费 | Scheduler `BRPopLPush` 后 gRPC 推给 Worker | Worker 自己 `XREADGROUP` 消费 `deepoj:jobs` |
+| 消费 | Scheduler 拉取队列后推送给 Worker | Worker 自己 `XREADGROUP` 消费 `deepoj:jobs` |
 | 重试/回收 | Scheduler + 队列回推 | Worker `XAUTOCLAIM` reclaim + DB reclaim CAS |
 | 最终落库 | Scheduler ACK Listener 回写 DB | Worker 侧按 attempt fencing 写 DB，成功后才 `XACK` |
 | 真相源 | Redis + DB 混合语义 | DB 状态机为唯一真相源 |
 
 ## Scheduler 现在的作用
 
-Scheduler 目前不再是主数据面的唯一入口，更接近控制面和兼容层：
+Scheduler 已收敛为控制面：
 
-1. 继续做 Worker 发现相关能力（Etcd watch、活性视角、指标）。
-2. 保留 legacy 分发链路（`queue:pending -> queue:processing -> gRPC Dispatch`），用于兼容旧路径或回退。
-3. 保留 ACK Listener / Watchdog / Slow Path 等守护逻辑，主要服务于 legacy 语义和运维兜底。
-4. 提供 Scheduler 侧 metrics（队列深度、活跃 worker 等）。
-
-结论：主链路已经不再依赖 Scheduler 来“拉任务并分发”，但仓库里仍保留了可运行的 legacy 逻辑，属于过渡期双轨并存。
+1. 维护 Worker 发现视图与基础指标（活跃 worker 数等）。
+2. 暴露调度器 metrics 端点，供观测系统抓取。
+3. 不再执行任何派单/回写/重试/慢路径数据面循环。
 
 ## 当前主链路架构
 
@@ -39,10 +36,7 @@ flowchart LR
     Worker -->|claim_and_fenced_finalize| PG
     Worker -->|run_judge| Judge[Cpp Sandbox]
     Worker -->|cache_result| Redis[(Redis)]
-
-    Worker -->|xadd_results_legacy| Results[(Redis Stream stream_results)]
-    Scheduler[Scheduler] -->|xreadgroup_legacy| Results
-    Scheduler -->|watch_and_metrics| Etcd[(Etcd)]
+    Scheduler[Scheduler] -->|metrics_only| Monitor[Observability]
 ```
 
 ## 核心不变量
@@ -61,10 +55,9 @@ flowchart LR
 | Outbox Dispatcher (`src/go/internal/api/outbox_dispatcher.go`) | 扫描 outbox，投递 `deepoj:jobs` |
 | Worker (`src/go/cmd/worker`) | 消费 Streams、执行判题、租约心跳、reclaim、fenced finalize |
 | Judge Engine (`src/core/worker`) | 编译/运行/资源隔离/结果输出 |
-| Scheduler (`src/go/cmd/scheduler`) | 控制面能力 + legacy 兼容逻辑 |
+| Scheduler (`src/go/cmd/scheduler`) | 控制面能力（发现与指标） |
 | PostgreSQL | 提交状态机、attempt/lease/fencing、outbox 记录 |
 | Redis | Streams、payload 缓存、结果缓存 |
-| Etcd | Worker 注册发现 |
 
 ## 快速开始
 
@@ -85,7 +78,6 @@ export REDIS_PASSWORD='deepoj_redis_change_me'
 export POSTGRES_PASSWORD='deepoj_pg_password_change_me'
 export MINIO_ROOT_USER='deepoj_minio_user'
 export MINIO_ROOT_PASSWORD='deepoj_minio_password_change_me'
-export WORKER_AUTH_TOKEN='deepoj_worker_token_change_me'
 
 docker compose up -d --build
 docker compose ps
diff --git a/RUNBOOK.md b/RUNBOOK.md
index dac3378..1a58118 100644
--- a/RUNBOOK.md
+++ b/RUNBOOK.md
@@ -13,7 +13,6 @@ export REDIS_PASSWORD='deepoj_redis_change_me'
 export POSTGRES_PASSWORD='deepoj_pg_password_change_me'
 export MINIO_ROOT_USER='deepoj_minio_user'
 export MINIO_ROOT_PASSWORD='deepoj_minio_password_change_me'
-export WORKER_AUTH_TOKEN='deepoj_worker_token_change_me'
 
 docker compose down -v --remove-orphans
 docker compose up -d --build
@@ -35,6 +34,18 @@ python3 tests/integration/test_e2e.py
 bash scripts/verify_b1_no_etcd.sh
 ```
 
+### 0.2 无 legacy 数据面验证（B2）
+
+系统已移除 legacy 数据面（Redis List + scheduler gRPC push + 旧回写链路），仅保留 Streams-only：
+
+`API(outbox) -> Redis Stream -> Worker(XREADGROUP) -> DB finalize -> XACK -> XAUTOCLAIM reclaim`
+
+验证命令：
+
+```bash
+bash scripts/verify_b2_no_legacy_dataplane.sh
+```
+
 ## 1. 环境与依赖
 
 最低要求（生产建议高于此基线）：
@@ -82,8 +93,8 @@ cd /home/diguo/Deep_OJ/src/go && go test ./internal/worker -run TestJudgeSelfTes
 | 组件 | 容器名 | 容器内端口 | 宿主机端口 | 健康检查方式 |
 | --- | --- | --- | --- | --- |
 | API | `oj-api` | `18080` | `18080` | `curl -fsS http://127.0.0.1:18080/api/v1/health` |
-| Scheduler | `oj-scheduler` | `9091`(metrics), `50052`(预留) | `50052` | `docker compose logs --tail=100 scheduler`（应看到 `Scheduler started`）；metrics 见第 5 节 |
-| Worker | `oj-worker` | `50051`(gRPC), `9092`(metrics) | 无映射 | `docker compose logs --tail=100 worker`（应看到 `gRPC server listening`） |
+| Scheduler | `oj-scheduler` | `9091`(metrics) | 无映射 | `docker compose logs --tail=100 scheduler`（应看到 `控制面模式` 相关日志）；metrics 见第 5 节 |
+| Worker | `oj-worker` | `9092`(metrics) | 无映射 | `docker compose logs --tail=100 worker`（应看到 `工作节点流消费器` 已启动） |
 | Redis | `oj-redis` | `6379` | 无映射 | `docker exec -it oj-redis redis-cli -a "$REDIS_PASSWORD" PING` |
 | PostgreSQL | `oj-postgres` | `5432` | 无映射 | `docker exec -it oj-postgres pg_isready -U deep_oj -d deep_oj` |
 | MinIO | `oj-minio` | `9000`,`9001` | 无映射 | `docker run --rm --network deep-oj-net curlimages/curl:8.7.1 -fsS http://oj-minio:9000/minio/health/live` |
@@ -109,7 +120,6 @@ export REDIS_PASSWORD='deepoj_redis_change_me'
 export POSTGRES_PASSWORD='deepoj_pg_password_change_me'
 export MINIO_ROOT_USER='deepoj_minio_user'
 export MINIO_ROOT_PASSWORD='deepoj_minio_password_change_me'
-export WORKER_AUTH_TOKEN='deepoj_worker_token_change_me'
 
 docker compose up -d --build
 curl -fsS "$API_BASE/api/v1/health"
@@ -193,8 +203,8 @@ done
 | --- | --- | --- | --- |
 | Redis 不可达，API 报 `QUEUE_ERROR` | `REDIS_PASSWORD` 不一致、Redis 容器未启动、网络不可达 | `docker compose ps redis`; `docker exec -it oj-redis redis-cli -a "$REDIS_PASSWORD" PING`; `docker compose logs --tail=200 api` | 统一 `REDIS_PASSWORD`，重启 `redis/api/scheduler/worker`，必要时 `docker compose up -d` |
 | DB 连接池耗尽，接口变慢/5xx | `PG_MAX_CONNS` 太小、长事务、慢 SQL | `docker compose logs --tail=300 api | rg -i "database|timeout|too many"`; `docker exec -it oj-postgres psql -U deep_oj -d deep_oj -c "select * from pg_stat_activity;"` | 增大 `PG_MAX_CONNS`，排查慢查询，缩短事务并重启 API |
-| 队列堆积（`queue:pending` 持续增长） | Worker 不可用、Scheduler 调度失败、下游执行慢 | `docker exec -it oj-redis redis-cli -a "$REDIS_PASSWORD" LLEN queue:pending`; `docker compose logs --tail=200 scheduler worker` | 先恢复 worker 可用性，再根据 CPU/内存扩容 worker 或降低提交速率 |
-| Worker 不消费任务 | gRPC 不可达、worker 启动失败、`WORKER_ADDR` 配置错误 | `docker compose logs --tail=200 worker`; `docker compose logs --tail=200 scheduler | rg -i "No workers|dispatch"` | 修复 `WORKER_ADDR/WORKER_ADDRS`，确认 worker 可达，重启 worker/scheduler |
+| Stream backlog 持续增长（`deepoj:jobs`） | Worker 消费异常、下游执行慢、Redis 压力高 | `docker exec -it oj-redis redis-cli -a "$REDIS_PASSWORD" XLEN deepoj:jobs`; `docker exec -it oj-redis redis-cli -a "$REDIS_PASSWORD" XPENDING deepoj:jobs deepoj:workers`; `docker compose logs --tail=200 worker` | 优先恢复 worker 消费能力，再按资源瓶颈扩容 worker 或限流 |
+| Worker 不消费任务 | Worker 启动失败、流消费组配置错误、DB/Redis 依赖异常 | `docker compose logs --tail=200 worker`; `docker exec -it oj-redis redis-cli -a "$REDIS_PASSWORD" XINFO GROUPS deepoj:jobs` | 修复 `JOB_STREAM_*` 与依赖配置，确认消费组存在并可读写，重启 worker |
 | Judge 出现超时/TLE 异常增多 | 用户代码死循环、时间限制过低、机器负载过高 | `docker compose logs --tail=300 worker | rg -i "Time Limit|timed out|command timed out"` | 调整题目时限、检查资源压力、必要时横向扩容 worker |
 | Judge 出现 OOM/MLE 异常增多 | 代码内存占用过大、内存限制过低、容器内存紧张 | `docker compose logs --tail=300 worker | rg -i "Memory Limit|OOM|killed"` | 调整内存上限，检查宿主机内存，限制异常任务并重试 |
 | 出现僵尸进程或残留判题进程 | kill/cleanup 未完整执行、worker 异常中断 | `docker exec -it oj-worker ps -eo pid,ppid,stat,cmd | rg "judge_engine| Z "` | 重启 worker 清理现场；后续按任务 G1/G2 加强进程组 kill 与幂等清理 |
@@ -425,7 +435,7 @@ bash scripts/repo_survey_probe.sh
 
 ### 6.2 输出解释
 
-- `[1/3]`：输出 `queue:pending` / `queue:processing` / `stream:results` 长度
+- `[1/3]`：输出 `deepoj:jobs` 的 `XLEN`
 - `[2/3]`：输出 PG 表列表与 `submissions` 字段定义
 - `[3/3]`：输出核心 metrics 名称匹配结果
 
diff --git a/b1_no_etcd_unified.diff b/b1_no_etcd_unified.diff
deleted file mode 100644
index 3c3837a..0000000
--- a/b1_no_etcd_unified.diff
+++ /dev/null
@@ -1,1478 +0,0 @@
-diff --git a/FAILURE_MODES.md b/FAILURE_MODES.md
-index 7e98bf5..f9090b7 100644
---- a/FAILURE_MODES.md
-+++ b/FAILURE_MODES.md
-@@ -14,7 +14,7 @@
- - 触发点：`src/go/cmd/scheduler/main.go` `selectWorker` 失败
- - 现象：任务被 `RequeueTask(queue:processing -> queue:pending)` 循环重试
- - 定位：日志关键字 `No workers available`
--- 恢复：恢复 worker/etcd 注册，观察 pending 回落
-+- 恢复：恢复 worker 可达（`WORKER_ADDR/WORKER_ADDRS`），观察 pending 回落
- 
- ## 3. Worker 执行完成但结果未入 Stream
- 
-@@ -43,4 +43,3 @@
- - 现象：当前 SQL 条件 `state != 'done'` 下被忽略
- - 定位：日志关键字 `Duplicate result ignored (already done)`
- - 恢复：无需人工处理；后续需升级到 attempt_id fencing（C5）
--
-diff --git a/RUNBOOK.md b/RUNBOOK.md
-index 3ab4ac8..dac3378 100644
---- a/RUNBOOK.md
-+++ b/RUNBOOK.md
-@@ -24,9 +24,17 @@ python3 tests/integration/test_e2e.py
- 
- 预期：
- 
--- `docker compose ps` 中 `api/scheduler/worker/redis/postgres/minio/etcd` 为 `Up`
-+- `docker compose ps` 中 `api/scheduler/worker/redis/postgres/minio` 为 `Up`
- - `test_e2e.py` 输出 `Submitted, Job ID:`，并在轮询后出现 `Result: Accepted`（或可预期判题状态）
- 
-+### 0.1 无 etcd 启动验证（B1）
-+
-+系统已移除 etcd 运行时依赖。请使用以下脚本执行完整验收：
-+
-+```bash
-+bash scripts/verify_b1_no_etcd.sh
-+```
-+
- ## 1. 环境与依赖
- 
- 最低要求（生产建议高于此基线）：
-@@ -46,7 +54,6 @@ python3 tests/integration/test_e2e.py
- - Redis（兼容 compose 中 `redis:alpine`）
- - PostgreSQL 15（compose 使用 `postgres:15-alpine`）
- - MinIO（项目使用对象存储下载测试数据；非可选于完整判题链路）
--- Etcd 3.5（Worker 服务发现）
- 
- ### 1.1 本地运行 judge 并查看 JSON 输出
- 
-@@ -187,7 +194,7 @@ done
- | Redis 不可达，API 报 `QUEUE_ERROR` | `REDIS_PASSWORD` 不一致、Redis 容器未启动、网络不可达 | `docker compose ps redis`; `docker exec -it oj-redis redis-cli -a "$REDIS_PASSWORD" PING`; `docker compose logs --tail=200 api` | 统一 `REDIS_PASSWORD`，重启 `redis/api/scheduler/worker`，必要时 `docker compose up -d` |
- | DB 连接池耗尽，接口变慢/5xx | `PG_MAX_CONNS` 太小、长事务、慢 SQL | `docker compose logs --tail=300 api | rg -i "database|timeout|too many"`; `docker exec -it oj-postgres psql -U deep_oj -d deep_oj -c "select * from pg_stat_activity;"` | 增大 `PG_MAX_CONNS`，排查慢查询，缩短事务并重启 API |
- | 队列堆积（`queue:pending` 持续增长） | Worker 不可用、Scheduler 调度失败、下游执行慢 | `docker exec -it oj-redis redis-cli -a "$REDIS_PASSWORD" LLEN queue:pending`; `docker compose logs --tail=200 scheduler worker` | 先恢复 worker 可用性，再根据 CPU/内存扩容 worker 或降低提交速率 |
--| Worker 不消费任务 | Etcd 注册失败、gRPC 不可达、worker 启动失败 | `docker compose logs --tail=200 worker`; `docker compose logs --tail=200 scheduler | rg -i "No workers|dispatch"` | 修复 `ETCD_ENDPOINTS/WORKER_ADDR`，确认 `oj-etcd` 可达，重启 worker/scheduler |
-+| Worker 不消费任务 | gRPC 不可达、worker 启动失败、`WORKER_ADDR` 配置错误 | `docker compose logs --tail=200 worker`; `docker compose logs --tail=200 scheduler | rg -i "No workers|dispatch"` | 修复 `WORKER_ADDR/WORKER_ADDRS`，确认 worker 可达，重启 worker/scheduler |
- | Judge 出现超时/TLE 异常增多 | 用户代码死循环、时间限制过低、机器负载过高 | `docker compose logs --tail=300 worker | rg -i "Time Limit|timed out|command timed out"` | 调整题目时限、检查资源压力、必要时横向扩容 worker |
- | Judge 出现 OOM/MLE 异常增多 | 代码内存占用过大、内存限制过低、容器内存紧张 | `docker compose logs --tail=300 worker | rg -i "Memory Limit|OOM|killed"` | 调整内存上限，检查宿主机内存，限制异常任务并重试 |
- | 出现僵尸进程或残留判题进程 | kill/cleanup 未完整执行、worker 异常中断 | `docker exec -it oj-worker ps -eo pid,ppid,stat,cmd | rg "judge_engine| Z "` | 重启 worker 清理现场；后续按任务 G1/G2 加强进程组 kill 与幂等清理 |
-diff --git a/config.yaml b/config.yaml
-index 10b7deb..90353ff 100644
---- a/config.yaml
-+++ b/config.yaml
-@@ -96,8 +96,6 @@ api:
-     job_payload_ttl_sec: 86400
- 
- scheduler:
--  etcd_endpoints:
--    - "localhost:2379"
-   redis_url: "localhost:6379"
-   database_url: "postgres://deep_oj:change_me@localhost:5432/deep_oj?sslmode=disable"
-   worker_capacity: 4
-@@ -108,7 +106,6 @@ scheduler:
-   dispatch_enabled: false       # legacy_grpc_push requires true
-   metrics_port: 9091
-   metrics_poll_ms: 1000
--  etcd_dial_timeout_ms: 5000
-   queue:
-     brpop_timeout_sec: 5
-     no_worker_sleep_ms: 1000
-@@ -145,10 +142,6 @@ worker:
-   id: ""
-   addr: ""
-   port: 50051
--  etcd_endpoints:
--    - "localhost:2379"
--  etcd_dial_timeout_ms: 5000
--  etcd_lease_ttl_sec: 10
-   redis_url: "localhost:6379"
-   database_url: "postgres://deep_oj:change_me@localhost:5432/deep_oj?sslmode=disable"
-   stream:
-diff --git a/docker-compose.yml b/docker-compose.yml
-index 87ef737..ea1e314 100644
---- a/docker-compose.yml
-+++ b/docker-compose.yml
-@@ -37,7 +37,6 @@ services:
-       - MINIO_ACCESS_KEY=${MINIO_ROOT_USER:-deepoj_minio_user}
-       - MINIO_SECRET_KEY=${MINIO_ROOT_PASSWORD:-deepoj_minio_password_change_me}
-       - MINIO_BUCKET=deep-oj-problems
--      - ETCD_ENDPOINTS=oj-etcd:2379
-       - WORKER_ADDR=oj-worker:50051
-       - JOB_STREAM_KEY=deepoj:jobs
-       - JOB_STREAM_GROUP=deepoj:workers
-@@ -70,7 +69,6 @@ services:
-       - REDIS_URL=oj-redis:6379
-       - REDIS_PASSWORD=${REDIS_PASSWORD:-deepoj_redis_change_me}
-       - WORKER_ADDR=oj-worker:50051
--      - ETCD_ENDPOINTS=oj-etcd:2379 # Need Etcd service
-       - PGPASSWORD=${POSTGRES_PASSWORD:-deepoj_pg_password_change_me}
-       - DATABASE_URL=postgres://deep_oj:${POSTGRES_PASSWORD:-deepoj_pg_password_change_me}@oj-postgres:5432/deep_oj?sslmode=disable
-       - WORKER_AUTH_TOKEN=${WORKER_AUTH_TOKEN:-deepoj_worker_token_change_me}
-@@ -103,6 +101,8 @@ services:
-       - MINIO_ACCESS_KEY=${MINIO_ROOT_USER:-deepoj_minio_user}
-       - MINIO_SECRET_KEY=${MINIO_ROOT_PASSWORD:-deepoj_minio_password_change_me}
-       - MINIO_BUCKET=deep-oj-problems
-+      - JWT_SECRET=${JWT_SECRET:-dev_jwt_secret_change_me}
-+      - ADMIN_USERS=${ADMIN_USERS:-admin}
-       - METRICS_TOKEN=${METRICS_TOKEN:-}
-     networks:
-       - deep-oj-net
-@@ -143,16 +143,6 @@ services:
-     networks:
-       - deep-oj-net
- 
--  # 7. Etcd (Service Discovery)
--  etcd:
--    image: registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.0-0
--    container_name: oj-etcd
--    command: /usr/local/bin/etcd --listen-client-urls http://0.0.0.0:2379 --advertise-client-urls http://oj-etcd:2379
--    expose:
--      - "2379"
--    networks:
--      - deep-oj-net
--
- networks:
-   deep-oj-net:
-     driver: bridge
-diff --git a/scripts/check_deps.go b/scripts/check_deps.go
-index d29f56a..afbb953 100644
---- a/scripts/check_deps.go
-+++ b/scripts/check_deps.go
-@@ -10,7 +10,6 @@ import (
- func main() {
- 	services := map[string]string{
- 		"Redis":    "127.0.0.1:6379",
--		"Etcd":     "127.0.0.1:2379",
- 		"Postgres": "127.0.0.1:5432",
- 	}
- 
-diff --git a/scripts/start_docker.sh b/scripts/start_docker.sh
-index c8db489..7524b25 100755
---- a/scripts/start_docker.sh
-+++ b/scripts/start_docker.sh
-@@ -19,16 +19,7 @@ if [ ! "$(docker ps -q -f name=oj-minio)" ]; then
-         minio/minio:RELEASE.2024-01-18T22-51-28Z server /data --console-address ":9001"
- fi
- 
--# 2. Etcd
--echo "Starting Etcd..."
--docker rm -f oj-etcd || true
--docker run -d --name oj-etcd \
--    --network deep-oj-net \
--    -p 2381:2379 \
--    registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.0-0 \
--    /usr/local/bin/etcd --listen-client-urls http://0.0.0.0:2379 --advertise-client-urls http://oj-etcd:2379
--
--# 3. Redis
-+# 2. Redis
- echo "Starting Redis..."
- docker rm -f oj-redis || true
- docker run -d --name oj-redis \
-@@ -36,7 +27,7 @@ docker run -d --name oj-redis \
-     -p 6380:6379 \
-     redis:alpine
- 
--# 4. Postgres
-+# 3. Postgres
- echo "Starting Postgres..."
- docker rm -f oj-postgres || true
- docker run -d --name oj-postgres \
-@@ -52,7 +43,7 @@ docker run -d --name oj-postgres \
- echo "Waiting for Postgres..."
- sleep 5
- 
--# 5. Worker
-+# 4. Worker
- echo "Starting Worker..."
- docker rm -f oj-worker || true
- docker run -d --name oj-worker \
-@@ -66,13 +57,12 @@ docker run -d --name oj-worker \
-     -e MINIO_ACCESS_KEY=minioadmin \
-     -e MINIO_SECRET_KEY=minioadmin \
-     -e MINIO_BUCKET=deep-oj-problems \
--    -e ETCD_ENDPOINTS=oj-etcd:2379 \
-     -e WORKER_ADDR=oj-worker:50051 \
-     -e JUDGER_BIN=/app/judge_engine \
-     -e WORKSPACE=/data/workspace \
-     deep-oj:v3 /app/oj_worker
- 
--# 6. Scheduler
-+# 5. Scheduler
- echo "Starting Scheduler..."
- docker rm -f oj-scheduler || true
- docker run -d --name oj-scheduler \
-@@ -80,12 +70,11 @@ docker run -d --name oj-scheduler \
-     -p 50052:50052 \
-     -e REDIS_URL=oj-redis:6379 \
-     -e WORKER_ADDR=oj-worker:50051 \
--    -e ETCD_ENDPOINTS=oj-etcd:2379 \
-     -e PGPASSWORD=secret \
-     -e DATABASE_URL=postgres://deep_oj:secret@oj-postgres:5432/deep_oj?sslmode=disable \
-     deep-oj:v3 /app/oj_scheduler
- 
--# 7. API
-+# 6. API
- echo "Starting API..."
- docker rm -f oj-api || true
- docker run -d --name oj-api \
-diff --git a/scripts/start_integration.sh b/scripts/start_integration.sh
-index def4dd3..d939881 100755
---- a/scripts/start_integration.sh
-+++ b/scripts/start_integration.sh
-@@ -8,7 +8,6 @@ NC='\033[0m'
- 
- cleanup() {
-     echo -e "\n${RED}停止服务中...${NC}"
--    pkill -f "etcd"
-     pkill -f "bin/api"
-     pkill -f "bin/scheduler"
-     echo "qsrlys67" | sudo -S pkill -f "oj_worker"
-@@ -18,7 +17,7 @@ trap cleanup EXIT
- # 1. 检查依赖
- echo -e "${GREEN}检查依赖项...${NC}"
- MISSING=0
--for cmd in etcd psql python3 go; do
-+for cmd in psql python3 go; do
-     if ! command -v $cmd &> /dev/null; then
-         echo -e "${RED}缺失依赖: $cmd${NC}"
-         MISSING=1
-@@ -33,12 +32,6 @@ fi
- # 2. 启动基础设施
- echo -e "${GREEN}启动基础设施...${NC}"
- 
--# 启动 Etcd
--if ! pgrep etcd > /dev/null; then
--    nohup etcd > /dev/null 2>&1 &
--    sleep 2
--fi
--
- # 检查 Redis
- if ! redis-cli ping > /dev/null 2>&1; then
-     echo -e "${RED}Redis 未运行! 请启动 Redis.${NC}"
-@@ -121,8 +114,7 @@ cd ..
- 
- # 启动 C++ Worker (需要 root 权限配置 Cgroup)
- # 使用 sudo -b 后台运行，并且把日志重定向
--# 必须设置 ETCD_ENDPOINTS 才能进行服务注册
--echo "qsrlys67" | sudo -S -E -b ETCD_ENDPOINTS=localhost:2379 WORKER_ADDR=localhost:50051 ./build/oj_worker > worker.log 2>&1
-+echo "qsrlys67" | sudo -S -E -b WORKER_ADDR=localhost:50051 ./build/oj_worker > worker.log 2>&1
- echo "Worker 已启动 (sudo)"
- 
- # 等待服务就绪
-diff --git a/scripts/verify_d3_backpressure.sh b/scripts/verify_d3_backpressure.sh
-index ff12d4b..837e762 100755
---- a/scripts/verify_d3_backpressure.sh
-+++ b/scripts/verify_d3_backpressure.sh
-@@ -111,7 +111,7 @@ emit_diagnostics() {
- 
- cleanup_conflicting_containers() {
-   local name
--  for name in oj-api oj-worker oj-scheduler oj-redis oj-postgres oj-minio oj-etcd; do
-+  for name in oj-api oj-worker oj-scheduler oj-redis oj-postgres oj-minio; do
-     if docker ps -a --format '{{.Names}}' | grep -qx "$name"; then
-       docker rm -f "$name" >/dev/null 2>&1 || true
-     fi
-@@ -242,9 +242,9 @@ ensure_stream_group() {
- 
- export JWT_SECRET ADMIN_USERS REDIS_PASSWORD STREAM_KEY STREAM_GROUP
- 
--echo "[1/8] start core services (api/scheduler/redis/postgres/minio/etcd)"
-+echo "[1/8] start core services (api/scheduler/redis/postgres/minio)"
- cleanup_conflicting_containers
--if ! docker compose up -d --build api scheduler redis postgres minio etcd; then
-+if ! docker compose up -d --build api scheduler redis postgres minio; then
-   fatal "docker compose up failed"
- fi
- 
-diff --git a/scripts/verify_mvp3_crash_recover.sh b/scripts/verify_mvp3_crash_recover.sh
-index c4a68b3..a4ba7a4 100755
---- a/scripts/verify_mvp3_crash_recover.sh
-+++ b/scripts/verify_mvp3_crash_recover.sh
-@@ -34,6 +34,13 @@ READINESS_HEALTH_STATUS="<none>"
- READINESS_HEALTH_BODY_FILE=""
- READINESS_LAST_SOURCE="<none>"
- HAS_RG=0
-+COMPOSE_FILES=(-f docker-compose.yml)
-+if [[ -f docker-compose.verify-g1.yml ]]; then
-+  COMPOSE_FILES+=(-f docker-compose.verify-g1.yml)
-+fi
-+if [[ -f docker-compose.override.yml ]]; then
-+  COMPOSE_FILES+=(-f docker-compose.override.yml)
-+fi
- 
- cleanup() {
-   if [[ "$KEEP_TMP" == "1" ]]; then
-@@ -81,12 +88,16 @@ rg_or_grep() {
-   fi
- }
- 
-+compose_cmd() {
-+  docker compose "${COMPOSE_FILES[@]}" "$@"
-+}
-+
- compose_service_exists() {
-   local service="$1"
-   local svc
-   while IFS= read -r svc; do
-     [[ "$svc" == "$service" ]] && return 0
--  done < <(docker compose config --services 2>/dev/null || true)
-+  done < <(compose_cmd config --services 2>/dev/null || true)
-   return 1
- }
- 
-@@ -95,7 +106,7 @@ service_running() {
-   local svc
-   while IFS= read -r svc; do
-     [[ "$svc" == "$service" ]] && return 0
--  done < <(docker compose ps --status running --services 2>/dev/null || true)
-+  done < <(compose_cmd ps --status running --services 2>/dev/null || true)
-   return 1
- }
- 
-@@ -124,7 +135,7 @@ emit_diagnostics() {
-   echo
-   echo "========== diagnostics ==========" >&2
-   echo "[docker compose ps]" >&2
--  docker compose ps >&2 || true
-+  compose_cmd ps >&2 || true
-   echo "[api readiness snapshot]" >&2
-   echo "  source=${READINESS_LAST_SOURCE}" >&2
-   echo "  health_status=${READINESS_HEALTH_STATUS}" >&2
-@@ -134,14 +145,14 @@ emit_diagnostics() {
-   fi
- 
-   local services
--  services="$(docker compose config --services 2>/dev/null || true)"
-+  services="$(compose_cmd config --services 2>/dev/null || true)"
-   if [[ -z "$services" ]]; then
-     echo "[docker compose config --services] <none>" >&2
-   else
-     while IFS= read -r service; do
-       [[ -z "$service" ]] && continue
-       echo "[docker compose logs --tail=200 ${service}]" >&2
--      docker compose logs --tail=200 "$service" >&2 || true
-+      compose_cmd logs --tail=200 "$service" >&2 || true
-     done <<< "$services"
-   fi
-   echo "=================================" >&2
-@@ -291,7 +302,7 @@ wait_api_ready() {
-     fi
- 
-     if compose_service_exists "api"; then
--      if docker compose logs --tail=200 api 2>&1 | rg_or_grep "API Server starting" >/dev/null 2>&1; then
-+      if compose_cmd logs --tail=200 api 2>&1 | rg_or_grep "API Server starting" >/dev/null 2>&1; then
-         READINESS_LAST_SOURCE="compose_log"
-         return 0
-       fi
-@@ -388,11 +399,11 @@ fi
- echo "[1/10] start services"
- FAILED_STEP="start services"
- if [[ "$COMPOSE_BUILD" == "1" ]]; then
--  if ! docker compose up -d --build; then
-+  if ! compose_cmd up -d --build; then
-     fail_verify "$FAILED_STEP" "<none>" "" "docker compose up -d --build failed"
-   fi
- else
--  if ! docker compose up -d; then
-+  if ! compose_cmd up -d; then
-     fail_verify "$FAILED_STEP" "<none>" "" "docker compose up -d failed"
-   fi
- fi
-@@ -579,7 +590,7 @@ if ! is_int "$XPENDING_BEFORE"; then
- fi
- 
- FAILED_STEP="kill worker"
--if ! docker compose kill -s SIGKILL "$WORKER_SERVICE" >/dev/null 2>&1; then
-+if ! compose_cmd kill -s SIGKILL "$WORKER_SERVICE" >/dev/null 2>&1; then
-   worker_container="$(find_container_by_service "$WORKER_SERVICE")"
-   if [[ -z "$worker_container" ]]; then
-     fail_verify "$FAILED_STEP" "N/A" "" "failed to find worker container for SIGKILL"
-@@ -619,7 +630,7 @@ fi
- 
- sleep "$CRASH_WAIT_SEC"
- FAILED_STEP="restart worker"
--if ! docker compose up -d "$WORKER_SERVICE" >/dev/null 2>&1; then
-+if ! compose_cmd up -d "$WORKER_SERVICE" >/dev/null 2>&1; then
-   fail_verify "$FAILED_STEP" "N/A" "" "docker compose up -d worker failed"
- fi
- if ! wait_service_running "$WORKER_SERVICE" 90; then
-@@ -676,7 +687,7 @@ fi
- echo "[8/10] reclaim evidence"
- FAILED_STEP="collect reclaim evidence"
- WORKER_LOGS_SINCE_FILE="$TMP_DIR/worker_logs_since_crash.log"
--docker compose logs --since "$CRASH_TS" --tail=800 "$WORKER_SERVICE" > "$WORKER_LOGS_SINCE_FILE" 2>&1 || true
-+compose_cmd logs --since "$CRASH_TS" --tail=800 "$WORKER_SERVICE" > "$WORKER_LOGS_SINCE_FILE" 2>&1 || true
- RECLAIM_MATCHES="$(rg_or_grep "DB reclaim success|reclaim_claimed|XAUTOCLAIM|reclaim" "$WORKER_LOGS_SINCE_FILE" || true)"
- RECLAIM_MATCH_COUNT="$(printf '%s\n' "$RECLAIM_MATCHES" | sed '/^[[:space:]]*$/d' | wc -l | tr -d ' ')"
- 
-diff --git a/src/go/cmd/scheduler/main.go b/src/go/cmd/scheduler/main.go
-index ba66246..1156092 100644
---- a/src/go/cmd/scheduler/main.go
-+++ b/src/go/cmd/scheduler/main.go
-@@ -1,33 +1,4 @@
--/**
-- * @file main.go
-- * @brief Go Scheduler 入口
-- *
-- * 架构定位: 任务调度层
-- * 技术选型: Etcd (服务发现) + gRPC (Worker 通信) + Redis (任务队列)
-- *
-- *
-- * 1. Etcd 服务发现 vs 传统配置:
-- *    - 传统: 硬编码 Worker 地址，重启才能更新
-- *    - Etcd: Worker 动态注册，实时感知变化
-- *    - Lease 机制: Worker 定期续约，超时自动注销
-- *
-- * 2. 负载均衡策略:
-- *    - Round-Robin: 简单轮询，适合同构服务
-- *    - Weighted: 加权轮询，根据 Worker 能力分配
-- *    - Least-Connections: 最少连接优先
-- *    - Consistent-Hashing: 一致性哈希，适合缓存场景
-- *
-- * 3. gRPC 优势:
-- *    - HTTP/2: 多路复用，头部压缩
-- *    - Protobuf: 紧凑的二进制序列化
-- *    - 流式传输: 双向流支持
-- *    - 代码生成: 强类型接口
-- *
-- * 4. 可靠性设计:
-- *    - ACK 机制: 任务确认后才从队列移除
-- *    - 超时检测: 处理中任务超时后重新入队
-- *    - 重试策略: 指数退避 (Exponential Backoff)
-- */
-+// 调度器入口：不依赖 etcd，使用 WORKER_ADDR/WORKER_ADDRS 进行工作节点发现。
- package main
- 
- import (
-@@ -38,7 +9,6 @@ import (
- 	"os"
- 	"os/signal"
- 	"strconv"
--	"strings"
- 	"sync"
- 	"syscall"
- 	"time"
-@@ -82,7 +52,6 @@ func main() {
- 		appconfig.SetEnvIfEmptyInt("PG_MAX_CONN_LIFETIME_MIN", cfg.Postgres.MaxConnLifetimeMin)
- 		appconfig.SetEnvIfEmptyInt("PG_MAX_CONN_IDLE_MIN", cfg.Postgres.MaxConnIdleMin)
- 
--		appconfig.SetEnvIfEmptySlice("ETCD_ENDPOINTS", cfg.Scheduler.EtcdEndpoints)
- 		appconfig.SetEnvIfEmpty("REDIS_URL", cfg.Scheduler.RedisURL)
- 		appconfig.SetEnvIfEmpty("DATABASE_URL", cfg.Scheduler.DatabaseURL)
- 		appconfig.SetEnvIfEmptyInt("WORKER_CAPACITY", cfg.Scheduler.WorkerCapacity)
-@@ -91,7 +60,6 @@ func main() {
- 		appconfig.SetEnvIfEmpty("SCHEDULER_ID", cfg.Scheduler.SchedulerID)
- 		appconfig.SetEnvIfEmptyInt("SCHEDULER_METRICS_PORT", cfg.Scheduler.MetricsPort)
- 		appconfig.SetEnvIfEmptyInt("SCHEDULER_METRICS_POLL_INTERVAL_MS", cfg.Scheduler.MetricsPollMs)
--		appconfig.SetEnvIfEmptyInt("ETCD_DIAL_TIMEOUT_MS", cfg.Scheduler.EtcdDialTimeoutMs)
- 		appconfig.SetEnvIfEmptyInt("QUEUE_BRPOP_TIMEOUT_SEC", cfg.Scheduler.Queue.BRPopTimeoutSec)
- 		appconfig.SetEnvIfEmptyInt("NO_WORKER_SLEEP_MS", cfg.Scheduler.Queue.NoWorkerSleepMs)
- 		appconfig.SetEnvIfEmptyInt("ASSIGNMENT_TTL_SEC", cfg.Scheduler.Queue.AssignmentTTLSec)
-@@ -119,11 +87,6 @@ func main() {
- 	}
- 
- 	// 1. 读取配置
--	etcdEndpoints := os.Getenv("ETCD_ENDPOINTS")
--	if etcdEndpoints == "" {
--		etcdEndpoints = "localhost:2379"
--	}
--
- 	redisURL := os.Getenv("REDIS_URL")
- 	if redisURL == "" {
- 		redisURL = "localhost:6379"
-@@ -148,17 +111,16 @@ func main() {
- 		cancel()
- 	}()
- 
--	// 3. 初始化 Etcd 服务发现
--	endpoints := strings.Split(etcdEndpoints, ",")
--	discovery, err := scheduler.NewEtcdDiscovery(endpoints)
-+	// 3. 初始化工作节点发现（无 etcd 依赖）
-+	discovery, err := scheduler.NewWorkerDiscovery()
- 	if err != nil {
--		slog.Error("连接 Etcd 失败", "error", err)
-+		slog.Error("初始化工作节点发现失败", "error", err)
- 		os.Exit(1)
- 	}
- 	defer discovery.Close()
--	slog.Info("已连接 Etcd")
-+	slog.Info("工作节点发现已就绪", "worker_count", discovery.GetWorkerCount())
- 
--	// 启动 Worker 监听
-+	// 保留调用路径，当前实现会等待 ctx 退出。
- 	go discovery.WatchWorkers(ctx)
- 
- 	// 4. 初始化 Redis 客户端
-@@ -188,7 +150,7 @@ func main() {
- 
- 	// 6. 启动监控（探针与指标）
- 
--	// 6.1 启动指标轮询（Redis/Etcd 状态）
-+	// 6.1 启动指标轮询（Redis/Worker 状态）
- 	go scheduler.StartMetricsPoller(ctx, redisClient, discovery)
- 
- 	// 6.2 暴露 Prometheus 指标端点
-@@ -322,7 +284,7 @@ func main() {
- 	}
- }
- 
--func selectWorker(ctx context.Context, redisClient *repository.RedisClient, discovery *scheduler.EtcdDiscovery, capacity int) (string, string, bool) {
-+func selectWorker(ctx context.Context, redisClient *repository.RedisClient, discovery *scheduler.WorkerDiscovery, capacity int) (string, string, bool) {
- 	total := discovery.GetWorkerCount()
- 	if total == 0 {
- 		return "", "", false
-diff --git a/src/go/cmd/worker/main.go b/src/go/cmd/worker/main.go
-index 4c53a34..a511d02 100644
---- a/src/go/cmd/worker/main.go
-+++ b/src/go/cmd/worker/main.go
-@@ -15,7 +15,6 @@ import (
- 	"strings"
- 	"sync"
- 	"syscall"
--	"time"
- 
- 	"github.com/d1guo/deep_oj/internal/appconfig"
- 	"github.com/d1guo/deep_oj/internal/repository"
-@@ -23,7 +22,6 @@ import (
- 	"github.com/d1guo/deep_oj/pkg/observability"
- 	pb "github.com/d1guo/deep_oj/pkg/proto"
- 	"github.com/redis/go-redis/v9"
--	clientv3 "go.etcd.io/etcd/client/v3"
- 	"google.golang.org/grpc"
- 	"google.golang.org/grpc/codes"
- 	"google.golang.org/grpc/credentials"
-@@ -117,7 +115,6 @@ func main() {
- 		appconfig.SetEnvIfEmpty("WORKER_ID", wcfg.ID)
- 		appconfig.SetEnvIfEmpty("WORKER_ADDR", wcfg.Addr)
- 		appconfig.SetEnvIfEmptyInt("WORKER_PORT", wcfg.Port)
--		appconfig.SetEnvIfEmptySlice("ETCD_ENDPOINTS", wcfg.EtcdEndpoints)
- 		appconfig.SetEnvIfEmpty("REDIS_URL", wcfg.RedisURL)
- 		appconfig.SetEnvIfEmpty("DATABASE_URL", wcfg.DatabaseURL)
- 		appconfig.SetEnvIfEmpty("MINIO_ENDPOINT", wcfg.MinIO.Endpoint)
-@@ -143,8 +140,6 @@ func main() {
- 		appconfig.SetEnvIfEmptyInt("CLEANUP_TIMEOUT_SEC", wcfg.CleanupTimeoutSec)
- 		appconfig.SetEnvIfEmptyInt("RESULT_STREAM_MAX_RETRIES", wcfg.ResultStreamMaxRetries)
- 		appconfig.SetEnvIfEmptyInt("RESULT_STREAM_BACKOFF_MS", wcfg.ResultStreamBackoffMs)
--		appconfig.SetEnvIfEmptyInt("ETCD_DIAL_TIMEOUT_MS", wcfg.EtcdDialTimeoutMs)
--		appconfig.SetEnvIfEmptyInt("ETCD_LEASE_TTL_SEC", wcfg.EtcdLeaseTTLSec)
- 		appconfig.SetEnvIfEmptyBool("ALLOW_HOST_CHECKER", wcfg.AllowHostChecker)
- 		appconfig.SetEnvIfEmptyBool("REQUIRE_CGROUPS_V2", wcfg.RequireCgroupsV2)
- 		appconfig.SetEnvIfEmpty("GRPC_TLS_CERT", wcfg.GRPCTLS.Cert)
-@@ -254,34 +249,7 @@ func main() {
- 		}
- 	}()
- 
--	// 4. Etcd 注册
--	dialMs := getEnvInt("ETCD_DIAL_TIMEOUT_MS", 5000)
--	cli, err := clientv3.New(clientv3.Config{
--		Endpoints:   cfg.EtcdEndpoints,
--		DialTimeout: time.Duration(dialMs) * time.Millisecond,
--	})
--	if err != nil {
--		slog.Error("连接 etcd 失败", "error", err)
--		os.Exit(1)
--	}
--	defer cli.Close()
--
--	leaseTTL := int64(getEnvInt("ETCD_LEASE_TTL_SEC", 10))
--	key := fmt.Sprintf("/deep-oj/workers/%s", cfg.WorkerID)
--	val := cfg.WorkerAddr
--
--	regCtx, cancelReg := context.WithCancel(context.Background())
--	defer cancelReg()
--	leaseID, keepAliveCh, err := registerWorkerWithLease(regCtx, cli, key, val, leaseTTL)
--	if err != nil {
--		slog.Error("首次注册工作节点失败", "error", err)
--		os.Exit(1)
--	}
--	go maintainWorkerRegistration(regCtx, cli, key, val, leaseTTL, leaseID, keepAliveCh)
--
--	slog.Info("工作节点已注册", "key", key, "lease_id", leaseID)
--
--	// 5. 启动服务
-+	// 4. 启动服务
- 	go func() {
- 		slog.Info("gRPC 服务监听中", "addr", lis.Addr())
- 		if err := grpcServer.Serve(lis); err != nil {
-@@ -296,11 +264,9 @@ func main() {
- 	<-quit
- 
- 	slog.Info("正在关闭...")
--	cancelReg()
- 	cancelStream()
- 	streamWG.Wait()
- 	grpcServer.GracefulStop()
--	_, _ = cli.Delete(context.Background(), key)
- 	slog.Info("工作节点已退出")
- }
- 
-@@ -330,64 +296,3 @@ func loadServerTLS() (grpc.ServerOption, error) {
- 	}
- 	return grpc.Creds(credentials.NewTLS(tlsConfig)), nil
- }
--
--func registerWorkerWithLease(
--	ctx context.Context,
--	cli *clientv3.Client,
--	key, val string,
--	leaseTTL int64,
--) (clientv3.LeaseID, <-chan *clientv3.LeaseKeepAliveResponse, error) {
--	lease, err := cli.Grant(ctx, leaseTTL)
--	if err != nil {
--		return 0, nil, err
--	}
--	if _, err := cli.Put(ctx, key, val, clientv3.WithLease(lease.ID)); err != nil {
--		return 0, nil, err
--	}
--	ch, err := cli.KeepAlive(ctx, lease.ID)
--	if err != nil {
--		return 0, nil, err
--	}
--	return lease.ID, ch, nil
--}
--
--func maintainWorkerRegistration(
--	ctx context.Context,
--	cli *clientv3.Client,
--	key, val string,
--	leaseTTL int64,
--	leaseID clientv3.LeaseID,
--	ch <-chan *clientv3.LeaseKeepAliveResponse,
--) {
--	currentLeaseID := leaseID
--	currentCh := ch
--
--	for {
--		select {
--		case <-ctx.Done():
--			return
--		case _, ok := <-currentCh:
--			if ok {
--				continue
--			}
--			slog.Warn("Etcd 保活通道已关闭，尝试重新注册", "lease_id", currentLeaseID)
--			for {
--				select {
--				case <-ctx.Done():
--					return
--				default:
--				}
--				newLeaseID, newCh, err := registerWorkerWithLease(ctx, cli, key, val, leaseTTL)
--				if err != nil {
--					slog.Error("工作节点重新注册失败", "error", err)
--					time.Sleep(time.Second)
--					continue
--				}
--				currentLeaseID = newLeaseID
--				currentCh = newCh
--				slog.Info("工作节点已重新注册到 etcd", "lease_id", currentLeaseID)
--				break
--			}
--		}
--	}
--}
-diff --git a/src/go/go.mod b/src/go/go.mod
-index b55340d..fa0f060 100644
---- a/src/go/go.mod
-+++ b/src/go/go.mod
-@@ -10,7 +10,6 @@ require (
- 	github.com/minio/minio-go/v7 v7.0.98
- 	github.com/prometheus/client_golang v1.23.2
- 	github.com/redis/go-redis/v9 v9.18.0
--	go.etcd.io/etcd/client/v3 v3.5.10
- 	golang.org/x/crypto v0.48.0
- 	golang.org/x/oauth2 v0.35.0
- 	golang.org/x/sync v0.19.0
-@@ -26,8 +25,6 @@ require (
- 	github.com/bytedance/sonic v1.9.1 // indirect
- 	github.com/cespare/xxhash/v2 v2.3.0 // indirect
- 	github.com/chenzhuoyu/base64x v0.0.0-20221115062448-fe3a3abad311 // indirect
--	github.com/coreos/go-semver v0.3.0 // indirect
--	github.com/coreos/go-systemd/v22 v22.3.2 // indirect
- 	github.com/dgryski/go-rendezvous v0.0.0-20200823014737-9f7001d12a5f // indirect
- 	github.com/dustin/go-humanize v1.0.1 // indirect
- 	github.com/gabriel-vasile/mimetype v1.4.2 // indirect
-@@ -37,8 +34,6 @@ require (
- 	github.com/go-playground/universal-translator v0.18.1 // indirect
- 	github.com/go-playground/validator/v10 v10.14.0 // indirect
- 	github.com/goccy/go-json v0.10.2 // indirect
--	github.com/gogo/protobuf v1.3.2 // indirect
--	github.com/golang/protobuf v1.5.4 // indirect
- 	github.com/jackc/pgpassfile v1.0.0 // indirect
- 	github.com/jackc/pgservicefile v0.0.0-20240606120523-5a60cdf6a761 // indirect
- 	github.com/jackc/puddle/v2 v2.2.2 // indirect
-@@ -63,17 +58,12 @@ require (
- 	github.com/tinylib/msgp v1.6.1 // indirect
- 	github.com/twitchyliquid64/golang-asm v0.15.1 // indirect
- 	github.com/ugorji/go/codec v1.2.11 // indirect
--	go.etcd.io/etcd/api/v3 v3.5.10 // indirect
--	go.etcd.io/etcd/client/pkg/v3 v3.5.10 // indirect
- 	go.uber.org/atomic v1.11.0 // indirect
--	go.uber.org/multierr v1.6.0 // indirect
--	go.uber.org/zap v1.17.0 // indirect
- 	go.yaml.in/yaml/v2 v2.4.2 // indirect
- 	go.yaml.in/yaml/v3 v3.0.4 // indirect
- 	golang.org/x/arch v0.3.0 // indirect
- 	golang.org/x/net v0.50.0 // indirect
- 	golang.org/x/sys v0.41.0 // indirect
- 	golang.org/x/text v0.34.0 // indirect
--	google.golang.org/genproto/googleapis/api v0.0.0-20251029180050-ab9386a59fda // indirect
- 	google.golang.org/genproto/googleapis/rpc v0.0.0-20260209200024-4cfbd4190f57 // indirect
- )
-diff --git a/src/go/go.sum b/src/go/go.sum
-index 22e8446..4a7f959 100644
---- a/src/go/go.sum
-+++ b/src/go/go.sum
-@@ -12,10 +12,6 @@ github.com/cespare/xxhash/v2 v2.3.0/go.mod h1:VGX0DQ3Q6kWi7AoAeZDth3/j3BFtOZR5XL
- github.com/chenzhuoyu/base64x v0.0.0-20211019084208-fb5309c8db06/go.mod h1:DH46F32mSOjUmXrMHnKwZdA8wcEefY7UVqBKYGjpdQY=
- github.com/chenzhuoyu/base64x v0.0.0-20221115062448-fe3a3abad311 h1:qSGYFH7+jGhDF8vLC+iwCD4WpbV1EBDSzWkJODFLams=
- github.com/chenzhuoyu/base64x v0.0.0-20221115062448-fe3a3abad311/go.mod h1:b583jCggY9gE99b6G5LEC39OIiVsWj+R97kbl5odCEk=
--github.com/coreos/go-semver v0.3.0 h1:wkHLiw0WNATZnSG7epLsujiMCgPAc9xhjJ4tgnAxmfM=
--github.com/coreos/go-semver v0.3.0/go.mod h1:nnelYz7RCh+5ahJtPPxZlU+153eP4D4r3EedlOD2RNk=
--github.com/coreos/go-systemd/v22 v22.3.2 h1:D9/bQk5vlXQFZ6Kwuu6zaiXJ9oTPe68++AzAJc1DzSI=
--github.com/coreos/go-systemd/v22 v22.3.2/go.mod h1:Y58oyj3AT4RCenI/lSvhwexgC+NSVTIJ3seZv2GcEnc=
- github.com/creack/pty v1.1.9/go.mod h1:oKZEueFk5CKHvIhNR5MUki03XCEU+Q6VDXinZuGJ33E=
- github.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
- github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=
-@@ -46,9 +42,6 @@ github.com/go-playground/validator/v10 v10.14.0 h1:vgvQWe3XCz3gIeFDm/HnTIbj6UGmg
- github.com/go-playground/validator/v10 v10.14.0/go.mod h1:9iXMNT7sEkjXb0I+enO7QXmzG6QCsPWY4zveKFVRSyU=
- github.com/goccy/go-json v0.10.2 h1:CrxCmQqYDkv1z7lO7Wbh2HN93uovUHgrECaO5ZrCXAU=
- github.com/goccy/go-json v0.10.2/go.mod h1:6MelG93GURQebXPDq3khkgXZkazVtN9CRI+MGFi0w8I=
--github.com/godbus/dbus/v5 v5.0.4/go.mod h1:xhWf0FNVPg57R7Z0UbKHbJfkEywrmjJnf7w5xrFpKfA=
--github.com/gogo/protobuf v1.3.2 h1:Ov1cvc58UF3b5XjBnZv7+opcTcQFZebYjWzi34vdm4Q=
--github.com/gogo/protobuf v1.3.2/go.mod h1:P1XiOD3dCwIKUDQYPy72D8LYyHL2YPYrpS2s69NZV8Q=
- github.com/golang-jwt/jwt/v5 v5.3.1 h1:kYf81DTWFe7t+1VvL7eS+jKFVWaUnK9cB1qbwn63YCY=
- github.com/golang-jwt/jwt/v5 v5.3.1/go.mod h1:fxCRLWMO43lRc8nhHWY6LGqRcf+1gQWArsqaEUEa5bE=
- github.com/golang/protobuf v1.5.4 h1:i7eJL8qZTpSEXOPTxNKhASYpMn+8e5Q6AdndVa1dWek=
-@@ -68,8 +61,6 @@ github.com/jackc/puddle/v2 v2.2.2 h1:PR8nw+E/1w0GLuRFSmiioY6UooMp6KJv0/61nB7icHo
- github.com/jackc/puddle/v2 v2.2.2/go.mod h1:vriiEXHvEE654aYKXXjOvZM39qJ0q+azkZFrfEOc3H4=
- github.com/json-iterator/go v1.1.12 h1:PV8peI4a0ysnczrg+LtxykD8LfKY9ML6u2jnxaEnrnM=
- github.com/json-iterator/go v1.1.12/go.mod h1:e30LSqwooZae/UwlEbR2852Gd8hjQvJoHmT4TnhNGBo=
--github.com/kisielk/errcheck v1.5.0/go.mod h1:pFxgyoBC7bSaBwPgfKdkLd5X25qrDl4LWUI2bnpBCr8=
--github.com/kisielk/gotool v1.0.0/go.mod h1:XhKaO+MFFWcvkIS/tQcRk01m1F5IRFswLeQ+oQHNcck=
- github.com/klauspost/compress v1.18.2 h1:iiPHWW0YrcFgpBYhsA6D1+fqHssJscY/Tm/y2Uqnapk=
- github.com/klauspost/compress v1.18.2/go.mod h1:R0h/fSBs8DE4ENlcrlib3PsXS61voFxhIs2DeRhCvJ4=
- github.com/klauspost/cpuid/v2 v2.0.1/go.mod h1:FInQzS24/EEf25PyTYn52gqo7WaD8xa0213Md/qVLRg=
-@@ -105,8 +96,6 @@ github.com/pelletier/go-toml/v2 v2.0.8 h1:0ctb6s9mE31h0/lhu+J6OPmVeDxJn+kYnJc2jZ
- github.com/pelletier/go-toml/v2 v2.0.8/go.mod h1:vuYfssBdrU2XDZ9bYydBu6t+6a6PYNcZljzZR9VXg+4=
- github.com/philhofer/fwd v1.2.0 h1:e6DnBTl7vGY+Gz322/ASL4Gyp1FspeMvx1RNDoToZuM=
- github.com/philhofer/fwd v1.2.0/go.mod h1:RqIHx9QI14HlwKwm98g9Re5prTQ6LdeRQn+gXJFxsJM=
--github.com/pkg/errors v0.8.1 h1:iURUrRGxPUNPdy5/HRSm+Yj6okJ6UtLINN0Q9M4+h3I=
--github.com/pkg/errors v0.8.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=
- github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=
- github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
- github.com/prometheus/client_golang v1.23.2 h1:Je96obch5RDVy3FDMndoUsjAhG5Edi49h0RJWRi/o0o=
-@@ -141,16 +130,8 @@ github.com/twitchyliquid64/golang-asm v0.15.1 h1:SU5vSMR7hnwNxj24w34ZyCi/FmDZTkS
- github.com/twitchyliquid64/golang-asm v0.15.1/go.mod h1:a1lVb/DtPvCB8fslRZhAngC2+aY1QWCk3Cedj/Gdt08=
- github.com/ugorji/go/codec v1.2.11 h1:BMaWp1Bb6fHwEtbplGBGJ498wD+LKlNSl25MjdZY4dU=
- github.com/ugorji/go/codec v1.2.11/go.mod h1:UNopzCgEMSXjBc6AOMqYvWC1ktqTAfzJZUZgYf6w6lg=
--github.com/yuin/goldmark v1.1.27/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=
--github.com/yuin/goldmark v1.2.1/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=
- github.com/zeebo/xxh3 v1.0.2 h1:xZmwmqxHZA8AI603jOQ0tMqmBr9lPeFwGg6d+xy9DC0=
- github.com/zeebo/xxh3 v1.0.2/go.mod h1:5NWz9Sef7zIDm2JHfFlcQvNekmcEl9ekUZQQKCYaDcA=
--go.etcd.io/etcd/api/v3 v3.5.10 h1:szRajuUUbLyppkhs9K6BRtjY37l66XQQmw7oZRANE4k=
--go.etcd.io/etcd/api/v3 v3.5.10/go.mod h1:TidfmT4Uycad3NM/o25fG3J07odo4GBB9hoxaodFCtI=
--go.etcd.io/etcd/client/pkg/v3 v3.5.10 h1:kfYIdQftBnbAq8pUWFXfpuuxFSKzlmM5cSn76JByiT0=
--go.etcd.io/etcd/client/pkg/v3 v3.5.10/go.mod h1:DYivfIviIuQ8+/lCq4vcxuseg2P2XbHygkKwFo9fc8U=
--go.etcd.io/etcd/client/v3 v3.5.10 h1:W9TXNZ+oB3MCd/8UjxHTWK5J9Nquw9fQBLJd5ne5/Ao=
--go.etcd.io/etcd/client/v3 v3.5.10/go.mod h1:RVeBnDz2PUEZqTpgqwAtUd8nAPf5kjyFyND7P1VkOKc=
- go.opentelemetry.io/auto/sdk v1.2.1 h1:jXsnJ4Lmnqd11kwkBV2LgLoFMZKizbCi5fNZ/ipaZ64=
- go.opentelemetry.io/auto/sdk v1.2.1/go.mod h1:KRTj+aOaElaLi+wW1kO/DZRXwkF4C5xPbEe3ZiIhN7Y=
- go.opentelemetry.io/otel v1.38.0 h1:RkfdswUDRimDg0m2Az18RKOsnI8UDzppJAtj01/Ymk8=
-@@ -163,15 +144,10 @@ go.opentelemetry.io/otel/sdk/metric v1.38.0 h1:aSH66iL0aZqo//xXzQLYozmWrXxyFkBJ6
- go.opentelemetry.io/otel/sdk/metric v1.38.0/go.mod h1:dg9PBnW9XdQ1Hd6ZnRz689CbtrUp0wMMs9iPcgT9EZA=
- go.opentelemetry.io/otel/trace v1.38.0 h1:Fxk5bKrDZJUH+AMyyIXGcFAPah0oRcT+LuNtJrmcNLE=
- go.opentelemetry.io/otel/trace v1.38.0/go.mod h1:j1P9ivuFsTceSWe1oY+EeW3sc+Pp42sO++GHkg4wwhs=
--go.uber.org/atomic v1.7.0/go.mod h1:fEN4uk6kAWBTFdckzkM89CLk9XfWZrxpCo0nPH17wJc=
- go.uber.org/atomic v1.11.0 h1:ZvwS0R+56ePWxUNi+Atn9dWONBPp/AUETXlHW0DxSjE=
- go.uber.org/atomic v1.11.0/go.mod h1:LUxbIzbOniOlMKjJjyPfpl4v+PKK2cNJn91OQbhoJI0=
- go.uber.org/goleak v1.3.0 h1:2K3zAYmnTNqV73imy9J1T3WC+gmCePx2hEGkimedGto=
- go.uber.org/goleak v1.3.0/go.mod h1:CoHD4mav9JJNrW/WLlf7HGZPjdw8EucARQHekz1X6bE=
--go.uber.org/multierr v1.6.0 h1:y6IPFStTAIT5Ytl7/XYmHvzXQ7S3g/IeZW9hyZ5thw4=
--go.uber.org/multierr v1.6.0/go.mod h1:cdWPpRnG4AhwMwsgIHip0KRBQjJy5kYEpYjJxpXp9iU=
--go.uber.org/zap v1.17.0 h1:MTjgFu6ZLKvY6Pvaqk97GlxNBuMpV4Hy/3P6tRGlI2U=
--go.uber.org/zap v1.17.0/go.mod h1:MXVU+bhUf/A7Xi2HNOnopQOrmycQ5Ih87HtOu4q5SSo=
- go.yaml.in/yaml/v2 v2.4.2 h1:DzmwEr2rDGHl7lsFgAHxmNz/1NlQ7xLIrlN2h5d1eGI=
- go.yaml.in/yaml/v2 v2.4.2/go.mod h1:081UH+NErpNdqlCXm3TtEran0rJZGxAYx9hb/ELlsPU=
- go.yaml.in/yaml/v3 v3.0.4 h1:tfq32ie2Jv2UxXFdLJdh3jXuOzWiL1fo0bu/FbuKpbc=
-@@ -179,48 +155,21 @@ go.yaml.in/yaml/v3 v3.0.4/go.mod h1:DhzuOOF2ATzADvBadXxruRBLzYTpT36CKvDb3+aBEFg=
- golang.org/x/arch v0.0.0-20210923205945-b76863e36670/go.mod h1:5om86z9Hs0C8fWVUuoMHwpExlXzs5Tkyp9hOrfG7pp8=
- golang.org/x/arch v0.3.0 h1:02VY4/ZcO/gBOH6PUaoiptASxtXU10jazRCP865E97k=
- golang.org/x/arch v0.3.0/go.mod h1:5om86z9Hs0C8fWVUuoMHwpExlXzs5Tkyp9hOrfG7pp8=
--golang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=
--golang.org/x/crypto v0.0.0-20191011191535-87dc89f01550/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=
--golang.org/x/crypto v0.0.0-20200622213623-75b288015ac9/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=
- golang.org/x/crypto v0.48.0 h1:/VRzVqiRSggnhY7gNRxPauEQ5Drw9haKdM0jqfcCFts=
- golang.org/x/crypto v0.48.0/go.mod h1:r0kV5h3qnFPlQnBSrULhlsRfryS2pmewsg+XfMgkVos=
--golang.org/x/mod v0.2.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=
--golang.org/x/mod v0.3.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=
--golang.org/x/net v0.0.0-20190404232315-eb5bcb51f2a3/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=
--golang.org/x/net v0.0.0-20190620200207-3b0461eec859/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
--golang.org/x/net v0.0.0-20200226121028-0de0cce0169b/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
--golang.org/x/net v0.0.0-20201021035429-f5854403a974/go.mod h1:sp8m0HH+o8qH0wwXwYZr8TS3Oi6o0r6Gce1SSxlDquU=
- golang.org/x/net v0.50.0 h1:ucWh9eiCGyDR3vtzso0WMQinm2Dnt8cFMuQa9K33J60=
- golang.org/x/net v0.50.0/go.mod h1:UgoSli3F/pBgdJBHCTc+tp3gmrU4XswgGRgtnwWTfyM=
- golang.org/x/oauth2 v0.35.0 h1:Mv2mzuHuZuY2+bkyWXIHMfhNdJAdwW3FuWeCPYN5GVQ=
- golang.org/x/oauth2 v0.35.0/go.mod h1:lzm5WQJQwKZ3nwavOZ3IS5Aulzxi68dUSgRHujetwEA=
--golang.org/x/sync v0.0.0-20190423024810-112230192c58/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
--golang.org/x/sync v0.0.0-20190911185100-cd5d95a43a6e/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
--golang.org/x/sync v0.0.0-20201020160332-67f06af15bc9/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
- golang.org/x/sync v0.19.0 h1:vV+1eWNmZ5geRlYjzm2adRgW2/mcpevXNg50YZtPCE4=
- golang.org/x/sync v0.19.0/go.mod h1:9KTHXmSnoGruLpwFjVSX0lNNA75CykiMECbovNTZqGI=
--golang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=
--golang.org/x/sys v0.0.0-20190412213103-97732733099d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
--golang.org/x/sys v0.0.0-20200930185726-fdedc70b468f/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
- golang.org/x/sys v0.6.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
- golang.org/x/sys v0.41.0 h1:Ivj+2Cp/ylzLiEU89QhWblYnOE9zerudt9Ftecq2C6k=
- golang.org/x/sys v0.41.0/go.mod h1:OgkHotnGiDImocRcuBABYBEXf8A9a87e/uXjp9XT3ks=
--golang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=
--golang.org/x/text v0.3.3/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=
- golang.org/x/text v0.34.0 h1:oL/Qq0Kdaqxa1KbNeMKwQq0reLCCaFtqu2eNuSeNHbk=
- golang.org/x/text v0.34.0/go.mod h1:homfLqTYRFyVYemLBFl5GgL/DWEiH5wcsQ5gSh1yziA=
--golang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=
--golang.org/x/tools v0.0.0-20191119224855-298f0cb1881e/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=
--golang.org/x/tools v0.0.0-20200619180055-7c47624df98f/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=
--golang.org/x/tools v0.0.0-20210106214847-113979e3529a/go.mod h1:emZCQorbCU4vsT4fOWvOPXz4eW1wZW4PmDk9uLelYpA=
--golang.org/x/xerrors v0.0.0-20190717185122-a985d3407aa7/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
--golang.org/x/xerrors v0.0.0-20191011141410-1b5146add898/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
--golang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
--golang.org/x/xerrors v0.0.0-20200804184101-5ec99f83aff1/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
- gonum.org/v1/gonum v0.16.0 h1:5+ul4Swaf3ESvrOnidPp4GZbzf0mxVQpDCYUQE7OJfk=
- gonum.org/v1/gonum v0.16.0/go.mod h1:fef3am4MQ93R2HHpKnLk4/Tbh/s0+wqD5nfa6Pnwy4E=
--google.golang.org/genproto/googleapis/api v0.0.0-20251029180050-ab9386a59fda h1:+2XxjfsAu6vqFxwGBRcHiMaDCuZiqXGDUDVWVtrFAnE=
--google.golang.org/genproto/googleapis/api v0.0.0-20251029180050-ab9386a59fda/go.mod h1:fDMmzKV90WSg1NbozdqrE64fkuTv6mlq2zxo9ad+3yo=
- google.golang.org/genproto/googleapis/rpc v0.0.0-20260209200024-4cfbd4190f57 h1:mWPCjDEyshlQYzBpMNHaEof6UX1PmHcaUODUywQ0uac=
- google.golang.org/genproto/googleapis/rpc v0.0.0-20260209200024-4cfbd4190f57/go.mod h1:j9x/tPzZkyxcgEFkiKEEGxfvyumM01BEtsW8xzOahRQ=
- google.golang.org/grpc v1.78.0 h1:K1XZG/yGDJnzMdd/uZHAkVqJE+xIDOcmdSFZkBUicNc=
-@@ -230,11 +179,7 @@ google.golang.org/protobuf v1.36.11/go.mod h1:HTf+CrKn2C3g5S8VImy6tdcUvCska2kB7j
- gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
- gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c h1:Hei/4ADfdWqJk1ZMxUNpqntNwaWcugrBjAiHlqqRiVk=
- gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c/go.mod h1:JHkPIbrfpd72SG/EVd6muEfDQjcINNoR0C8j2r3qZ4Q=
--gopkg.in/yaml.v2 v2.2.8/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=
--gopkg.in/yaml.v2 v2.4.0 h1:D8xgwECY7CYvx+Y2n4sBz93Jn9JRvxdiyyo8CTfuKaY=
--gopkg.in/yaml.v2 v2.4.0/go.mod h1:RDklbk79AGWmwhnvt/jBztapEOGDOx6ZbXqjP6csGnQ=
- gopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
--gopkg.in/yaml.v3 v3.0.0-20210107192922-496545a6307b/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
- gopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=
- gopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
- rsc.io/pdf v0.1.1/go.mod h1:n8OzWcQ6Sp37PL01nO98y4iUCRdTGarVfzxY20ICaU4=
-diff --git a/src/go/internal/appconfig/config.go b/src/go/internal/appconfig/config.go
-index 6bdbb57..45726bd 100644
---- a/src/go/internal/appconfig/config.go
-+++ b/src/go/internal/appconfig/config.go
-@@ -105,23 +105,21 @@ type ProblemDefaults struct {
- }
- 
- type SchedulerConfig struct {
--	EtcdEndpoints     []string          `yaml:"etcd_endpoints"`
--	RedisURL          string            `yaml:"redis_url"`
--	DatabaseURL       string            `yaml:"database_url"`
--	WorkerCapacity    int               `yaml:"worker_capacity"`
--	MaxRetry          int               `yaml:"max_retry"`
--	RetryTTLSec       int               `yaml:"retry_ttl_sec"`
--	SchedulerID       string            `yaml:"scheduler_id"`
--	GRPCTLS           TLSConfig         `yaml:"grpc_tls"`
--	Metrics           Metrics           `yaml:"metrics"`
--	MetricsPort       int               `yaml:"metrics_port"`
--	MetricsPollMs     int               `yaml:"metrics_poll_ms"`
--	EtcdDialTimeoutMs int               `yaml:"etcd_dial_timeout_ms"`
--	Queue             SchedulerQueue    `yaml:"queue"`
--	AckListener       AckListenerConfig `yaml:"ack_listener"`
--	SlowPath          SlowPathConfig    `yaml:"slow_path"`
--	Watchdog          WatchdogConfig    `yaml:"watchdog"`
--	Dispatch          DispatchConfig    `yaml:"dispatch"`
-+	RedisURL       string            `yaml:"redis_url"`
-+	DatabaseURL    string            `yaml:"database_url"`
-+	WorkerCapacity int               `yaml:"worker_capacity"`
-+	MaxRetry       int               `yaml:"max_retry"`
-+	RetryTTLSec    int               `yaml:"retry_ttl_sec"`
-+	SchedulerID    string            `yaml:"scheduler_id"`
-+	GRPCTLS        TLSConfig         `yaml:"grpc_tls"`
-+	Metrics        Metrics           `yaml:"metrics"`
-+	MetricsPort    int               `yaml:"metrics_port"`
-+	MetricsPollMs  int               `yaml:"metrics_poll_ms"`
-+	Queue          SchedulerQueue    `yaml:"queue"`
-+	AckListener    AckListenerConfig `yaml:"ack_listener"`
-+	SlowPath       SlowPathConfig    `yaml:"slow_path"`
-+	Watchdog       WatchdogConfig    `yaml:"watchdog"`
-+	Dispatch       DispatchConfig    `yaml:"dispatch"`
- }
- 
- type SchedulerQueue struct {
-@@ -162,9 +160,6 @@ type WorkerConfig struct {
- 	ID                     string             `yaml:"id"`
- 	Addr                   string             `yaml:"addr"`
- 	Port                   int                `yaml:"port"`
--	EtcdEndpoints          []string           `yaml:"etcd_endpoints"`
--	EtcdDialTimeoutMs      int                `yaml:"etcd_dial_timeout_ms"`
--	EtcdLeaseTTLSec        int                `yaml:"etcd_lease_ttl_sec"`
- 	RedisURL               string             `yaml:"redis_url"`
- 	DatabaseURL            string             `yaml:"database_url"`
- 	Stream                 WorkerStreamConfig `yaml:"stream"`
-diff --git a/src/go/internal/scheduler/discovery.go b/src/go/internal/scheduler/discovery.go
-index cea3854..e8493c4 100644
---- a/src/go/internal/scheduler/discovery.go
-+++ b/src/go/internal/scheduler/discovery.go
-@@ -2,125 +2,150 @@ package scheduler
- 
- import (
- 	"context"
-+	"fmt"
- 	"log/slog"
--	"sync"
-+	"net"
-+	"net/url"
-+	"os"
-+	"strings"
- 	"sync/atomic"
- 	"time"
--
--	clientv3 "go.etcd.io/etcd/client/v3"
- )
- 
--const (
--	// WorkerPrefix Etcd 中 Worker 注册的前缀
--	WorkerPrefix = "/deep-oj/workers/"
--)
-+type workerEndpoint struct {
-+	id   string
-+	addr string
-+}
- 
--// EtcdDiscovery Etcd 服务发现
--type EtcdDiscovery struct {
--	client  *clientv3.Client
--	workers sync.Map // map[workerID]workerAddress
--	rrIndex uint64   // Round-Robin 计数器
-+// WorkerDiscovery 提供无 etcd 的工作节点发现。
-+// 支持两种环境变量：
-+// - WORKER_ADDR=host:port
-+// - WORKER_ADDRS=id1=host1:port1,id2=host2:port2 或 host1:port1,host2:port2
-+type WorkerDiscovery struct {
-+	workers      []workerEndpoint
-+	rrIndex      uint64
-+	probeTimeout time.Duration
- }
- 
--// NewEtcdDiscovery 创建 Etcd 服务发现实例
--func NewEtcdDiscovery(endpoints []string) (*EtcdDiscovery, error) {
--	dialMs := getEnvInt("ETCD_DIAL_TIMEOUT_MS", 5000)
--	client, err := clientv3.New(clientv3.Config{
--		Endpoints:   endpoints,
--		DialTimeout: time.Duration(dialMs) * time.Millisecond,
--	})
--	if err != nil {
--		return nil, err
-+func NewWorkerDiscovery() (*WorkerDiscovery, error) {
-+	rawWorkers := strings.TrimSpace(os.Getenv("WORKER_ADDRS"))
-+	if rawWorkers == "" {
-+		rawWorkers = strings.TrimSpace(os.Getenv("WORKER_ADDR"))
- 	}
--
--	return &EtcdDiscovery{
--		client: client,
-+	workers := parseWorkerEndpoints(rawWorkers)
-+	if len(workers) == 0 {
-+		slog.Warn("未配置可用工作节点，调度器将等待 worker 上线", "env", "WORKER_ADDR/WORKER_ADDRS")
-+	}
-+	probeMs := getEnvInt("WORKER_PROBE_TIMEOUT_MS", 1000)
-+	if probeMs <= 0 {
-+		probeMs = 1000
-+	}
-+	return &WorkerDiscovery{
-+		workers:      workers,
-+		probeTimeout: time.Duration(probeMs) * time.Millisecond,
- 	}, nil
- }
- 
--// Close 关闭 Etcd 连接
--func (d *EtcdDiscovery) Close() error {
--	return d.client.Close()
--}
--
--// WatchWorkers 监听 Worker 注册/注销事件
--func (d *EtcdDiscovery) WatchWorkers(ctx context.Context) {
--	slog.Info("正在监听工作节点", "prefix", WorkerPrefix)
--
--	// 1. 首先获取现有的 Workers
--	resp, err := d.client.Get(ctx, WorkerPrefix, clientv3.WithPrefix())
--	if err != nil {
--		slog.Error("获取初始工作节点失败", "error", err)
--	} else {
--		for _, kv := range resp.Kvs {
--			workerID := string(kv.Key)[len(WorkerPrefix):]
--			workerAddr := string(kv.Value)
--			d.workers.Store(workerID, workerAddr)
--			slog.Info("发现工作节点", "worker_id", workerID, "addr", workerAddr)
--		}
-+func parseWorkerEndpoints(raw string) []workerEndpoint {
-+	if strings.TrimSpace(raw) == "" {
-+		return nil
- 	}
--
--	// 2. 开始监听变化
--	watchChan := d.client.Watch(ctx, WorkerPrefix, clientv3.WithPrefix())
--
--	for resp := range watchChan {
--		for _, ev := range resp.Events {
--			workerID := string(ev.Kv.Key)[len(WorkerPrefix):]
--
--			switch ev.Type {
--			case clientv3.EventTypePut:
--				// Worker 注册或更新
--				workerAddr := string(ev.Kv.Value)
--				d.workers.Store(workerID, workerAddr)
--				slog.Info("工作节点已注册", "worker_id", workerID, "addr", workerAddr)
--
--			case clientv3.EventTypeDelete:
--				// Worker 注销 (Lease 过期或主动注销)
--				d.workers.Delete(workerID)
--				slog.Info("工作节点已注销", "worker_id", workerID)
--			}
-+	parts := strings.Split(raw, ",")
-+	workers := make([]workerEndpoint, 0, len(parts))
-+	seenIDs := make(map[string]struct{}, len(parts))
-+	for idx, part := range parts {
-+		part = strings.TrimSpace(part)
-+		if part == "" {
-+			continue
- 		}
-+		id := ""
-+		addr := part
-+		if strings.Contains(part, "=") {
-+			pair := strings.SplitN(part, "=", 2)
-+			id = strings.TrimSpace(pair[0])
-+			addr = strings.TrimSpace(pair[1])
-+		}
-+		if addr == "" {
-+			continue
-+		}
-+		if id == "" {
-+			id = fmt.Sprintf("worker-%d", idx+1)
-+		}
-+		if _, exists := seenIDs[id]; exists {
-+			id = fmt.Sprintf("%s-%d", id, idx+1)
-+		}
-+		seenIDs[id] = struct{}{}
-+		workers = append(workers, workerEndpoint{
-+			id:   id,
-+			addr: addr,
-+		})
- 	}
-+	return workers
- }
- 
--// GetNextWorker 使用 Round-Robin 获取下一个 Worker
--func (d *EtcdDiscovery) GetNextWorker() (string, string, bool) {
--	// 收集所有 Worker 信息
--	type workerInfo struct {
--		id   string
--		addr string
--	}
--	var workers []workerInfo
--	d.workers.Range(func(key, value interface{}) bool {
--		workers = append(workers, workerInfo{
--			id:   key.(string),
--			addr: value.(string),
--		})
--		return true
--	})
-+// Close 为接口兼容保留，当前无外部连接需要关闭。
-+func (d *WorkerDiscovery) Close() error {
-+	return nil
-+}
- 
--	if len(workers) == 0 {
-+// WatchWorkers 在无 etcd 模式下仅阻塞等待退出信号，避免调用方改动。
-+func (d *WorkerDiscovery) WatchWorkers(ctx context.Context) {
-+	<-ctx.Done()
-+}
-+
-+// GetNextWorker 使用 Round-Robin 获取下一个 worker。
-+func (d *WorkerDiscovery) GetNextWorker() (string, string, bool) {
-+	if len(d.workers) == 0 {
- 		return "", "", false
- 	}
--
--	// 原子递增计数器，取模得到索引
- 	idx := atomic.AddUint64(&d.rrIndex, 1)
--	w := workers[idx%uint64(len(workers))]
-+	w := d.workers[(idx-1)%uint64(len(d.workers))]
- 	return w.id, w.addr, true
- }
- 
--// GetWorkerCount 获取当前 Worker 数量
--func (d *EtcdDiscovery) GetWorkerCount() int {
--	count := 0
--	d.workers.Range(func(key, value interface{}) bool {
--		count++
--		return true
--	})
--	return count
-+func (d *WorkerDiscovery) GetWorkerCount() int {
-+	return len(d.workers)
-+}
-+
-+// IsWorkerActive 通过 TCP 探活判断 worker 是否在线。
-+func (d *WorkerDiscovery) IsWorkerActive(workerID string) bool {
-+	for _, w := range d.workers {
-+		if w.id == workerID {
-+			return probeTCPAddress(w.addr, d.probeTimeout)
-+		}
-+	}
-+	return false
- }
- 
--// IsWorkerActive 检查 Worker 是否在线
--func (d *EtcdDiscovery) IsWorkerActive(workerID string) bool {
--	_, ok := d.workers.Load(workerID)
--	return ok
-+func probeTCPAddress(rawAddr string, timeout time.Duration) bool {
-+	addr := normalizeDialAddress(rawAddr)
-+	if addr == "" {
-+		return false
-+	}
-+	conn, err := net.DialTimeout("tcp", addr, timeout)
-+	if err != nil {
-+		return false
-+	}
-+	if err := conn.Close(); err != nil {
-+		slog.Debug("关闭探活连接失败", "addr", addr, "error", err)
-+	}
-+	return true
-+}
-+
-+func normalizeDialAddress(raw string) string {
-+	addr := strings.TrimSpace(raw)
-+	if addr == "" {
-+		return ""
-+	}
-+	if strings.Contains(addr, "://") {
-+		u, err := url.Parse(addr)
-+		if err != nil {
-+			return ""
-+		}
-+		if u.Host != "" {
-+			return strings.TrimSpace(u.Host)
-+		}
-+		return ""
-+	}
-+	return addr
- }
-diff --git a/src/go/internal/scheduler/discovery_test.go b/src/go/internal/scheduler/discovery_test.go
-index e77af12..53b12b0 100644
---- a/src/go/internal/scheduler/discovery_test.go
-+++ b/src/go/internal/scheduler/discovery_test.go
-@@ -5,36 +5,26 @@ import (
- )
- 
- func TestGetNextWorker_RoundRobin(t *testing.T) {
--	// Initialize EtcdDiscovery
--	d := &EtcdDiscovery{}
--
--	// Populate sync.Map
--	d.workers.Store("worker-1", "127.0.0.1:5001")
--	d.workers.Store("worker-2", "127.0.0.1:5002")
--
--	// Note: sync.Map Range order is not guaranteed to be consistent or sorted.
--	// However, usually for small set it might be stable enough for simple test,
--	// OR GetNextWorker implementation collects them.
--	// The implementation collects them into a slice. The order of accumulation depends on Range.
--	// sync.Map Range order is random-ish.
--	// So Round-Robin test is tricky if order changes.
--	// But Round-Robin just means "next index".
--	// If the slice content changes order, RR index might preserve "fairness" but not strict "1 then 2".
--	// Let's verify that we get *A* worker, then *Another* worker (if we only have 2 and call 2 times? No).
--	// Actually, if Range order changes, RR is unpredictable.
--	// But let's assume for this test we trigger it enough times to cover.
-+	d := &WorkerDiscovery{
-+		workers: []workerEndpoint{
-+			{id: "worker-1", addr: "127.0.0.1:5001"},
-+			{id: "worker-2", addr: "127.0.0.1:5002"},
-+		},
-+	}
- 
--	// Let's just check that it returns valid workers.
--	id1, _, _ := d.GetNextWorker()
--	id2, _, _ := d.GetNextWorker()
-+	id1, _, ok1 := d.GetNextWorker()
-+	id2, _, ok2 := d.GetNextWorker()
- 
--	if id1 == "" || id2 == "" {
--		t.Error("Returned empty worker ID")
-+	if !ok1 || !ok2 {
-+		t.Fatalf("expected round robin to return available workers")
-+	}
-+	if id1 != "worker-1" || id2 != "worker-2" {
-+		t.Fatalf("unexpected round robin order: got %s then %s", id1, id2)
- 	}
- }
- 
- func TestGetNextWorker_Empty(t *testing.T) {
--	d := &EtcdDiscovery{}
-+	d := &WorkerDiscovery{}
- 
- 	_, _, ok := d.GetNextWorker()
- 	if ok {
-diff --git a/src/go/internal/scheduler/metrics.go b/src/go/internal/scheduler/metrics.go
-index b996b29..c375e30 100644
---- a/src/go/internal/scheduler/metrics.go
-+++ b/src/go/internal/scheduler/metrics.go
-@@ -38,7 +38,7 @@ var (
- 	schedulerActiveWorkers = prometheus.NewGauge(
- 		prometheus.GaugeOpts{
- 			Name: "scheduler_active_workers",
--			Help: "Etcd 中活跃的工作节点数",
-+			Help: "当前发现到的活跃工作节点数",
- 		},
- 	)
- 
-@@ -99,7 +99,7 @@ func SetLegacyLoopsStarted(count int) {
- }
- 
- // StartMetricsPoller starts a background loop to update Gauge metrics
--func StartMetricsPoller(ctx context.Context, redis *repository.RedisClient, discovery *EtcdDiscovery) {
-+func StartMetricsPoller(ctx context.Context, redis *repository.RedisClient, discovery *WorkerDiscovery) {
- 	pollMs := getEnvInt("SCHEDULER_METRICS_POLL_INTERVAL_MS", 1000)
- 	ticker := time.NewTicker(time.Duration(pollMs) * time.Millisecond)
- 	defer ticker.Stop()
-@@ -133,7 +133,7 @@ func updateQueueMetrics(ctx context.Context, redis *repository.RedisClient) {
- 	}
- }
- 
--func updateWorkerMetrics(ctx context.Context, discovery *EtcdDiscovery) {
-+func updateWorkerMetrics(ctx context.Context, discovery *WorkerDiscovery) {
- 	count := discovery.GetWorkerCount()
- 	schedulerActiveWorkers.Set(float64(count))
- }
-diff --git a/src/go/internal/scheduler/watchdog.go b/src/go/internal/scheduler/watchdog.go
-index 2827335..6edc09e 100644
---- a/src/go/internal/scheduler/watchdog.go
-+++ b/src/go/internal/scheduler/watchdog.go
-@@ -23,13 +23,13 @@ const (
- // Watchdog 负责扫描处理中队列，回收僵尸任务。
- type Watchdog struct {
- 	redisClient *repository.RedisClient
--	discovery   *EtcdDiscovery
-+	discovery   *WorkerDiscovery
- 	db          *repository.PostgresDB
- 	interval    time.Duration
- }
- 
- // NewWatchdog 创建看门狗实例。
--func NewWatchdog(redisClient *repository.RedisClient, discovery *EtcdDiscovery, db *repository.PostgresDB, interval time.Duration) *Watchdog {
-+func NewWatchdog(redisClient *repository.RedisClient, discovery *WorkerDiscovery, db *repository.PostgresDB, interval time.Duration) *Watchdog {
- 	return &Watchdog{
- 		redisClient: redisClient,
- 		discovery:   discovery,
-diff --git a/src/go/internal/worker/config.go b/src/go/internal/worker/config.go
-index 87926d4..4cd2df5 100644
---- a/src/go/internal/worker/config.go
-+++ b/src/go/internal/worker/config.go
-@@ -15,7 +15,6 @@ import (
- type Config struct {
- 	WorkerID              string
- 	WorkerAddr            string // gRPC listen addr
--	EtcdEndpoints         []string
- 	RedisURL              string
- 	DatabaseURL           string
- 	MinIOEndpoint         string
-@@ -163,20 +162,6 @@ func LoadConfig() *Config {
- 	cfg.UnzipMaxFileBytes = getEnvInt64("UNZIP_MAX_FILE_BYTES", 64*1024*1024)
- 	cfg.AllowHostChecker = getEnvBool("ALLOW_HOST_CHECKER", false)
- 
--	if endpoints := getEnv("ETCD_ENDPOINTS", "localhost:2379"); endpoints != "" {
--		parts := strings.Split(endpoints, ",")
--		for _, p := range parts {
--			p = strings.TrimSpace(p)
--			if p == "" {
--				continue
--			}
--			cfg.EtcdEndpoints = append(cfg.EtcdEndpoints, p)
--		}
--		if len(cfg.EtcdEndpoints) == 0 {
--			cfg.EtcdEndpoints = []string{"localhost:2379"}
--		}
--	}
--
- 	return cfg
- }
- 
-diff --git a/scripts/verify_b1_no_etcd.sh b/scripts/verify_b1_no_etcd.sh
-new file mode 100755
-index 0000000..9722026
---- /dev/null
-+++ b/scripts/verify_b1_no_etcd.sh
-@@ -0,0 +1,133 @@
-+#!/usr/bin/env bash
-+set -euo pipefail
-+
-+ROOT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
-+cd "$ROOT_DIR"
-+
-+KEEP_TMP="${KEEP_TMP:-0}"
-+TMP_DIR="$(mktemp -d)"
-+
-+cleanup() {
-+  if [[ "$KEEP_TMP" == "1" ]]; then
-+    echo "KEEP_TMP=1, 保留临时目录: $TMP_DIR" >&2
-+    return
-+  fi
-+  rm -rf "$TMP_DIR"
-+}
-+trap cleanup EXIT
-+
-+compose_service_exists() {
-+  local service="$1"
-+  local svc
-+  while IFS= read -r svc; do
-+    [[ "$svc" == "$service" ]] && return 0
-+  done < <(docker compose config --services 2>/dev/null || true)
-+  return 1
-+}
-+
-+emit_diagnostics() {
-+  echo >&2
-+  echo "========== diagnostics ==========" >&2
-+  echo "[docker compose ps]" >&2
-+  docker compose ps >&2 || true
-+
-+  for service in api worker scheduler; do
-+    if compose_service_exists "$service"; then
-+      echo "[docker compose logs --tail=200 $service]" >&2
-+      docker compose logs --tail=200 "$service" >&2 || true
-+    fi
-+  done
-+  echo "=================================" >&2
-+}
-+
-+fatal() {
-+  local msg="$1"
-+  echo "ERROR: $msg" >&2
-+  emit_diagnostics
-+  exit 1
-+}
-+
-+require_cmd() {
-+  local cmd="$1"
-+  if ! command -v "$cmd" >/dev/null 2>&1; then
-+    fatal "缺少命令: $cmd"
-+  fi
-+}
-+
-+assert_no_etcd_service_defined() {
-+  if docker compose config --services 2>/dev/null | grep -qx 'etcd'; then
-+    fatal "compose 配置中仍存在 etcd 服务"
-+  fi
-+}
-+
-+assert_no_etcd_service_running() {
-+  if docker compose ps --services 2>/dev/null | grep -qx 'etcd'; then
-+    fatal "compose 运行列表中存在 etcd 服务"
-+  fi
-+  if docker compose ps 2>/dev/null | grep -Eiq '(^|[[:space:]])(etcd|oj-etcd)([[:space:]]|$)'; then
-+    fatal "compose ps 输出中检测到 etcd/oj-etcd"
-+  fi
-+}
-+
-+run_script_step() {
-+  local label="$1"
-+  local script="$2"
-+  echo "$label"
-+  if [[ ! -f "$script" ]]; then
-+    fatal "脚本不存在: $script"
-+  fi
-+  if ! bash "$script"; then
-+    fatal "脚本执行失败: $script"
-+  fi
-+}
-+
-+run_optional_g1() {
-+  if [[ ! -f scripts/verify_g1_kill_all.sh ]]; then
-+    echo "[7/8] 跳过 G1：未找到 scripts/verify_g1_kill_all.sh"
-+    return 0
-+  fi
-+
-+  if [[ -f scripts/verify_g1_prereq.sh ]]; then
-+    echo "[7/8] 检查 G1 运行前置条件"
-+    if ! bash scripts/verify_g1_prereq.sh; then
-+      echo "[7/8] 跳过 G1：当前环境不满足前置条件" >&2
-+      return 0
-+    fi
-+  else
-+    echo "[7/8] 未找到 verify_g1_prereq.sh，按可运行处理并直接执行 G1"
-+  fi
-+
-+  echo "[7/8] 执行 scripts/verify_g1_kill_all.sh"
-+  if ! bash scripts/verify_g1_kill_all.sh; then
-+    fatal "脚本执行失败: scripts/verify_g1_kill_all.sh"
-+  fi
-+}
-+
-+require_cmd docker
-+require_cmd bash
-+
-+export JWT_SECRET="${JWT_SECRET:-dev_jwt_secret_change_me}"
-+export ADMIN_USERS="${ADMIN_USERS:-admin}"
-+export REDIS_PASSWORD="${REDIS_PASSWORD:-deepoj_redis_change_me}"
-+export POSTGRES_PASSWORD="${POSTGRES_PASSWORD:-deepoj_pg_password_change_me}"
-+export MINIO_ROOT_USER="${MINIO_ROOT_USER:-deepoj_minio_user}"
-+export MINIO_ROOT_PASSWORD="${MINIO_ROOT_PASSWORD:-deepoj_minio_password_change_me}"
-+export WORKER_AUTH_TOKEN="${WORKER_AUTH_TOKEN:-deepoj_worker_token_change_me}"
-+
-+echo "[1/8] docker compose up -d --build"
-+assert_no_etcd_service_defined
-+if ! docker compose up -d --build; then
-+  fatal "docker compose up 失败"
-+fi
-+
-+echo "[2/8] docker compose ps 断言不存在 etcd"
-+assert_no_etcd_service_defined
-+assert_no_etcd_service_running
-+
-+run_script_step "[3/8] 执行 scripts/verify_mvp2_e2e.sh" "scripts/verify_mvp2_e2e.sh"
-+run_script_step "[4/8] 执行 scripts/verify_mvp3_crash_recover.sh" "scripts/verify_mvp3_crash_recover.sh"
-+run_script_step "[5/8] 执行 scripts/verify_mvp4_observability.sh" "scripts/verify_mvp4_observability.sh"
-+run_script_step "[6/8] 执行 scripts/verify_ci.sh" "scripts/verify_ci.sh"
-+run_optional_g1
-+
-+echo "[8/8] 无 etcd 启动验证通过"
-diff --git a/scripts/verify_ci.sh b/scripts/verify_ci.sh
-new file mode 100755
-index 0000000..ed7ebd6
---- /dev/null
-+++ b/scripts/verify_ci.sh
-@@ -0,0 +1,33 @@
-+#!/usr/bin/env bash
-+set -euo pipefail
-+
-+ROOT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
-+cd "$ROOT_DIR"
-+
-+if ! command -v go >/dev/null 2>&1; then
-+  echo "ERROR: 未找到 go 命令" >&2
-+  exit 2
-+fi
-+if ! command -v bash >/dev/null 2>&1; then
-+  echo "ERROR: 未找到 bash 命令" >&2
-+  exit 2
-+fi
-+
-+echo "[1/3] go test ./..."
-+(
-+  cd src/go
-+  go test ./...
-+)
-+
-+echo "[2/3] go vet ./..."
-+(
-+  cd src/go
-+  go vet ./...
-+)
-+
-+echo "[3/3] 脚本语法检查（bash -n）"
-+while IFS= read -r script; do
-+  bash -n "$script"
-+done < <(find scripts -maxdepth 1 -type f -name '*.sh' | sort)
-+
-+echo "verify_ci: PASS"
diff --git a/config.yaml b/config.yaml
index 90353ff..695f75f 100644
--- a/config.yaml
+++ b/config.yaml
@@ -98,42 +98,9 @@ api:
 scheduler:
   redis_url: "localhost:6379"
   database_url: "postgres://deep_oj:change_me@localhost:5432/deep_oj?sslmode=disable"
-  worker_capacity: 4
-  max_retry: 3
-  retry_ttl_sec: 86400
   scheduler_id: "scheduler-1"
-  dispatch_mode: "streams_only" # streams_only | legacy_grpc_push
-  dispatch_enabled: false       # legacy_grpc_push requires true
   metrics_port: 9091
   metrics_poll_ms: 1000
-  queue:
-    brpop_timeout_sec: 5
-    no_worker_sleep_ms: 1000
-    assignment_ttl_sec: 600
-    payload_ttl_sec: 1800
-    processing_start_ttl_sec: 1800
-    inflight_ttl_sec: 1800
-  ack_listener:
-    pending_count: 20
-    pending_block_ms: 1000
-    new_count: 10
-    new_block_ms: 5000
-  slow_path:
-    tick_sec: 60
-    processing_cutoff_sec: 30
-    pending_stale_sec: 60
-    db_scan_limit: 100
-  watchdog:
-    interval_sec: 5
-  dispatch:
-    conn_timeout_ms: 3000
-    rpc_timeout_ms: 5000
-    max_retries: 3
-    backoff_base_ms: 100
-  grpc_tls:
-    cert: ""
-    key: ""
-    ca: ""
   metrics:
     service_name: "deep-oj-scheduler"
     instance_id: ""
@@ -168,8 +135,6 @@ worker:
   result_ttl_sec: 600
   checker_timeout_ms: 2000
   cleanup_timeout_sec: 5
-  result_stream_max_retries: 3
-  result_stream_backoff_ms: 100
   timeouts:
     compile_ms: 10000
     exec_buffer_ms: 500
@@ -184,10 +149,6 @@ worker:
     max_file_bytes: 67108864 # 64MB
   allow_host_checker: false
   require_cgroups_v2: false
-  grpc_tls:
-    cert: ""
-    key: ""
-    ca: ""
   metrics:
     service_name: "deep-oj-worker"
     instance_id: ""
diff --git a/docker-compose.yml b/docker-compose.yml
index ea1e314..c144449 100644
--- a/docker-compose.yml
+++ b/docker-compose.yml
@@ -24,8 +24,6 @@ services:
       - SETGID
     security_opt:
       - no-new-privileges:true
-    expose:
-      - "50051"
     volumes:
       - /sys/fs/cgroup:/sys/fs/cgroup:rw
       - ./data/workspace:/data/workspace:rw
@@ -47,8 +45,6 @@ services:
       - JOB_RECLAIM_GRACE_SEC=15
       - JUDGER_BIN=/app/judge_engine
       - WORKSPACE=/data/workspace
-      - WORKER_AUTH_TOKEN=${WORKER_AUTH_TOKEN:-deepoj_worker_token_change_me}
-      - ALLOW_INSECURE_WORKER_GRPC=true
     networks:
       - deep-oj-net
     depends_on:
@@ -63,16 +59,12 @@ services:
     restart: on-failure
     user: deep_oj
     command: /app/oj_scheduler
-    ports:
-      - "50052:50052"
     environment:
       - REDIS_URL=oj-redis:6379
       - REDIS_PASSWORD=${REDIS_PASSWORD:-deepoj_redis_change_me}
       - WORKER_ADDR=oj-worker:50051
       - PGPASSWORD=${POSTGRES_PASSWORD:-deepoj_pg_password_change_me}
       - DATABASE_URL=postgres://deep_oj:${POSTGRES_PASSWORD:-deepoj_pg_password_change_me}@oj-postgres:5432/deep_oj?sslmode=disable
-      - WORKER_AUTH_TOKEN=${WORKER_AUTH_TOKEN:-deepoj_worker_token_change_me}
-      - ALLOW_INSECURE_WORKER_GRPC=true
     networks:
       - deep-oj-net
     depends_on:
@@ -94,7 +86,6 @@ services:
       - PORT=18080
       - REDIS_URL=oj-redis:6379
       - REDIS_PASSWORD=${REDIS_PASSWORD:-deepoj_redis_change_me}
-      - SCHEDULER_ADDR=oj-scheduler:50052
       - PGPASSWORD=${POSTGRES_PASSWORD:-deepoj_pg_password_change_me}
       - DATABASE_URL=postgres://deep_oj:${POSTGRES_PASSWORD:-deepoj_pg_password_change_me}@oj-postgres:5432/deep_oj?sslmode=disable
       - MINIO_ENDPOINT=http://oj-minio:9000
diff --git a/docs/REPO_SURVEY.md b/docs/REPO_SURVEY.md
index bc0ac86..4a61878 100644
--- a/docs/REPO_SURVEY.md
+++ b/docs/REPO_SURVEY.md
@@ -1,115 +1,38 @@
-# Deep-OJ 仓库侦察报告（A1）
-
-更新时间：2026-02-14  
-范围：当前主干代码（Go API/Scheduler/Worker + SQL migrations + docs）
+# REPO_SURVEY（B2 后）
 
 ## 1. 调研方法与范围
 
-使用的命令（可复现）：
-
-```bash
-rg -n "BRPopLPush|XReadGroup|XAck|XAdd|queue:|stream:|processing" src/go -g'*.go'
-rg -n "CREATE TABLE|ALTER TABLE|FOREIGN KEY|INDEX" sql/migrations -g'*.sql'
-rg -n "prometheus|CounterVec|Histogram|Gauge|slog" src/go -g'*.go'
-```
-
-重点代码路径：
-
-- API 提交入口：`src/go/internal/api/handler.go`
-- Scheduler 主循环：`src/go/cmd/scheduler/main.go`
-- 结果确认：`src/go/internal/scheduler/ack_listener.go`
-- Worker 执行与回传：`src/go/internal/worker/judge.go`
-- DB 访问：`src/go/internal/repository/postgres.go`
-- Redis 访问：`src/go/internal/repository/redis.go`
+- 代码检索：`rg -n "XADD|XREADGROUP|XACK|XAUTOCLAIM|outbox|stream" src/go scripts docs`
+- 关键入口：
+  - `src/go/internal/api/outbox_dispatcher.go`
+  - `src/go/internal/worker/stream_consumer.go`
+  - `src/go/cmd/scheduler/main.go`
+- 只覆盖当前主链路：API(outbox) -> Stream -> Worker -> DB finalize -> XACK/reclaim。
 
 ## 2. 队列实现现状
 
-### 2.1 Redis key/stream 现状
-
-- List 主队列：`queue:pending`
-- List 处理中队列：`queue:processing`
-- List 死信队列：`queue:dead`
-- Result cache：`result:{job_id}`
-- Result stream：`stream:results`（group: `results-group`）
-- 任务辅助键：`task:worker:{job_id}`、`task:payload:{job_id}`、`task:processing_start:{job_id}`、`task:processing:zset`
-
-### 2.2 执行链路（当前）
-
-1. API 在 `src/go/internal/api/handler.go` 写 DB `submissions`（`state=pending`）后，`LPush queue:pending`。  
-2. Scheduler 在 `src/go/cmd/scheduler/main.go` 用 `BRPopLPush(queue:pending -> queue:processing)` 拉取任务。  
-3. Scheduler 通过 gRPC 调用 Worker，记录 assignment/payload/processing_start/zset。  
-4. Worker 在 `src/go/internal/worker/judge.go` 执行后：`SETNX result:{job_id}` + `XADD stream:results`。  
-5. ACK Listener 在 `src/go/internal/scheduler/ack_listener.go` 用 `XReadGroup` 消费 `stream:results`，更新 DB 后 `XAck`。  
-6. ACK 成功后清理 `queue:processing` 对应元素与辅助键。  
-
-结论：当前是“List 作为任务主队列 + Stream 作为结果回传”，尚未切换到“Streams + Consumer Group 作为主任务队列”。
+- 当前唯一数据面是 Redis Stream：`deepoj:jobs`。
+- API 端通过 outbox 分发器把事件写入 `deepoj:jobs`：`src/go/internal/api/outbox_dispatcher.go`。
+- Worker 端通过消费组读取：`src/go/internal/worker/stream_consumer.go`（`XREADGROUP` + `XAUTOCLAIM`）。
+- Scheduler 已不承载派单数据面，只保留控制面与指标：`src/go/cmd/scheduler/main.go`、`src/go/internal/scheduler/metrics.go`。
 
 ## 3. PostgreSQL Schema 现状
 
-迁移文件：
-
-- `sql/migrations/001_init.sql`：`submissions` 基表
-- `sql/migrations/002_add_problems_table.sql`：`problems`
-- `sql/migrations/003_add_users_table.sql`：`users`
-- `sql/migrations/004_add_submission_state_and_indexes.sql`：新增 `submissions.state`
-- `sql/migrations/005_add_submission_foreign_keys.sql`：`submissions -> problems/users` FK
-
-当前核心表：`submissions`
-
-- 主键：`id BIGSERIAL`
-- 业务键：`job_id VARCHAR(64) UNIQUE`
-- 判题字段：`status VARCHAR(32)`, `state TEXT`, `result JSONB`
-- 资源限制字段：`time_limit`, `memory_limit`
-- 关联字段：`problem_id`, `user_id`
-
-当前完成写入口径（代码现状）：
-
-- `src/go/internal/repository/postgres.go` 使用  
-  `UPDATE submissions SET ... state='done' WHERE job_id=$1 AND state != 'done'`
-- 该实现有基本幂等能力，但尚未引入 `attempt_id` / `RUNNING` CAS fencing（后续任务处理）。
+- 基础表结构来源：`sql/migrations/001_init.sql`。
+- Outbox 表结构来源：`sql/migrations/007_add_outbox_events.sql`。
+- 提交流程主仓储：`src/go/internal/repository/postgres.go`。
+- Outbox 仓储：`src/go/internal/repository/postgres_outbox.go`。
+- 关键表：`submissions`、`outbox_events`。
 
 ## 4. Worker 执行链路现状
 
-入口与流程：
-
-- gRPC 入口：`src/go/cmd/worker/main.go` 注册 `JudgeService.ExecuteTask`
-- 任务执行：`src/go/internal/worker/judge.go`
-- 执行器调用 C++：`src/go/internal/worker/executor.go`
-
-关键步骤：
-
-1. `ExecuteTask` 先做并发信号量限流（`PoolSize`）。  
-2. 校验 `job_id` 白名单格式（正则）并准备 testcase。  
-3. 调 C++ core 编译；逐测试点执行并比对输出。  
-4. 汇总结果后 `reportResult`：写 `result:{job_id}`，再写 `stream:results`。  
-5. 最后触发 cleanup（失败仅 warn，不中断主流程）。  
+- 流消息读取与 claim/reclaim：`src/go/internal/worker/stream_consumer.go`。
+- 执行器调用链：`src/go/internal/worker/judge.go`。
+- 最终写入采用 fenced finalize，成功后 `XACK`；异常由 reclaim 继续处理。
 
 ## 5. 可观测性现状
 
-Metrics 文件：
-
-- API：`src/go/internal/api/metrics.go`
-- Scheduler：`src/go/internal/scheduler/metrics.go`
-- Worker：`src/go/internal/worker/metrics.go`
-
-已确认的标签：
-
-- API：`method/path/status`、`language/status`
-- Scheduler：`queue`、`status/language`
-- Worker：`status`
-
-现状结论：
-
-- 指标定义中未使用 `job_id` label（符合“metrics 禁止 job_id label”约束）。
-- 日志广泛使用 `slog`，链路关键日志带 `job_id` 和 `trace_id`（如 `handler.go`, `main.go`, `judge.go`）。
-- 当前尚未看到统一日志采样/限流层；后续任务可补齐。
-
-## 6. 现场探针脚本
-
-已提供可复现脚本：`scripts/repo_survey_probe.sh`
-
-用途：
-
-- 采样 `queue:pending` / `queue:processing` / `stream:results` 深度
-- 打印 PostgreSQL 表与 `submissions` 列定义
-- 抽样 API/Scheduler/Worker `/metrics`
+- API 指标定义：`src/go/internal/api/metrics.go`。
+- Scheduler 指标定义：`src/go/internal/scheduler/metrics.go`（控制面指标）。
+- Worker 指标定义：`src/go/internal/worker/metrics.go`。
+- 采样脚本：`scripts/repo_survey_probe.sh`。
diff --git a/docs/runbook.md b/docs/runbook.md
index 50aef43..3261e92 100644
--- a/docs/runbook.md
+++ b/docs/runbook.md
@@ -1,26 +1,26 @@
 # 运维 Runbook（简版）
 
-## 1. Worker 无响应 / 任务堆积
-1. 查看 `queue:processing` 长度与 `processing:zset` 超时任务
-2. 检查 Worker 进程与 gRPC 端口是否存活
-3. 如 Worker 崩溃：Watchdog/SlowPath 会自动重入队
+## 1. Stream backlog 持续增长
+1. 查看 `deepoj:jobs` 长度与消费组 pending：`XLEN` / `XPENDING`
+2. 检查 Worker 进程是否正常、`XREADGROUP` 是否持续消费
+3. 必要时扩容 Worker 或限流提交
 
 ## 2. Redis 异常
-1. 检查 Redis 连接与慢查询
-2. 如 Redis 不可用：提交与队列暂停（API 返回 5xx）
-3. 恢复后：SlowPath 扫描 DB 重建队列
+1. 检查 Redis 连通性与认证配置
+2. Redis 不可用时，提交与消费都会受影响
+3. 恢复后由 outbox + reclaim 机制继续推进
 
 ## 3. MinIO 不可用
-1. Worker 下载测试数据失败 -> 任务失败
-2. 检查 MinIO 状态与桶权限
-3. 恢复后重试任务（重入队）
+1. Worker 下载测试数据失败，任务会进入错误分支
+2. 检查 MinIO 状态、桶权限与对象完整性
+3. 恢复后重试任务
 
-## 4. 判题结果丢失
-1. 检查 Redis Stream `stream:results` 与消费者组
-2. 确认 DB 是否已写入 `state=done`
-3. 如 Redis 丢失：DB 仍为权威结果
+## 4. 判题状态卡住
+1. 检查 Worker 日志中的 `db_finalize_*` 与 `xack_*`
+2. 校验 `submissions` 的 `state/status/attempt_id`
+3. 如存在 PEL 堆积，观察 reclaim 是否在推进
 
 ## 5. Cgroups v2 未启用
 1. 宿主机启用 cgroup v2（内核参数）
-2. Docker 启动时挂载 `/sys/fs/cgroup`
+2. Docker 挂载 `/sys/fs/cgroup`
 3. Worker 设置 `REQUIRE_CGROUPS_V2=1`
diff --git a/proto/judge.pb.go b/proto/judge.pb.go
deleted file mode 100644
index e0e8eab..0000000
--- a/proto/judge.pb.go
+++ /dev/null
@@ -1,540 +0,0 @@
-// Code generated by protoc-gen-go. DO NOT EDIT.
-// versions:
-// 	protoc-gen-go v1.36.11
-// 	protoc        v3.21.12
-// source: proto/judge.proto
-
-package proto
-
-import (
-	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
-	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
-	reflect "reflect"
-	sync "sync"
-	unsafe "unsafe"
-)
-
-const (
-	// Verify that this generated code is sufficiently up-to-date.
-	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
-	// Verify that runtime/protoimpl is sufficiently up-to-date.
-	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
-)
-
-type Language int32
-
-const (
-	Language_LANGUAGE_UNSPECIFIED Language = 0
-	Language_CPP                  Language = 1
-	Language_JAVA                 Language = 2
-	Language_PYTHON               Language = 3
-	Language_GO                   Language = 4
-)
-
-// Enum value maps for Language.
-var (
-	Language_name = map[int32]string{
-		0: "LANGUAGE_UNSPECIFIED",
-		1: "CPP",
-		2: "JAVA",
-		3: "PYTHON",
-		4: "GO",
-	}
-	Language_value = map[string]int32{
-		"LANGUAGE_UNSPECIFIED": 0,
-		"CPP":                  1,
-		"JAVA":                 2,
-		"PYTHON":               3,
-		"GO":                   4,
-	}
-)
-
-func (x Language) Enum() *Language {
-	p := new(Language)
-	*p = x
-	return p
-}
-
-func (x Language) String() string {
-	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
-}
-
-func (Language) Descriptor() protoreflect.EnumDescriptor {
-	return file_proto_judge_proto_enumTypes[0].Descriptor()
-}
-
-func (Language) Type() protoreflect.EnumType {
-	return &file_proto_judge_proto_enumTypes[0]
-}
-
-func (x Language) Number() protoreflect.EnumNumber {
-	return protoreflect.EnumNumber(x)
-}
-
-// Deprecated: Use Language.Descriptor instead.
-func (Language) EnumDescriptor() ([]byte, []int) {
-	return file_proto_judge_proto_rawDescGZIP(), []int{0}
-}
-
-type JudgeStatus int32
-
-const (
-	JudgeStatus_STATUS_UNSPECIFIED    JudgeStatus = 0
-	JudgeStatus_ACCEPTED              JudgeStatus = 1 // AC
-	JudgeStatus_WRONG_ANSWER          JudgeStatus = 2 // WA
-	JudgeStatus_COMPILE_ERROR         JudgeStatus = 3 // CE
-	JudgeStatus_TIME_LIMIT_EXCEEDED   JudgeStatus = 4 // TLE
-	JudgeStatus_MEMORY_LIMIT_EXCEEDED JudgeStatus = 5 // MLE
-	JudgeStatus_RUNTIME_ERROR         JudgeStatus = 6 // RE
-	JudgeStatus_SYSTEM_ERROR          JudgeStatus = 7 // SE
-)
-
-// Enum value maps for JudgeStatus.
-var (
-	JudgeStatus_name = map[int32]string{
-		0: "STATUS_UNSPECIFIED",
-		1: "ACCEPTED",
-		2: "WRONG_ANSWER",
-		3: "COMPILE_ERROR",
-		4: "TIME_LIMIT_EXCEEDED",
-		5: "MEMORY_LIMIT_EXCEEDED",
-		6: "RUNTIME_ERROR",
-		7: "SYSTEM_ERROR",
-	}
-	JudgeStatus_value = map[string]int32{
-		"STATUS_UNSPECIFIED":    0,
-		"ACCEPTED":              1,
-		"WRONG_ANSWER":          2,
-		"COMPILE_ERROR":         3,
-		"TIME_LIMIT_EXCEEDED":   4,
-		"MEMORY_LIMIT_EXCEEDED": 5,
-		"RUNTIME_ERROR":         6,
-		"SYSTEM_ERROR":          7,
-	}
-)
-
-func (x JudgeStatus) Enum() *JudgeStatus {
-	p := new(JudgeStatus)
-	*p = x
-	return p
-}
-
-func (x JudgeStatus) String() string {
-	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
-}
-
-func (JudgeStatus) Descriptor() protoreflect.EnumDescriptor {
-	return file_proto_judge_proto_enumTypes[1].Descriptor()
-}
-
-func (JudgeStatus) Type() protoreflect.EnumType {
-	return &file_proto_judge_proto_enumTypes[1]
-}
-
-func (x JudgeStatus) Number() protoreflect.EnumNumber {
-	return protoreflect.EnumNumber(x)
-}
-
-// Deprecated: Use JudgeStatus.Descriptor instead.
-func (JudgeStatus) EnumDescriptor() ([]byte, []int) {
-	return file_proto_judge_proto_rawDescGZIP(), []int{1}
-}
-
-// 任务请求 (对应 Redis 里的 queue:pending 内容)
-type TaskRequest struct {
-	state protoimpl.MessageState `protogen:"open.v1"`
-	JobId string                 `protobuf:"bytes,1,opt,name=job_id,json=jobId,proto3" json:"job_id,omitempty"`
-	// 【关键修改】这里改成 bytes！
-	// 避免 string 导致的自动转义 (backslash bug)
-	// 接收端收到后：std::string code(req.code().begin(), req.code().end());
-	Code          []byte   `protobuf:"bytes,2,opt,name=code,proto3" json:"code,omitempty"`
-	Language      Language `protobuf:"varint,3,opt,name=language,proto3,enum=deep_oj.Language" json:"language,omitempty"`
-	TimeLimit     int32    `protobuf:"varint,4,opt,name=time_limit,json=timeLimit,proto3" json:"time_limit,omitempty"`       // ms
-	MemoryLimit   int32    `protobuf:"varint,5,opt,name=memory_limit,json=memoryLimit,proto3" json:"memory_limit,omitempty"` // KB
-	CacheKey      string   `protobuf:"bytes,6,opt,name=cache_key,json=cacheKey,proto3" json:"cache_key,omitempty"`
-	SubmitTime    int64    `protobuf:"varint,7,opt,name=submit_time,json=submitTime,proto3" json:"submit_time,omitempty"` // Unix timestamp (ms) for timeout detection
-	ProblemId     uint32   `protobuf:"varint,8,opt,name=problem_id,json=problemId,proto3" json:"problem_id,omitempty"`
-	TraceId       string   `protobuf:"bytes,9,opt,name=trace_id,json=traceId,proto3" json:"trace_id,omitempty"` // [New] Full-link Trace ID
-	unknownFields protoimpl.UnknownFields
-	sizeCache     protoimpl.SizeCache
-}
-
-func (x *TaskRequest) Reset() {
-	*x = TaskRequest{}
-	mi := &file_proto_judge_proto_msgTypes[0]
-	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
-	ms.StoreMessageInfo(mi)
-}
-
-func (x *TaskRequest) String() string {
-	return protoimpl.X.MessageStringOf(x)
-}
-
-func (*TaskRequest) ProtoMessage() {}
-
-func (x *TaskRequest) ProtoReflect() protoreflect.Message {
-	mi := &file_proto_judge_proto_msgTypes[0]
-	if x != nil {
-		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
-		if ms.LoadMessageInfo() == nil {
-			ms.StoreMessageInfo(mi)
-		}
-		return ms
-	}
-	return mi.MessageOf(x)
-}
-
-// Deprecated: Use TaskRequest.ProtoReflect.Descriptor instead.
-func (*TaskRequest) Descriptor() ([]byte, []int) {
-	return file_proto_judge_proto_rawDescGZIP(), []int{0}
-}
-
-func (x *TaskRequest) GetJobId() string {
-	if x != nil {
-		return x.JobId
-	}
-	return ""
-}
-
-func (x *TaskRequest) GetCode() []byte {
-	if x != nil {
-		return x.Code
-	}
-	return nil
-}
-
-func (x *TaskRequest) GetLanguage() Language {
-	if x != nil {
-		return x.Language
-	}
-	return Language_LANGUAGE_UNSPECIFIED
-}
-
-func (x *TaskRequest) GetTimeLimit() int32 {
-	if x != nil {
-		return x.TimeLimit
-	}
-	return 0
-}
-
-func (x *TaskRequest) GetMemoryLimit() int32 {
-	if x != nil {
-		return x.MemoryLimit
-	}
-	return 0
-}
-
-func (x *TaskRequest) GetCacheKey() string {
-	if x != nil {
-		return x.CacheKey
-	}
-	return ""
-}
-
-func (x *TaskRequest) GetSubmitTime() int64 {
-	if x != nil {
-		return x.SubmitTime
-	}
-	return 0
-}
-
-func (x *TaskRequest) GetProblemId() uint32 {
-	if x != nil {
-		return x.ProblemId
-	}
-	return 0
-}
-
-func (x *TaskRequest) GetTraceId() string {
-	if x != nil {
-		return x.TraceId
-	}
-	return ""
-}
-
-// 判题结果 (Worker 汇报给 API 的)
-type TaskResult struct {
-	state         protoimpl.MessageState `protogen:"open.v1"`
-	JobId         string                 `protobuf:"bytes,1,opt,name=job_id,json=jobId,proto3" json:"job_id,omitempty"`
-	Status        JudgeStatus            `protobuf:"varint,2,opt,name=status,proto3,enum=deep_oj.JudgeStatus" json:"status,omitempty"`
-	ErrorMessage  string                 `protobuf:"bytes,3,opt,name=error_message,json=errorMessage,proto3" json:"error_message,omitempty"` // CE 的报错信息
-	TimeUsed      int32                  `protobuf:"varint,4,opt,name=time_used,json=timeUsed,proto3" json:"time_used,omitempty"`            // ms
-	MemoryUsed    int64                  `protobuf:"varint,5,opt,name=memory_used,json=memoryUsed,proto3" json:"memory_used,omitempty"`      // KB
-	TraceId       string                 `protobuf:"bytes,6,opt,name=trace_id,json=traceId,proto3" json:"trace_id,omitempty"`                // [New] Return for logging
-	Language      Language               `protobuf:"varint,7,opt,name=language,proto3,enum=deep_oj.Language" json:"language,omitempty"`      // [New] For metrics
-	unknownFields protoimpl.UnknownFields
-	sizeCache     protoimpl.SizeCache
-}
-
-func (x *TaskResult) Reset() {
-	*x = TaskResult{}
-	mi := &file_proto_judge_proto_msgTypes[1]
-	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
-	ms.StoreMessageInfo(mi)
-}
-
-func (x *TaskResult) String() string {
-	return protoimpl.X.MessageStringOf(x)
-}
-
-func (*TaskResult) ProtoMessage() {}
-
-func (x *TaskResult) ProtoReflect() protoreflect.Message {
-	mi := &file_proto_judge_proto_msgTypes[1]
-	if x != nil {
-		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
-		if ms.LoadMessageInfo() == nil {
-			ms.StoreMessageInfo(mi)
-		}
-		return ms
-	}
-	return mi.MessageOf(x)
-}
-
-// Deprecated: Use TaskResult.ProtoReflect.Descriptor instead.
-func (*TaskResult) Descriptor() ([]byte, []int) {
-	return file_proto_judge_proto_rawDescGZIP(), []int{1}
-}
-
-func (x *TaskResult) GetJobId() string {
-	if x != nil {
-		return x.JobId
-	}
-	return ""
-}
-
-func (x *TaskResult) GetStatus() JudgeStatus {
-	if x != nil {
-		return x.Status
-	}
-	return JudgeStatus_STATUS_UNSPECIFIED
-}
-
-func (x *TaskResult) GetErrorMessage() string {
-	if x != nil {
-		return x.ErrorMessage
-	}
-	return ""
-}
-
-func (x *TaskResult) GetTimeUsed() int32 {
-	if x != nil {
-		return x.TimeUsed
-	}
-	return 0
-}
-
-func (x *TaskResult) GetMemoryUsed() int64 {
-	if x != nil {
-		return x.MemoryUsed
-	}
-	return 0
-}
-
-func (x *TaskResult) GetTraceId() string {
-	if x != nil {
-		return x.TraceId
-	}
-	return ""
-}
-
-func (x *TaskResult) GetLanguage() Language {
-	if x != nil {
-		return x.Language
-	}
-	return Language_LANGUAGE_UNSPECIFIED
-}
-
-// 空响应 (占位符)
-type TaskResponse struct {
-	state         protoimpl.MessageState `protogen:"open.v1"`
-	Message       string                 `protobuf:"bytes,1,opt,name=message,proto3" json:"message,omitempty"` // [Fix] Add message field
-	unknownFields protoimpl.UnknownFields
-	sizeCache     protoimpl.SizeCache
-}
-
-func (x *TaskResponse) Reset() {
-	*x = TaskResponse{}
-	mi := &file_proto_judge_proto_msgTypes[2]
-	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
-	ms.StoreMessageInfo(mi)
-}
-
-func (x *TaskResponse) String() string {
-	return protoimpl.X.MessageStringOf(x)
-}
-
-func (*TaskResponse) ProtoMessage() {}
-
-func (x *TaskResponse) ProtoReflect() protoreflect.Message {
-	mi := &file_proto_judge_proto_msgTypes[2]
-	if x != nil {
-		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
-		if ms.LoadMessageInfo() == nil {
-			ms.StoreMessageInfo(mi)
-		}
-		return ms
-	}
-	return mi.MessageOf(x)
-}
-
-// Deprecated: Use TaskResponse.ProtoReflect.Descriptor instead.
-func (*TaskResponse) Descriptor() ([]byte, []int) {
-	return file_proto_judge_proto_rawDescGZIP(), []int{2}
-}
-
-func (x *TaskResponse) GetMessage() string {
-	if x != nil {
-		return x.Message
-	}
-	return ""
-}
-
-type Ack struct {
-	state         protoimpl.MessageState `protogen:"open.v1"`
-	unknownFields protoimpl.UnknownFields
-	sizeCache     protoimpl.SizeCache
-}
-
-func (x *Ack) Reset() {
-	*x = Ack{}
-	mi := &file_proto_judge_proto_msgTypes[3]
-	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
-	ms.StoreMessageInfo(mi)
-}
-
-func (x *Ack) String() string {
-	return protoimpl.X.MessageStringOf(x)
-}
-
-func (*Ack) ProtoMessage() {}
-
-func (x *Ack) ProtoReflect() protoreflect.Message {
-	mi := &file_proto_judge_proto_msgTypes[3]
-	if x != nil {
-		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
-		if ms.LoadMessageInfo() == nil {
-			ms.StoreMessageInfo(mi)
-		}
-		return ms
-	}
-	return mi.MessageOf(x)
-}
-
-// Deprecated: Use Ack.ProtoReflect.Descriptor instead.
-func (*Ack) Descriptor() ([]byte, []int) {
-	return file_proto_judge_proto_rawDescGZIP(), []int{3}
-}
-
-var File_proto_judge_proto protoreflect.FileDescriptor
-
-const file_proto_judge_proto_rawDesc = "" +
-	"\n" +
-	"\x11proto/judge.proto\x12\adeep_oj\"\xa1\x02\n" +
-	"\vTaskRequest\x12\x15\n" +
-	"\x06job_id\x18\x01 \x01(\tR\x05jobId\x12\x12\n" +
-	"\x04code\x18\x02 \x01(\fR\x04code\x12-\n" +
-	"\blanguage\x18\x03 \x01(\x0e2\x11.deep_oj.LanguageR\blanguage\x12\x1d\n" +
-	"\n" +
-	"time_limit\x18\x04 \x01(\x05R\ttimeLimit\x12!\n" +
-	"\fmemory_limit\x18\x05 \x01(\x05R\vmemoryLimit\x12\x1b\n" +
-	"\tcache_key\x18\x06 \x01(\tR\bcacheKey\x12\x1f\n" +
-	"\vsubmit_time\x18\a \x01(\x03R\n" +
-	"submitTime\x12\x1d\n" +
-	"\n" +
-	"problem_id\x18\b \x01(\rR\tproblemId\x12\x19\n" +
-	"\btrace_id\x18\t \x01(\tR\atraceId\"\xfe\x01\n" +
-	"\n" +
-	"TaskResult\x12\x15\n" +
-	"\x06job_id\x18\x01 \x01(\tR\x05jobId\x12,\n" +
-	"\x06status\x18\x02 \x01(\x0e2\x14.deep_oj.JudgeStatusR\x06status\x12#\n" +
-	"\rerror_message\x18\x03 \x01(\tR\ferrorMessage\x12\x1b\n" +
-	"\ttime_used\x18\x04 \x01(\x05R\btimeUsed\x12\x1f\n" +
-	"\vmemory_used\x18\x05 \x01(\x03R\n" +
-	"memoryUsed\x12\x19\n" +
-	"\btrace_id\x18\x06 \x01(\tR\atraceId\x12-\n" +
-	"\blanguage\x18\a \x01(\x0e2\x11.deep_oj.LanguageR\blanguage\"(\n" +
-	"\fTaskResponse\x12\x18\n" +
-	"\amessage\x18\x01 \x01(\tR\amessage\"\x05\n" +
-	"\x03Ack*K\n" +
-	"\bLanguage\x12\x18\n" +
-	"\x14LANGUAGE_UNSPECIFIED\x10\x00\x12\a\n" +
-	"\x03CPP\x10\x01\x12\b\n" +
-	"\x04JAVA\x10\x02\x12\n" +
-	"\n" +
-	"\x06PYTHON\x10\x03\x12\x06\n" +
-	"\x02GO\x10\x04*\xb1\x01\n" +
-	"\vJudgeStatus\x12\x16\n" +
-	"\x12STATUS_UNSPECIFIED\x10\x00\x12\f\n" +
-	"\bACCEPTED\x10\x01\x12\x10\n" +
-	"\fWRONG_ANSWER\x10\x02\x12\x11\n" +
-	"\rCOMPILE_ERROR\x10\x03\x12\x17\n" +
-	"\x13TIME_LIMIT_EXCEEDED\x10\x04\x12\x19\n" +
-	"\x15MEMORY_LIMIT_EXCEEDED\x10\x05\x12\x11\n" +
-	"\rRUNTIME_ERROR\x10\x06\x12\x10\n" +
-	"\fSYSTEM_ERROR\x10\a2}\n" +
-	"\fJudgeService\x12:\n" +
-	"\vExecuteTask\x12\x14.deep_oj.TaskRequest\x1a\x15.deep_oj.TaskResponse\x121\n" +
-	"\fUpdateStatus\x12\x13.deep_oj.TaskResult\x1a\f.deep_oj.AckB$Z\"github.com/d1guo/deep_oj/pkg/protob\x06proto3"
-
-var (
-	file_proto_judge_proto_rawDescOnce sync.Once
-	file_proto_judge_proto_rawDescData []byte
-)
-
-func file_proto_judge_proto_rawDescGZIP() []byte {
-	file_proto_judge_proto_rawDescOnce.Do(func() {
-		file_proto_judge_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_proto_judge_proto_rawDesc), len(file_proto_judge_proto_rawDesc)))
-	})
-	return file_proto_judge_proto_rawDescData
-}
-
-var file_proto_judge_proto_enumTypes = make([]protoimpl.EnumInfo, 2)
-var file_proto_judge_proto_msgTypes = make([]protoimpl.MessageInfo, 4)
-var file_proto_judge_proto_goTypes = []any{
-	(Language)(0),        // 0: deep_oj.Language
-	(JudgeStatus)(0),     // 1: deep_oj.JudgeStatus
-	(*TaskRequest)(nil),  // 2: deep_oj.TaskRequest
-	(*TaskResult)(nil),   // 3: deep_oj.TaskResult
-	(*TaskResponse)(nil), // 4: deep_oj.TaskResponse
-	(*Ack)(nil),          // 5: deep_oj.Ack
-}
-var file_proto_judge_proto_depIdxs = []int32{
-	0, // 0: deep_oj.TaskRequest.language:type_name -> deep_oj.Language
-	1, // 1: deep_oj.TaskResult.status:type_name -> deep_oj.JudgeStatus
-	0, // 2: deep_oj.TaskResult.language:type_name -> deep_oj.Language
-	2, // 3: deep_oj.JudgeService.ExecuteTask:input_type -> deep_oj.TaskRequest
-	3, // 4: deep_oj.JudgeService.UpdateStatus:input_type -> deep_oj.TaskResult
-	4, // 5: deep_oj.JudgeService.ExecuteTask:output_type -> deep_oj.TaskResponse
-	5, // 6: deep_oj.JudgeService.UpdateStatus:output_type -> deep_oj.Ack
-	5, // [5:7] is the sub-list for method output_type
-	3, // [3:5] is the sub-list for method input_type
-	3, // [3:3] is the sub-list for extension type_name
-	3, // [3:3] is the sub-list for extension extendee
-	0, // [0:3] is the sub-list for field type_name
-}
-
-func init() { file_proto_judge_proto_init() }
-func file_proto_judge_proto_init() {
-	if File_proto_judge_proto != nil {
-		return
-	}
-	type x struct{}
-	out := protoimpl.TypeBuilder{
-		File: protoimpl.DescBuilder{
-			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
-			RawDescriptor: unsafe.Slice(unsafe.StringData(file_proto_judge_proto_rawDesc), len(file_proto_judge_proto_rawDesc)),
-			NumEnums:      2,
-			NumMessages:   4,
-			NumExtensions: 0,
-			NumServices:   1,
-		},
-		GoTypes:           file_proto_judge_proto_goTypes,
-		DependencyIndexes: file_proto_judge_proto_depIdxs,
-		EnumInfos:         file_proto_judge_proto_enumTypes,
-		MessageInfos:      file_proto_judge_proto_msgTypes,
-	}.Build()
-	File_proto_judge_proto = out.File
-	file_proto_judge_proto_goTypes = nil
-	file_proto_judge_proto_depIdxs = nil
-}
diff --git a/proto/judge.proto b/proto/judge.proto
index c8914f6..a6e86bd 100644
--- a/proto/judge.proto
+++ b/proto/judge.proto
@@ -2,23 +2,8 @@ syntax = "proto3";
 
 package deep_oj;
 
-// 定义判题服务
 option go_package = "github.com/d1guo/deep_oj/pkg/proto";
 
-service JudgeService {
-    // -----------------------------------------------------------
-    // 动作 1: Scheduler -> Worker
-    // 调度器把任务派发给 Worker。Worker 收到后只回个 "收到"，然后异步去跑。
-    // -----------------------------------------------------------
-    rpc ExecuteTask (TaskRequest) returns (TaskResponse);
-
-    // -----------------------------------------------------------
-    // 动作 2: Worker -> API
-    // Worker 跑完后，主动调用 API 的这个接口汇报结果。
-    // -----------------------------------------------------------
-    rpc UpdateStatus (TaskResult) returns (Ack);
-}
-
 // -------------------- 枚举定义 --------------------
 
 enum Language {
@@ -42,7 +27,7 @@ enum JudgeStatus {
 
 // -------------------- 消息体定义 --------------------
 
-// 任务请求 (对应 Redis 里的 queue:pending 内容)
+// 任务请求（由 Worker Stream 消费链路反序列化）
 message TaskRequest {
     // 必须匹配 ^[A-Za-z0-9_-]{1,64}$，禁止路径分隔符。
     string job_id = 1;
@@ -62,20 +47,7 @@ message TaskRequest {
     string trace_id = 9;   // [New] Full-link Trace ID
 }
 
-// 判题结果 (Worker 汇报给 API 的)
-message TaskResult {
-    string job_id = 1;
-    JudgeStatus status = 2;
-    string error_message = 3;  // CE 的报错信息
-    int32 time_used = 4;       // ms
-    int64 memory_used = 5;     // KB
-    string trace_id = 6;       // [New] Return for logging
-    Language language = 7;     // [New] For metrics
-}
-
 // 空响应 (占位符)
 message TaskResponse {
     string message = 1; // [Fix] Add message field
 }
-
-message Ack {}
diff --git a/proto/judge_grpc.pb.go b/proto/judge_grpc.pb.go
deleted file mode 100644
index ca73706..0000000
--- a/proto/judge_grpc.pb.go
+++ /dev/null
@@ -1,175 +0,0 @@
-// Code generated by protoc-gen-go-grpc. DO NOT EDIT.
-// versions:
-// - protoc-gen-go-grpc v1.6.1
-// - protoc             v3.21.12
-// source: proto/judge.proto
-
-package proto
-
-import (
-	context "context"
-	grpc "google.golang.org/grpc"
-	codes "google.golang.org/grpc/codes"
-	status "google.golang.org/grpc/status"
-)
-
-// This is a compile-time assertion to ensure that this generated file
-// is compatible with the grpc package it is being compiled against.
-// Requires gRPC-Go v1.64.0 or later.
-const _ = grpc.SupportPackageIsVersion9
-
-const (
-	JudgeService_ExecuteTask_FullMethodName  = "/deep_oj.JudgeService/ExecuteTask"
-	JudgeService_UpdateStatus_FullMethodName = "/deep_oj.JudgeService/UpdateStatus"
-)
-
-// JudgeServiceClient is the client API for JudgeService service.
-//
-// For semantics around ctx use and closing/ending streaming RPCs, please refer to https://pkg.go.dev/google.golang.org/grpc/?tab=doc#ClientConn.NewStream.
-type JudgeServiceClient interface {
-	// -----------------------------------------------------------
-	// 动作 1: Scheduler -> Worker
-	// 调度器把任务派发给 Worker。Worker 收到后只回个 "收到"，然后异步去跑。
-	// -----------------------------------------------------------
-	ExecuteTask(ctx context.Context, in *TaskRequest, opts ...grpc.CallOption) (*TaskResponse, error)
-	// -----------------------------------------------------------
-	// 动作 2: Worker -> API
-	// Worker 跑完后，主动调用 API 的这个接口汇报结果。
-	// -----------------------------------------------------------
-	UpdateStatus(ctx context.Context, in *TaskResult, opts ...grpc.CallOption) (*Ack, error)
-}
-
-type judgeServiceClient struct {
-	cc grpc.ClientConnInterface
-}
-
-func NewJudgeServiceClient(cc grpc.ClientConnInterface) JudgeServiceClient {
-	return &judgeServiceClient{cc}
-}
-
-func (c *judgeServiceClient) ExecuteTask(ctx context.Context, in *TaskRequest, opts ...grpc.CallOption) (*TaskResponse, error) {
-	cOpts := append([]grpc.CallOption{grpc.StaticMethod()}, opts...)
-	out := new(TaskResponse)
-	err := c.cc.Invoke(ctx, JudgeService_ExecuteTask_FullMethodName, in, out, cOpts...)
-	if err != nil {
-		return nil, err
-	}
-	return out, nil
-}
-
-func (c *judgeServiceClient) UpdateStatus(ctx context.Context, in *TaskResult, opts ...grpc.CallOption) (*Ack, error) {
-	cOpts := append([]grpc.CallOption{grpc.StaticMethod()}, opts...)
-	out := new(Ack)
-	err := c.cc.Invoke(ctx, JudgeService_UpdateStatus_FullMethodName, in, out, cOpts...)
-	if err != nil {
-		return nil, err
-	}
-	return out, nil
-}
-
-// JudgeServiceServer is the server API for JudgeService service.
-// All implementations must embed UnimplementedJudgeServiceServer
-// for forward compatibility.
-type JudgeServiceServer interface {
-	// -----------------------------------------------------------
-	// 动作 1: Scheduler -> Worker
-	// 调度器把任务派发给 Worker。Worker 收到后只回个 "收到"，然后异步去跑。
-	// -----------------------------------------------------------
-	ExecuteTask(context.Context, *TaskRequest) (*TaskResponse, error)
-	// -----------------------------------------------------------
-	// 动作 2: Worker -> API
-	// Worker 跑完后，主动调用 API 的这个接口汇报结果。
-	// -----------------------------------------------------------
-	UpdateStatus(context.Context, *TaskResult) (*Ack, error)
-	mustEmbedUnimplementedJudgeServiceServer()
-}
-
-// UnimplementedJudgeServiceServer must be embedded to have
-// forward compatible implementations.
-//
-// NOTE: this should be embedded by value instead of pointer to avoid a nil
-// pointer dereference when methods are called.
-type UnimplementedJudgeServiceServer struct{}
-
-func (UnimplementedJudgeServiceServer) ExecuteTask(context.Context, *TaskRequest) (*TaskResponse, error) {
-	return nil, status.Error(codes.Unimplemented, "method ExecuteTask not implemented")
-}
-func (UnimplementedJudgeServiceServer) UpdateStatus(context.Context, *TaskResult) (*Ack, error) {
-	return nil, status.Error(codes.Unimplemented, "method UpdateStatus not implemented")
-}
-func (UnimplementedJudgeServiceServer) mustEmbedUnimplementedJudgeServiceServer() {}
-func (UnimplementedJudgeServiceServer) testEmbeddedByValue()                      {}
-
-// UnsafeJudgeServiceServer may be embedded to opt out of forward compatibility for this service.
-// Use of this interface is not recommended, as added methods to JudgeServiceServer will
-// result in compilation errors.
-type UnsafeJudgeServiceServer interface {
-	mustEmbedUnimplementedJudgeServiceServer()
-}
-
-func RegisterJudgeServiceServer(s grpc.ServiceRegistrar, srv JudgeServiceServer) {
-	// If the following call panics, it indicates UnimplementedJudgeServiceServer was
-	// embedded by pointer and is nil.  This will cause panics if an
-	// unimplemented method is ever invoked, so we test this at initialization
-	// time to prevent it from happening at runtime later due to I/O.
-	if t, ok := srv.(interface{ testEmbeddedByValue() }); ok {
-		t.testEmbeddedByValue()
-	}
-	s.RegisterService(&JudgeService_ServiceDesc, srv)
-}
-
-func _JudgeService_ExecuteTask_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
-	in := new(TaskRequest)
-	if err := dec(in); err != nil {
-		return nil, err
-	}
-	if interceptor == nil {
-		return srv.(JudgeServiceServer).ExecuteTask(ctx, in)
-	}
-	info := &grpc.UnaryServerInfo{
-		Server:     srv,
-		FullMethod: JudgeService_ExecuteTask_FullMethodName,
-	}
-	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
-		return srv.(JudgeServiceServer).ExecuteTask(ctx, req.(*TaskRequest))
-	}
-	return interceptor(ctx, in, info, handler)
-}
-
-func _JudgeService_UpdateStatus_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
-	in := new(TaskResult)
-	if err := dec(in); err != nil {
-		return nil, err
-	}
-	if interceptor == nil {
-		return srv.(JudgeServiceServer).UpdateStatus(ctx, in)
-	}
-	info := &grpc.UnaryServerInfo{
-		Server:     srv,
-		FullMethod: JudgeService_UpdateStatus_FullMethodName,
-	}
-	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
-		return srv.(JudgeServiceServer).UpdateStatus(ctx, req.(*TaskResult))
-	}
-	return interceptor(ctx, in, info, handler)
-}
-
-// JudgeService_ServiceDesc is the grpc.ServiceDesc for JudgeService service.
-// It's only intended for direct use with grpc.RegisterService,
-// and not to be introspected or modified (even as a copy)
-var JudgeService_ServiceDesc = grpc.ServiceDesc{
-	ServiceName: "deep_oj.JudgeService",
-	HandlerType: (*JudgeServiceServer)(nil),
-	Methods: []grpc.MethodDesc{
-		{
-			MethodName: "ExecuteTask",
-			Handler:    _JudgeService_ExecuteTask_Handler,
-		},
-		{
-			MethodName: "UpdateStatus",
-			Handler:    _JudgeService_UpdateStatus_Handler,
-		},
-	},
-	Streams:  []grpc.StreamDesc{},
-	Metadata: "proto/judge.proto",
-}
diff --git a/scripts/repo_survey_probe.sh b/scripts/repo_survey_probe.sh
index 5da105f..5805c6f 100755
--- a/scripts/repo_survey_probe.sh
+++ b/scripts/repo_survey_probe.sh
@@ -12,7 +12,7 @@ usage() {
 Usage: scripts/repo_survey_probe.sh
 
 Read-only probe for A1 repo survey:
-1) Redis queue/stream depth
+1) Redis Stream 深度
 2) PostgreSQL schema snapshot
 3) Metrics endpoint smoke check
 
@@ -47,11 +47,9 @@ if [[ "${1:-}" == "-h" || "${1:-}" == "--help" ]]; then
   exit 0
 fi
 
-echo "[1/3] Redis queue/stream depth"
+echo "[1/3] Redis Stream depth"
 if command -v redis-cli >/dev/null 2>&1; then
-  redis-cli -u "${REDIS_URL}" LLEN queue:pending || true
-  redis-cli -u "${REDIS_URL}" LLEN queue:processing || true
-  redis-cli -u "${REDIS_URL}" XLEN stream:results || true
+  redis-cli -u "${REDIS_URL}" XLEN deepoj:jobs || true
 else
   echo "redis-cli not found; skip Redis probes."
 fi
@@ -67,7 +65,7 @@ fi
 echo "[3/3] Metrics endpoint smoke check"
 if command -v curl >/dev/null 2>&1; then
   curl -fsS "${API_METRICS_URL}" | filter_stream "http_requests_total|submission_total" || true
-  curl -fsS "${SCHEDULER_METRICS_URL}" | filter_stream "scheduler_queue_depth|job_latency_seconds" || true
+  curl -fsS "${SCHEDULER_METRICS_URL}" | filter_stream "scheduler_active_workers|control_plane_only" || true
   curl -fsS "${WORKER_METRICS_URL}" | filter_stream "worker_task_total|worker_task_duration_seconds" || true
 else
   echo "curl not found; skip metrics probes."
diff --git a/scripts/start_docker.sh b/scripts/start_docker.sh
index 7524b25..7ea3f10 100755
--- a/scripts/start_docker.sh
+++ b/scripts/start_docker.sh
@@ -49,7 +49,6 @@ docker rm -f oj-worker || true
 docker run -d --name oj-worker \
     --network deep-oj-net \
     --privileged \
-    -p 50051:50051 \
     -v /sys/fs/cgroup:/sys/fs/cgroup:rw \
     -v $(pwd)/data/workspace:/data/workspace:rw \
     -e REDIS_URL=oj-redis:6379 \
@@ -67,7 +66,6 @@ echo "Starting Scheduler..."
 docker rm -f oj-scheduler || true
 docker run -d --name oj-scheduler \
     --network deep-oj-net \
-    -p 50052:50052 \
     -e REDIS_URL=oj-redis:6379 \
     -e WORKER_ADDR=oj-worker:50051 \
     -e PGPASSWORD=secret \
@@ -82,7 +80,6 @@ docker run -d --name oj-api \
     -p 18080:18080 \
     -e PORT=18080 \
     -e REDIS_URL=oj-redis:6379 \
-    -e SCHEDULER_ADDR=oj-scheduler:50052 \
     -e PGPASSWORD=secret \
     -e DATABASE_URL=postgres://deep_oj:secret@oj-postgres:5432/deep_oj?sslmode=disable \
     -e MINIO_ENDPOINT=oj-minio:9000 \
diff --git a/scripts/start_integration.sh b/scripts/start_integration.sh
index d939881..18640e6 100755
--- a/scripts/start_integration.sh
+++ b/scripts/start_integration.sh
@@ -142,9 +142,9 @@ if [ $EXIT_CODE -eq 0 ]; then
     curl -s http://localhost:8080/metrics | grep "http_requests_total" | head -n 5
     
     echo -e "\n--- 调度器指标 (:9091) ---"
-    curl -s http://localhost:9091/metrics | grep "scheduler_queue_depth"
-    curl -s http://localhost:9091/metrics | grep "submission_result_total"
     curl -s http://localhost:9091/metrics | grep "scheduler_active_workers"
+    curl -s http://localhost:9091/metrics | grep "control_plane_only"
+    curl -s http://localhost:9091/metrics | grep "legacy_loops_started"
 else
     echo -e "${RED}集成测试验证失败! 请检查日志: api.log, scheduler.log, worker.log${NC}"
 fi
diff --git a/scripts/verify_b1_no_etcd.sh b/scripts/verify_b1_no_etcd.sh
index 9722026..7122604 100755
--- a/scripts/verify_b1_no_etcd.sh
+++ b/scripts/verify_b1_no_etcd.sh
@@ -112,7 +112,6 @@ export REDIS_PASSWORD="${REDIS_PASSWORD:-deepoj_redis_change_me}"
 export POSTGRES_PASSWORD="${POSTGRES_PASSWORD:-deepoj_pg_password_change_me}"
 export MINIO_ROOT_USER="${MINIO_ROOT_USER:-deepoj_minio_user}"
 export MINIO_ROOT_PASSWORD="${MINIO_ROOT_PASSWORD:-deepoj_minio_password_change_me}"
-export WORKER_AUTH_TOKEN="${WORKER_AUTH_TOKEN:-deepoj_worker_token_change_me}"
 
 echo "[1/8] docker compose up -d --build"
 assert_no_etcd_service_defined
diff --git a/src/go/cmd/scheduler/main.go b/src/go/cmd/scheduler/main.go
index 1156092..6852fc8 100644
--- a/src/go/cmd/scheduler/main.go
+++ b/src/go/cmd/scheduler/main.go
@@ -1,26 +1,19 @@
-// 调度器入口：不依赖 etcd，使用 WORKER_ADDR/WORKER_ADDRS 进行工作节点发现。
+// 调度器入口：B2 后仅保留控制面能力，不执行任何 legacy 数据面动作。
 package main
 
 import (
 	"context"
-	"errors"
 	"fmt"
 	"log/slog"
 	"os"
 	"os/signal"
 	"strconv"
-	"sync"
 	"syscall"
-	"time"
 
 	"github.com/d1guo/deep_oj/internal/appconfig"
 	"github.com/d1guo/deep_oj/internal/repository"
 	"github.com/d1guo/deep_oj/internal/scheduler"
-	"github.com/d1guo/deep_oj/pkg/common"
 	"github.com/d1guo/deep_oj/pkg/observability"
-	pb "github.com/d1guo/deep_oj/pkg/proto"
-	"github.com/redis/go-redis/v9"
-	"google.golang.org/protobuf/proto"
 )
 
 func getEnvInt(key string, fallback int) int {
@@ -54,55 +47,25 @@ func main() {
 
 		appconfig.SetEnvIfEmpty("REDIS_URL", cfg.Scheduler.RedisURL)
 		appconfig.SetEnvIfEmpty("DATABASE_URL", cfg.Scheduler.DatabaseURL)
-		appconfig.SetEnvIfEmptyInt("WORKER_CAPACITY", cfg.Scheduler.WorkerCapacity)
-		appconfig.SetEnvIfEmptyInt("MAX_RETRY", cfg.Scheduler.MaxRetry)
-		appconfig.SetEnvIfEmptyInt("RETRY_TTL_SEC", cfg.Scheduler.RetryTTLSec)
-		appconfig.SetEnvIfEmpty("SCHEDULER_ID", cfg.Scheduler.SchedulerID)
 		appconfig.SetEnvIfEmptyInt("SCHEDULER_METRICS_PORT", cfg.Scheduler.MetricsPort)
 		appconfig.SetEnvIfEmptyInt("SCHEDULER_METRICS_POLL_INTERVAL_MS", cfg.Scheduler.MetricsPollMs)
-		appconfig.SetEnvIfEmptyInt("QUEUE_BRPOP_TIMEOUT_SEC", cfg.Scheduler.Queue.BRPopTimeoutSec)
-		appconfig.SetEnvIfEmptyInt("NO_WORKER_SLEEP_MS", cfg.Scheduler.Queue.NoWorkerSleepMs)
-		appconfig.SetEnvIfEmptyInt("ASSIGNMENT_TTL_SEC", cfg.Scheduler.Queue.AssignmentTTLSec)
-		appconfig.SetEnvIfEmptyInt("PAYLOAD_TTL_SEC", cfg.Scheduler.Queue.PayloadTTLSec)
-		appconfig.SetEnvIfEmptyInt("PROCESSING_START_TTL_SEC", cfg.Scheduler.Queue.ProcessingStartTTLSec)
-		appconfig.SetEnvIfEmptyInt("INFLIGHT_TTL_SEC", cfg.Scheduler.Queue.InflightTTLSec)
-		appconfig.SetEnvIfEmptyInt("ACK_PENDING_COUNT", cfg.Scheduler.AckListener.PendingCount)
-		appconfig.SetEnvIfEmptyInt("ACK_PENDING_BLOCK_MS", cfg.Scheduler.AckListener.PendingBlockMs)
-		appconfig.SetEnvIfEmptyInt("ACK_NEW_COUNT", cfg.Scheduler.AckListener.NewCount)
-		appconfig.SetEnvIfEmptyInt("ACK_NEW_BLOCK_MS", cfg.Scheduler.AckListener.NewBlockMs)
-		appconfig.SetEnvIfEmptyInt("SLOW_PATH_TICK_SEC", cfg.Scheduler.SlowPath.TickSec)
-		appconfig.SetEnvIfEmptyInt("SLOW_PATH_PROCESSING_CUTOFF_SEC", cfg.Scheduler.SlowPath.ProcessingCutoffSec)
-		appconfig.SetEnvIfEmptyInt("PENDING_STALE_SEC", cfg.Scheduler.SlowPath.PendingStaleSec)
-		appconfig.SetEnvIfEmptyInt("SLOW_PATH_DB_SCAN_LIMIT", cfg.Scheduler.SlowPath.DBScanLimit)
-		appconfig.SetEnvIfEmptyInt("WATCHDOG_INTERVAL_SEC", cfg.Scheduler.Watchdog.IntervalSec)
-		appconfig.SetEnvIfEmptyInt("DISPATCH_CONN_TIMEOUT_MS", cfg.Scheduler.Dispatch.ConnTimeoutMs)
-		appconfig.SetEnvIfEmptyInt("DISPATCH_RPC_TIMEOUT_MS", cfg.Scheduler.Dispatch.RPCTimeoutMs)
-		appconfig.SetEnvIfEmptyInt("DISPATCH_MAX_RETRIES", cfg.Scheduler.Dispatch.MaxRetries)
-		appconfig.SetEnvIfEmptyInt("DISPATCH_BACKOFF_BASE_MS", cfg.Scheduler.Dispatch.BackoffBaseMs)
-		appconfig.SetEnvIfEmpty("GRPC_TLS_CERT", cfg.Scheduler.GRPCTLS.Cert)
-		appconfig.SetEnvIfEmpty("GRPC_TLS_KEY", cfg.Scheduler.GRPCTLS.Key)
-		appconfig.SetEnvIfEmpty("GRPC_TLS_CA", cfg.Scheduler.GRPCTLS.CA)
 		appconfig.SetEnvIfEmpty("SERVICE_NAME", cfg.Scheduler.Metrics.ServiceName)
 		appconfig.SetEnvIfEmpty("INSTANCE_ID", cfg.Scheduler.Metrics.InstanceID)
 	}
 
-	// 1. 读取配置
 	redisURL := os.Getenv("REDIS_URL")
 	if redisURL == "" {
 		redisURL = "localhost:6379"
 	}
-	workerCapacity := 4
-	if v := os.Getenv("WORKER_CAPACITY"); v != "" {
-		if n, err := strconv.Atoi(v); err == nil && n > 0 {
-			workerCapacity = n
-		}
+	postgresURL := os.Getenv("DATABASE_URL")
+	if postgresURL == "" {
+		slog.Error("必须设置 DATABASE_URL")
+		os.Exit(1)
 	}
 
-	// 2. 初始化 Context (支持优雅关闭)
 	ctx, cancel := context.WithCancel(context.Background())
 	defer cancel()
 
-	// 监听中断信号
 	sigCh := make(chan os.Signal, 1)
 	signal.Notify(sigCh, syscall.SIGINT, syscall.SIGTERM)
 	go func() {
@@ -111,201 +74,37 @@ func main() {
 		cancel()
 	}()
 
-	// 3. 初始化工作节点发现（无 etcd 依赖）
 	discovery, err := scheduler.NewWorkerDiscovery()
 	if err != nil {
 		slog.Error("初始化工作节点发现失败", "error", err)
 		os.Exit(1)
 	}
 	defer discovery.Close()
-	slog.Info("工作节点发现已就绪", "worker_count", discovery.GetWorkerCount())
-
-	// 保留调用路径，当前实现会等待 ctx 退出。
 	go discovery.WatchWorkers(ctx)
 
-	// 4. 初始化 Redis 客户端
 	redisClient := repository.NewRedisClient(redisURL)
 	if err := redisClient.Ping(ctx); err != nil {
 		slog.Error("连接 Redis 失败", "error", err)
 		os.Exit(1)
 	}
-	slog.Info("已连接 Redis")
+	defer redisClient.Close()
 
-	// 4.5 初始化 PostgreSQL (用于 ACK 回调更新状态)
-	postgresURL := os.Getenv("DATABASE_URL")
-	if postgresURL == "" {
-		slog.Error("必须设置 DATABASE_URL")
-		os.Exit(1)
-	}
 	db, err := repository.NewPostgresDB(ctx, postgresURL)
 	if err != nil {
 		slog.Error("连接 PostgreSQL 失败", "error", err)
 		os.Exit(1)
 	}
 	defer db.Close()
-	slog.Info("已连接 PostgreSQL")
-
-	// 启动 ACK 监听器
-	go scheduler.StartAckListener(ctx, redisClient, db)
 
-	// 6. 启动监控（探针与指标）
+	scheduler.SetControlPlaneOnly(true)
+	scheduler.SetLegacyLoopsStarted(0)
+	go scheduler.StartMetricsPoller(ctx, discovery)
 
-	// 6.1 启动指标轮询（Redis/Worker 状态）
-	go scheduler.StartMetricsPoller(ctx, redisClient, discovery)
-
-	// 6.2 暴露 Prometheus 指标端点
 	metricsPort := getEnvInt("SCHEDULER_METRICS_PORT", 9091)
 	observability.StartMetricsServer(fmt.Sprintf(":%d", metricsPort))
 
-	// 启动慢路径兜底
-	go scheduler.StartSlowPath(ctx, redisClient, db)
-
-	// 启动看门狗（防止工作节点宕机导致任务泄漏）
-	watchdogInterval := time.Duration(getEnvInt("WATCHDOG_INTERVAL_SEC", 5)) * time.Second
-	watchdog := scheduler.NewWatchdog(redisClient, discovery, db, watchdogInterval)
-	go watchdog.Start(ctx)
-
-	// 5. 启动任务分发循环
-	slog.Info("调度器已启动，等待任务")
-
-	var wg sync.WaitGroup
-
-	for {
-		select {
-		case <-ctx.Done():
-			slog.Info("调度器停止中，等待活跃任务结束")
-			wg.Wait()
-			slog.Info("调度器已退出")
-			return
-		default:
-		}
-
-		// 阻塞等待任务 (5 秒超时)
-		brpopTimeout := time.Duration(getEnvInt("QUEUE_BRPOP_TIMEOUT_SEC", 5)) * time.Second
-		result, err := redisClient.BRPopLPush(ctx, common.QueuePending, common.QueueProcessing, brpopTimeout)
-		if err != nil || result == "" {
-			continue
-		}
-
-		// 解析任务 (Protobuf)
-		task := &pb.TaskRequest{}
-		if err := proto.Unmarshal([]byte(result), task); err != nil {
-			slog.Warn("任务解析失败", "error", err)
-			_ = redisClient.LRem(ctx, common.QueueProcessing, 1, result)
-			_ = redisClient.LPush(ctx, common.QueueDead, result)
-			continue
-		}
-
-		jobID := task.JobId
-		slog.Info("收到任务", "job_id", jobID)
-		processingStart := time.Now().UnixMilli()
+	slog.Info("调度器已进入控制面模式（legacy 数据面已移除）", "worker_count", discovery.GetWorkerCount())
 
-		// 获取可用 Worker
-		workerID, workerAddr, ok := selectWorker(ctx, redisClient, discovery, workerCapacity)
-		if !ok {
-			slog.Warn("暂无可用工作节点，任务稍后重试", "job_id", jobID)
-			if err := redisClient.RequeueTask(ctx, common.QueueProcessing, common.QueuePending, result); err != nil {
-				slog.Error("任务重新入队失败", "job_id", jobID, "error", err)
-			}
-			noWorkerSleep := time.Duration(getEnvInt("NO_WORKER_SLEEP_MS", 1000)) * time.Millisecond
-			time.Sleep(noWorkerSleep)
-			continue
-		}
-
-		// 【关键】记录任务分配关系 (Job -> Worker)
-		// TTL 设置为 10 分钟 (假设判题不会超过 10 分钟)
-		assignmentKey := common.TaskAssignmentPrefix + jobID
-		assignmentTTL := time.Duration(getEnvInt("ASSIGNMENT_TTL_SEC", 600)) * time.Second
-		if err := redisClient.Set(ctx, assignmentKey, workerID, assignmentTTL); err != nil {
-			slog.Error("写入任务分配关系失败", "job_id", jobID, "error", err)
-			// 即使失败也尝试继续，或者选择回滚
-		}
-
-		// 记录任务 payload 便于 O(1) 清理
-		payloadKey := common.TaskPayloadPrefix + jobID
-		payloadTTL := time.Duration(getEnvInt("PAYLOAD_TTL_SEC", 1800)) * time.Second
-		if err := redisClient.Set(ctx, payloadKey, result, payloadTTL); err != nil {
-			slog.Error("写入任务负载失败", "job_id", jobID, "error", err)
-		}
-
-		// 记录 processing_start 与 ZSET
-		startKey := common.TaskProcessingStartPrefix + jobID
-		processingTTL := time.Duration(getEnvInt("PROCESSING_START_TTL_SEC", 1800)) * time.Second
-		if err := redisClient.Set(ctx, startKey, fmt.Sprintf("%d", processingStart), processingTTL); err != nil {
-			slog.Error("写入 processing_start 失败", "job_id", jobID, "error", err)
-		}
-		if err := redisClient.ZAdd(ctx, common.TaskProcessingZSet, &redis.Z{Score: float64(processingStart), Member: jobID}); err != nil {
-			slog.Error("写入 processing zset 失败", "job_id", jobID, "error", err)
-		}
-
-		// 更新 DB 状态为 processing
-		if err := db.UpdateSubmissionState(ctx, jobID, "processing"); err != nil {
-			slog.Error("更新提交状态失败", "job_id", jobID, "error", err)
-		}
-
-		// 增加 inflight 计数
-		inflightKey := common.WorkerInflightPrefix + workerID
-		_, _ = redisClient.Incr(ctx, inflightKey)
-		inflightTTL := time.Duration(getEnvInt("INFLIGHT_TTL_SEC", 1800)) * time.Second
-		_, _ = redisClient.Expire(ctx, inflightKey, inflightTTL)
-
-		// 异步分发任务
-		wg.Add(1)
-		go func(addr string, taskData []byte) {
-			defer wg.Done()
-			if err := scheduler.DispatchTask(ctx, addr, taskData, redisClient); err != nil {
-				slog.Error("任务分发失败", "job_id", jobID, "error", err)
-
-				if errors.Is(err, common.ErrNonRetryable) {
-					slog.Error("丢弃不可重试任务", "job_id", jobID)
-					// Poison Pill: 进入死信队列并标记失败
-					_ = redisClient.LPush(ctx, common.QueueDead, string(taskData))
-					_ = db.UpdateSubmissionState(ctx, jobID, "failed")
-					_, _, _ = db.UpdateSubmissionResultIfNotDone(ctx, jobID, "System Error", map[string]any{
-						"status":        "System Error",
-						"error_message": "non-retryable dispatch error",
-					})
-				} else {
-					if err := scheduler.HandleRetry(ctx, redisClient, db, jobID, string(taskData), "dispatch retry exceeded"); err != nil {
-						slog.Error("重试处理失败", "job_id", jobID, "error", err)
-					}
-				}
-
-				redisClient.Del(ctx, assignmentKey)
-				redisClient.Del(ctx, payloadKey, startKey)
-				_ = redisClient.ZRem(ctx, common.TaskProcessingZSet, jobID)
-				// 回滚 inflight
-				_, _ = redisClient.Decr(ctx, inflightKey)
-			} else {
-				// 成功时不移除！等待 ACK Listener 移除
-				slog.Debug("任务分发成功", "job_id", jobID)
-			}
-		}(workerAddr, []byte(result))
-	}
-}
-
-func selectWorker(ctx context.Context, redisClient *repository.RedisClient, discovery *scheduler.WorkerDiscovery, capacity int) (string, string, bool) {
-	total := discovery.GetWorkerCount()
-	if total == 0 {
-		return "", "", false
-	}
-	for i := 0; i < total; i++ {
-		workerID, workerAddr, ok := discovery.GetNextWorker()
-		if !ok {
-			return "", "", false
-		}
-		inflightKey := common.WorkerInflightPrefix + workerID
-		inflightStr, err := redisClient.Get(ctx, inflightKey)
-		if err != nil || inflightStr == "" {
-			return workerID, workerAddr, true
-		}
-		n, err := strconv.Atoi(inflightStr)
-		if err != nil {
-			return workerID, workerAddr, true
-		}
-		if n < capacity {
-			return workerID, workerAddr, true
-		}
-	}
-	return "", "", false
+	<-ctx.Done()
+	slog.Info("调度器已退出")
 }
diff --git a/src/go/cmd/worker/main.go b/src/go/cmd/worker/main.go
index a511d02..4b605ec 100644
--- a/src/go/cmd/worker/main.go
+++ b/src/go/cmd/worker/main.go
@@ -3,11 +3,9 @@ package main
 import (
 	"context"
 	"crypto/tls"
-	"crypto/x509"
 	"errors"
 	"fmt"
 	"log/slog"
-	"net"
 	"os"
 	"os/signal"
 	"runtime"
@@ -20,13 +18,7 @@ import (
 	"github.com/d1guo/deep_oj/internal/repository"
 	"github.com/d1guo/deep_oj/internal/worker"
 	"github.com/d1guo/deep_oj/pkg/observability"
-	pb "github.com/d1guo/deep_oj/pkg/proto"
 	"github.com/redis/go-redis/v9"
-	"google.golang.org/grpc"
-	"google.golang.org/grpc/codes"
-	"google.golang.org/grpc/credentials"
-	"google.golang.org/grpc/metadata"
-	"google.golang.org/grpc/status"
 )
 
 func getEnvInt(key string, fallback int) int {
@@ -53,20 +45,6 @@ func getEnvBool(key string, fallback bool) bool {
 	}
 }
 
-func workerAuthUnaryInterceptor(expectedToken string) grpc.UnaryServerInterceptor {
-	return func(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (interface{}, error) {
-		md, ok := metadata.FromIncomingContext(ctx)
-		if !ok {
-			return nil, status.Error(codes.Unauthenticated, "missing metadata")
-		}
-		values := md.Get("x-worker-auth-token")
-		if len(values) == 0 || strings.TrimSpace(values[0]) != expectedToken {
-			return nil, status.Error(codes.Unauthenticated, "invalid worker auth token")
-		}
-		return handler(ctx, req)
-	}
-}
-
 func buildRedisOptions(addr string) *redis.Options {
 	opts := &redis.Options{Addr: addr}
 	if strings.HasPrefix(addr, "redis://") || strings.HasPrefix(addr, "rediss://") {
@@ -93,7 +71,7 @@ func main() {
 		slog.Info("已加载配置", "path", cfgPath)
 	}
 	if cfgFile != nil {
-		// Apply config file values as in-process defaults. Runtime reads from env via LoadConfig().
+		// 将配置文件值写入环境默认值，运行时仍由 LoadConfig() 读取。
 		appconfig.SetEnvIfEmptyInt("REDIS_POOL_SIZE", cfgFile.Redis.PoolSize)
 		appconfig.SetEnvIfEmptyInt("REDIS_MIN_IDLE_CONNS", cfgFile.Redis.MinIdleConns)
 		appconfig.SetEnvIfEmptyInt("REDIS_DIAL_TIMEOUT_MS", cfgFile.Redis.DialTimeoutMs)
@@ -101,7 +79,6 @@ func main() {
 		appconfig.SetEnvIfEmptyInt("REDIS_WRITE_TIMEOUT_MS", cfgFile.Redis.WriteTimeoutMs)
 
 		wcfg := cfgFile.Worker
-		// Backward-compatible fallback to server/path sections
 		if wcfg.Port == 0 && cfgFile.Server.Port > 0 {
 			wcfg.Port = cfgFile.Server.Port
 		}
@@ -138,13 +115,8 @@ func main() {
 		appconfig.SetEnvIfEmptyInt("RESULT_TTL_SEC", wcfg.ResultTTLSec)
 		appconfig.SetEnvIfEmptyInt("CHECKER_TIMEOUT_MS", wcfg.CheckerTimeoutMs)
 		appconfig.SetEnvIfEmptyInt("CLEANUP_TIMEOUT_SEC", wcfg.CleanupTimeoutSec)
-		appconfig.SetEnvIfEmptyInt("RESULT_STREAM_MAX_RETRIES", wcfg.ResultStreamMaxRetries)
-		appconfig.SetEnvIfEmptyInt("RESULT_STREAM_BACKOFF_MS", wcfg.ResultStreamBackoffMs)
 		appconfig.SetEnvIfEmptyBool("ALLOW_HOST_CHECKER", wcfg.AllowHostChecker)
 		appconfig.SetEnvIfEmptyBool("REQUIRE_CGROUPS_V2", wcfg.RequireCgroupsV2)
-		appconfig.SetEnvIfEmpty("GRPC_TLS_CERT", wcfg.GRPCTLS.Cert)
-		appconfig.SetEnvIfEmpty("GRPC_TLS_KEY", wcfg.GRPCTLS.Key)
-		appconfig.SetEnvIfEmpty("GRPC_TLS_CA", wcfg.GRPCTLS.CA)
 		appconfig.SetEnvIfEmpty("SERVICE_NAME", wcfg.Metrics.ServiceName)
 		appconfig.SetEnvIfEmpty("INSTANCE_ID", wcfg.Metrics.InstanceID)
 		appconfig.SetEnvIfEmptyInt("WORKER_METRICS_PORT", wcfg.MetricsPort)
@@ -160,9 +132,8 @@ func main() {
 		appconfig.SetEnvIfEmptyInt("JOB_RECLAIM_GRACE_SEC", wcfg.Stream.ReclaimGraceSec)
 	}
 
-	// 1. 配置
 	cfg := worker.LoadConfig()
-	slog.Info("工作节点启动中", "id", cfg.WorkerID, "addr", cfg.WorkerAddr, "bin", cfg.JudgerBin)
+	slog.Info("工作节点启动中", "id", cfg.WorkerID, "bin", cfg.JudgerBin)
 	slog.Info("工作节点并发配置", "max_concurrency", cfg.PoolSize, "num_cpu", runtime.NumCPU())
 
 	if getEnvBool("REQUIRE_CGROUPS_V2", false) {
@@ -172,12 +143,10 @@ func main() {
 		}
 	}
 
-	// 1.5 启动指标服务（Worker 默认使用 9092）
 	worker.InitMetrics()
 	metricsPort := getEnvInt("WORKER_METRICS_PORT", 9092)
 	observability.StartMetricsServer(fmt.Sprintf(":%d", metricsPort))
 
-	// 2. 依赖初始化
 	exec := worker.NewExecutor(cfg.JudgerBin)
 	slog.Info("初始化测试用例管理器...")
 	tcMgr, err := worker.NewTestCaseManager(cfg)
@@ -207,39 +176,11 @@ func main() {
 	defer db.Close()
 	slog.Info("已连接 PostgreSQL")
 
-	// 3. gRPC 服务
-	lis, err := net.Listen("tcp", cfg.WorkerAddr)
-	if err != nil {
-		slog.Error("监听端口失败", "error", err)
-		os.Exit(1)
-	}
-
-	serverOpts := make([]grpc.ServerOption, 0, 4)
-	serverOpts = append(serverOpts, grpc.MaxRecvMsgSize(getEnvInt("WORKER_GRPC_MAX_RECV_BYTES", 1<<20)))
-	serverOpts = append(serverOpts, grpc.MaxSendMsgSize(getEnvInt("WORKER_GRPC_MAX_SEND_BYTES", 1<<20)))
-
-	workerAuthToken := strings.TrimSpace(os.Getenv("WORKER_AUTH_TOKEN"))
-	if workerAuthToken == "" && !getEnvBool("ALLOW_INSECURE_WORKER_GRPC", false) {
-		slog.Error("除非 ALLOW_INSECURE_WORKER_GRPC=true，否则必须设置 WORKER_AUTH_TOKEN")
-		os.Exit(1)
-	}
-	if workerAuthToken != "" {
-		serverOpts = append(serverOpts, grpc.UnaryInterceptor(workerAuthUnaryInterceptor(workerAuthToken)))
-	}
-
-	if opt, err := loadServerTLS(); err != nil {
-		slog.Error("加载 TLS 失败", "error", err)
-		os.Exit(1)
-	} else if opt != nil {
-		serverOpts = append(serverOpts, opt)
-	}
-	grpcServer := grpc.NewServer(serverOpts...)
-	svc := worker.NewJudgeService(cfg, exec, tcMgr, rdb)
-	pb.RegisterJudgeServiceServer(grpcServer, svc)
-
-	streamConsumer := worker.NewStreamConsumer(cfg, rdb, db, svc)
+	runner := worker.NewJudgeService(cfg, exec, tcMgr, rdb)
+	streamConsumer := worker.NewStreamConsumer(cfg, rdb, db, runner)
 	streamCtx, cancelStream := context.WithCancel(context.Background())
 	defer cancelStream()
+
 	var streamWG sync.WaitGroup
 	streamWG.Add(1)
 	go func() {
@@ -249,16 +190,6 @@ func main() {
 		}
 	}()
 
-	// 4. 启动服务
-	go func() {
-		slog.Info("gRPC 服务监听中", "addr", lis.Addr())
-		if err := grpcServer.Serve(lis); err != nil {
-			slog.Error("服务运行失败", "error", err)
-			os.Exit(1)
-		}
-	}()
-
-	// 优雅关闭
 	quit := make(chan os.Signal, 1)
 	signal.Notify(quit, syscall.SIGINT, syscall.SIGTERM)
 	<-quit
@@ -266,33 +197,5 @@ func main() {
 	slog.Info("正在关闭...")
 	cancelStream()
 	streamWG.Wait()
-	grpcServer.GracefulStop()
 	slog.Info("工作节点已退出")
 }
-
-func loadServerTLS() (grpc.ServerOption, error) {
-	certFile := os.Getenv("GRPC_TLS_CERT")
-	keyFile := os.Getenv("GRPC_TLS_KEY")
-	caFile := os.Getenv("GRPC_TLS_CA")
-	if certFile == "" || keyFile == "" || caFile == "" {
-		return nil, nil
-	}
-	cert, err := tls.LoadX509KeyPair(certFile, keyFile)
-	if err != nil {
-		return nil, err
-	}
-	caData, err := os.ReadFile(caFile)
-	if err != nil {
-		return nil, err
-	}
-	certPool := x509.NewCertPool()
-	certPool.AppendCertsFromPEM(caData)
-
-	tlsConfig := &tls.Config{
-		Certificates: []tls.Certificate{cert},
-		ClientCAs:    certPool,
-		ClientAuth:   tls.RequireAndVerifyClientCert,
-		MinVersion:   tls.VersionTLS12,
-	}
-	return grpc.Creds(credentials.NewTLS(tlsConfig)), nil
-}
diff --git a/src/go/go.mod b/src/go/go.mod
index fa0f060..70eecf3 100644
--- a/src/go/go.mod
+++ b/src/go/go.mod
@@ -13,7 +13,6 @@ require (
 	golang.org/x/crypto v0.48.0
 	golang.org/x/oauth2 v0.35.0
 	golang.org/x/sync v0.19.0
-	google.golang.org/grpc v1.78.0
 	google.golang.org/protobuf v1.36.11
 	gopkg.in/yaml.v3 v3.0.1
 )
@@ -65,5 +64,4 @@ require (
 	golang.org/x/net v0.50.0 // indirect
 	golang.org/x/sys v0.41.0 // indirect
 	golang.org/x/text v0.34.0 // indirect
-	google.golang.org/genproto/googleapis/rpc v0.0.0-20260209200024-4cfbd4190f57 // indirect
 )
diff --git a/src/go/go.sum b/src/go/go.sum
index 4a7f959..7e4e69b 100644
--- a/src/go/go.sum
+++ b/src/go/go.sum
@@ -28,10 +28,6 @@ github.com/gin-gonic/gin v1.9.1 h1:4idEAncQnU5cB7BeOkPtxjfCSye0AAm1R0RVIqJ+Jmg=
 github.com/gin-gonic/gin v1.9.1/go.mod h1:hPrL7YrpYKXt5YId3A/Tnip5kqbEAP+KLuI3SUcPTeU=
 github.com/go-ini/ini v1.67.0 h1:z6ZrTEZqSWOTyH2FlglNbNgARyHG8oLW9gMELqKr06A=
 github.com/go-ini/ini v1.67.0/go.mod h1:ByCAeIL28uOIIG0E3PJtZPDL8WnHpFKFOtgjp+3Ies8=
-github.com/go-logr/logr v1.4.3 h1:CjnDlHq8ikf6E492q6eKboGOC0T8CDaOvkHCIg8idEI=
-github.com/go-logr/logr v1.4.3/go.mod h1:9T104GzyrTigFIr8wt5mBrctHMim0Nb2HLGrmQ40KvY=
-github.com/go-logr/stdr v1.2.2 h1:hSWxHoqTgW2S2qGc0LTAI563KZ5YKYRhT3MFKZMbjag=
-github.com/go-logr/stdr v1.2.2/go.mod h1:mMo/vtBO5dYbehREoey6XUKy/eSumjCCveDpRre4VKE=
 github.com/go-playground/assert/v2 v2.2.0 h1:JvknZsQTYeFEAhQwI4qEt9cyV5ONwRHC+lYKSsYSR8s=
 github.com/go-playground/assert/v2 v2.2.0/go.mod h1:VDjEfimB/XKnb+ZQfWdccd7VUvScMdVu0Titje2rxJ4=
 github.com/go-playground/locales v0.14.1 h1:EWaQ/wswjilfKLTECiXz7Rh+3BjFhfDFKv/oXslEjJA=
@@ -44,8 +40,6 @@ github.com/goccy/go-json v0.10.2 h1:CrxCmQqYDkv1z7lO7Wbh2HN93uovUHgrECaO5ZrCXAU=
 github.com/goccy/go-json v0.10.2/go.mod h1:6MelG93GURQebXPDq3khkgXZkazVtN9CRI+MGFi0w8I=
 github.com/golang-jwt/jwt/v5 v5.3.1 h1:kYf81DTWFe7t+1VvL7eS+jKFVWaUnK9cB1qbwn63YCY=
 github.com/golang-jwt/jwt/v5 v5.3.1/go.mod h1:fxCRLWMO43lRc8nhHWY6LGqRcf+1gQWArsqaEUEa5bE=
-github.com/golang/protobuf v1.5.4 h1:i7eJL8qZTpSEXOPTxNKhASYpMn+8e5Q6AdndVa1dWek=
-github.com/golang/protobuf v1.5.4/go.mod h1:lnTiLA8Wa4RWRcIUkrtSVa5nRhsEGBg48fD6rSs7xps=
 github.com/google/go-cmp v0.7.0 h1:wk8382ETsv4JYUZwIsn6YpYiWiBsYLSJiTsyBybVuN8=
 github.com/google/go-cmp v0.7.0/go.mod h1:pXiqmnSA92OHEEa9HXL2W4E7lf9JzCmGVUdgjX3N/iU=
 github.com/google/gofuzz v1.0.0/go.mod h1:dBl0BpW6vV/+mYPU4Po3pmUjxk6FQPldtuIdl/M65Eg=
@@ -132,18 +126,6 @@ github.com/ugorji/go/codec v1.2.11 h1:BMaWp1Bb6fHwEtbplGBGJ498wD+LKlNSl25MjdZY4d
 github.com/ugorji/go/codec v1.2.11/go.mod h1:UNopzCgEMSXjBc6AOMqYvWC1ktqTAfzJZUZgYf6w6lg=
 github.com/zeebo/xxh3 v1.0.2 h1:xZmwmqxHZA8AI603jOQ0tMqmBr9lPeFwGg6d+xy9DC0=
 github.com/zeebo/xxh3 v1.0.2/go.mod h1:5NWz9Sef7zIDm2JHfFlcQvNekmcEl9ekUZQQKCYaDcA=
-go.opentelemetry.io/auto/sdk v1.2.1 h1:jXsnJ4Lmnqd11kwkBV2LgLoFMZKizbCi5fNZ/ipaZ64=
-go.opentelemetry.io/auto/sdk v1.2.1/go.mod h1:KRTj+aOaElaLi+wW1kO/DZRXwkF4C5xPbEe3ZiIhN7Y=
-go.opentelemetry.io/otel v1.38.0 h1:RkfdswUDRimDg0m2Az18RKOsnI8UDzppJAtj01/Ymk8=
-go.opentelemetry.io/otel v1.38.0/go.mod h1:zcmtmQ1+YmQM9wrNsTGV/q/uyusom3P8RxwExxkZhjM=
-go.opentelemetry.io/otel/metric v1.38.0 h1:Kl6lzIYGAh5M159u9NgiRkmoMKjvbsKtYRwgfrA6WpA=
-go.opentelemetry.io/otel/metric v1.38.0/go.mod h1:kB5n/QoRM8YwmUahxvI3bO34eVtQf2i4utNVLr9gEmI=
-go.opentelemetry.io/otel/sdk v1.38.0 h1:l48sr5YbNf2hpCUj/FoGhW9yDkl+Ma+LrVl8qaM5b+E=
-go.opentelemetry.io/otel/sdk v1.38.0/go.mod h1:ghmNdGlVemJI3+ZB5iDEuk4bWA3GkTpW+DOoZMYBVVg=
-go.opentelemetry.io/otel/sdk/metric v1.38.0 h1:aSH66iL0aZqo//xXzQLYozmWrXxyFkBJ6qT5wthqPoM=
-go.opentelemetry.io/otel/sdk/metric v1.38.0/go.mod h1:dg9PBnW9XdQ1Hd6ZnRz689CbtrUp0wMMs9iPcgT9EZA=
-go.opentelemetry.io/otel/trace v1.38.0 h1:Fxk5bKrDZJUH+AMyyIXGcFAPah0oRcT+LuNtJrmcNLE=
-go.opentelemetry.io/otel/trace v1.38.0/go.mod h1:j1P9ivuFsTceSWe1oY+EeW3sc+Pp42sO++GHkg4wwhs=
 go.uber.org/atomic v1.11.0 h1:ZvwS0R+56ePWxUNi+Atn9dWONBPp/AUETXlHW0DxSjE=
 go.uber.org/atomic v1.11.0/go.mod h1:LUxbIzbOniOlMKjJjyPfpl4v+PKK2cNJn91OQbhoJI0=
 go.uber.org/goleak v1.3.0 h1:2K3zAYmnTNqV73imy9J1T3WC+gmCePx2hEGkimedGto=
@@ -168,12 +150,6 @@ golang.org/x/sys v0.41.0 h1:Ivj+2Cp/ylzLiEU89QhWblYnOE9zerudt9Ftecq2C6k=
 golang.org/x/sys v0.41.0/go.mod h1:OgkHotnGiDImocRcuBABYBEXf8A9a87e/uXjp9XT3ks=
 golang.org/x/text v0.34.0 h1:oL/Qq0Kdaqxa1KbNeMKwQq0reLCCaFtqu2eNuSeNHbk=
 golang.org/x/text v0.34.0/go.mod h1:homfLqTYRFyVYemLBFl5GgL/DWEiH5wcsQ5gSh1yziA=
-gonum.org/v1/gonum v0.16.0 h1:5+ul4Swaf3ESvrOnidPp4GZbzf0mxVQpDCYUQE7OJfk=
-gonum.org/v1/gonum v0.16.0/go.mod h1:fef3am4MQ93R2HHpKnLk4/Tbh/s0+wqD5nfa6Pnwy4E=
-google.golang.org/genproto/googleapis/rpc v0.0.0-20260209200024-4cfbd4190f57 h1:mWPCjDEyshlQYzBpMNHaEof6UX1PmHcaUODUywQ0uac=
-google.golang.org/genproto/googleapis/rpc v0.0.0-20260209200024-4cfbd4190f57/go.mod h1:j9x/tPzZkyxcgEFkiKEEGxfvyumM01BEtsW8xzOahRQ=
-google.golang.org/grpc v1.78.0 h1:K1XZG/yGDJnzMdd/uZHAkVqJE+xIDOcmdSFZkBUicNc=
-google.golang.org/grpc v1.78.0/go.mod h1:I47qjTo4OKbMkjA/aOOwxDIiPSBofUtQUI5EfpWvW7U=
 google.golang.org/protobuf v1.36.11 h1:fV6ZwhNocDyBLK0dj+fg8ektcVegBBuEolpbTQyBNVE=
 google.golang.org/protobuf v1.36.11/go.mod h1:HTf+CrKn2C3g5S8VImy6tdcUvCska2kB7j23XfzDpco=
 gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
diff --git a/src/go/internal/appconfig/config.go b/src/go/internal/appconfig/config.go
index 45726bd..4d20a29 100644
--- a/src/go/internal/appconfig/config.go
+++ b/src/go/internal/appconfig/config.go
@@ -105,83 +105,37 @@ type ProblemDefaults struct {
 }
 
 type SchedulerConfig struct {
-	RedisURL       string            `yaml:"redis_url"`
-	DatabaseURL    string            `yaml:"database_url"`
-	WorkerCapacity int               `yaml:"worker_capacity"`
-	MaxRetry       int               `yaml:"max_retry"`
-	RetryTTLSec    int               `yaml:"retry_ttl_sec"`
-	SchedulerID    string            `yaml:"scheduler_id"`
-	GRPCTLS        TLSConfig         `yaml:"grpc_tls"`
-	Metrics        Metrics           `yaml:"metrics"`
-	MetricsPort    int               `yaml:"metrics_port"`
-	MetricsPollMs  int               `yaml:"metrics_poll_ms"`
-	Queue          SchedulerQueue    `yaml:"queue"`
-	AckListener    AckListenerConfig `yaml:"ack_listener"`
-	SlowPath       SlowPathConfig    `yaml:"slow_path"`
-	Watchdog       WatchdogConfig    `yaml:"watchdog"`
-	Dispatch       DispatchConfig    `yaml:"dispatch"`
-}
-
-type SchedulerQueue struct {
-	BRPopTimeoutSec       int `yaml:"brpop_timeout_sec"`
-	NoWorkerSleepMs       int `yaml:"no_worker_sleep_ms"`
-	AssignmentTTLSec      int `yaml:"assignment_ttl_sec"`
-	PayloadTTLSec         int `yaml:"payload_ttl_sec"`
-	ProcessingStartTTLSec int `yaml:"processing_start_ttl_sec"`
-	InflightTTLSec        int `yaml:"inflight_ttl_sec"`
-}
-
-type AckListenerConfig struct {
-	PendingCount   int `yaml:"pending_count"`
-	PendingBlockMs int `yaml:"pending_block_ms"`
-	NewCount       int `yaml:"new_count"`
-	NewBlockMs     int `yaml:"new_block_ms"`
-}
-
-type SlowPathConfig struct {
-	TickSec             int `yaml:"tick_sec"`
-	ProcessingCutoffSec int `yaml:"processing_cutoff_sec"`
-	PendingStaleSec     int `yaml:"pending_stale_sec"`
-	DBScanLimit         int `yaml:"db_scan_limit"`
-}
-
-type WatchdogConfig struct {
-	IntervalSec int `yaml:"interval_sec"`
-}
-
-type DispatchConfig struct {
-	ConnTimeoutMs int `yaml:"conn_timeout_ms"`
-	RPCTimeoutMs  int `yaml:"rpc_timeout_ms"`
-	MaxRetries    int `yaml:"max_retries"`
-	BackoffBaseMs int `yaml:"backoff_base_ms"`
+	RedisURL      string  `yaml:"redis_url"`
+	DatabaseURL   string  `yaml:"database_url"`
+	SchedulerID   string  `yaml:"scheduler_id"`
+	Metrics       Metrics `yaml:"metrics"`
+	MetricsPort   int     `yaml:"metrics_port"`
+	MetricsPollMs int     `yaml:"metrics_poll_ms"`
 }
 
 type WorkerConfig struct {
-	ID                     string             `yaml:"id"`
-	Addr                   string             `yaml:"addr"`
-	Port                   int                `yaml:"port"`
-	RedisURL               string             `yaml:"redis_url"`
-	DatabaseURL            string             `yaml:"database_url"`
-	Stream                 WorkerStreamConfig `yaml:"stream"`
-	MinIO                  MinIOConfig        `yaml:"minio"`
-	Workspace              string             `yaml:"workspace"`
-	JudgerBin              string             `yaml:"judger_bin"`
-	JudgerConfig           string             `yaml:"judger_config"`
-	PoolSize               int                `yaml:"pool_size"`
-	KeepWorkdir            *bool              `yaml:"keep_workdir"`
-	Timeouts               WorkerTimeouts     `yaml:"timeouts"`
-	TestcaseCache          WorkerCache        `yaml:"testcase_cache"`
-	UnzipLimits            UnzipLimits        `yaml:"unzip_limits"`
-	ResultTTLSec           int                `yaml:"result_ttl_sec"`
-	CheckerTimeoutMs       int                `yaml:"checker_timeout_ms"`
-	CleanupTimeoutSec      int                `yaml:"cleanup_timeout_sec"`
-	ResultStreamMaxRetries int                `yaml:"result_stream_max_retries"`
-	ResultStreamBackoffMs  int                `yaml:"result_stream_backoff_ms"`
-	AllowHostChecker       *bool              `yaml:"allow_host_checker"`
-	RequireCgroupsV2       *bool              `yaml:"require_cgroups_v2"`
-	GRPCTLS                TLSConfig          `yaml:"grpc_tls"`
-	Metrics                Metrics            `yaml:"metrics"`
-	MetricsPort            int                `yaml:"metrics_port"`
+	ID                string             `yaml:"id"`
+	Addr              string             `yaml:"addr"`
+	Port              int                `yaml:"port"`
+	RedisURL          string             `yaml:"redis_url"`
+	DatabaseURL       string             `yaml:"database_url"`
+	Stream            WorkerStreamConfig `yaml:"stream"`
+	MinIO             MinIOConfig        `yaml:"minio"`
+	Workspace         string             `yaml:"workspace"`
+	JudgerBin         string             `yaml:"judger_bin"`
+	JudgerConfig      string             `yaml:"judger_config"`
+	PoolSize          int                `yaml:"pool_size"`
+	KeepWorkdir       *bool              `yaml:"keep_workdir"`
+	Timeouts          WorkerTimeouts     `yaml:"timeouts"`
+	TestcaseCache     WorkerCache        `yaml:"testcase_cache"`
+	UnzipLimits       UnzipLimits        `yaml:"unzip_limits"`
+	ResultTTLSec      int                `yaml:"result_ttl_sec"`
+	CheckerTimeoutMs  int                `yaml:"checker_timeout_ms"`
+	CleanupTimeoutSec int                `yaml:"cleanup_timeout_sec"`
+	AllowHostChecker  *bool              `yaml:"allow_host_checker"`
+	RequireCgroupsV2  *bool              `yaml:"require_cgroups_v2"`
+	Metrics           Metrics            `yaml:"metrics"`
+	MetricsPort       int                `yaml:"metrics_port"`
 }
 
 type WorkerStreamConfig struct {
diff --git a/src/go/internal/repository/redis.go b/src/go/internal/repository/redis.go
index f3820e4..88a7984 100644
--- a/src/go/internal/repository/redis.go
+++ b/src/go/internal/repository/redis.go
@@ -142,63 +142,6 @@ func (r *RedisClient) Expire(ctx context.Context, key string, expiration time.Du
 	return val, nil
 }
 
-// List 操作 (任务队列)
-
-// LPush 左侧入队
-func (r *RedisClient) LPush(ctx context.Context, key string, values ...interface{}) error {
-	if err := r.client.LPush(ctx, key, values...).Err(); err != nil {
-		return fmt.Errorf("Redis LPUSH %s 失败: %w", key, err)
-	}
-	return nil
-}
-
-// RPop 右侧出队
-func (r *RedisClient) RPop(ctx context.Context, key string) (string, error) {
-	val, err := r.client.RPop(ctx, key).Result()
-	if err != nil && err != redis.Nil {
-		return "", fmt.Errorf("Redis RPOP %s 失败: %w", key, err)
-	}
-	return val, err
-}
-
-// BRPop 阻塞右侧出队
-func (r *RedisClient) BRPop(ctx context.Context, timeout time.Duration, keys ...string) ([]string, error) {
-	result, err := r.client.BRPop(ctx, timeout, keys...).Result()
-	if err == redis.Nil {
-		return nil, nil
-	}
-	if err != nil {
-		return nil, fmt.Errorf("Redis BRPOP 失败: %w", err)
-	}
-	return result, nil
-}
-
-// BRPopLPush 阻塞弹出并推入另一队列 (可靠投递)
-func (r *RedisClient) BRPopLPush(ctx context.Context, source, destination string, timeout time.Duration) (string, error) {
-	val, err := r.client.BRPopLPush(ctx, source, destination, timeout).Result()
-	if err != nil && err != redis.Nil {
-		return "", fmt.Errorf("Redis BRPOPLPUSH 失败: %w", err)
-	}
-	return val, err
-}
-
-// LRem 移除队列中的元素
-func (r *RedisClient) LRem(ctx context.Context, key string, count int64, value interface{}) error {
-	if err := r.client.LRem(ctx, key, count, value).Err(); err != nil {
-		return fmt.Errorf("Redis LREM %s 失败: %w", key, err)
-	}
-	return nil
-}
-
-// LLen 获取队列长度
-func (r *RedisClient) LLen(ctx context.Context, key string) (int64, error) {
-	val, err := r.client.LLen(ctx, key).Result()
-	if err != nil {
-		return 0, fmt.Errorf("Redis LLEN %s 失败: %w", key, err)
-	}
-	return val, nil
-}
-
 // Hash 操作 (Worker 状态)
 
 // HSet 设置 Hash 字段
@@ -235,15 +178,6 @@ func (r *RedisClient) HDel(ctx context.Context, key string, fields ...string) er
 	return nil
 }
 
-// LRange 获取列表范围
-func (r *RedisClient) LRange(ctx context.Context, key string, start, stop int64) ([]string, error) {
-	val, err := r.client.LRange(ctx, key, start, stop).Result()
-	if err != nil {
-		return nil, fmt.Errorf("Redis LRANGE %s 失败: %w", key, err)
-	}
-	return val, nil
-}
-
 // ZAdd 向有序集合添加成员
 func (r *RedisClient) ZAdd(ctx context.Context, key string, members ...*redis.Z) error {
 	items := make([]redis.Z, 0, len(members))
@@ -278,20 +212,6 @@ func (r *RedisClient) ZRangeByScore(ctx context.Context, key string, opt *redis.
 	return val, nil
 }
 
-// RequeueTask 原子地将任务从 processing 移动到 pending (通过事务)
-func (r *RedisClient) RequeueTask(ctx context.Context, src, dst, value string) error {
-	pipe := r.client.TxPipeline()
-	// LREM count=1
-	pipe.LRem(ctx, src, 1, value)
-	// LPUSH
-	pipe.LPush(ctx, dst, value)
-	_, err := pipe.Exec(ctx)
-	if err != nil {
-		return fmt.Errorf("Redis 任务重新入队失败: %w", err)
-	}
-	return nil
-}
-
 // Eval 执行 Lua 脚本
 func (r *RedisClient) Eval(ctx context.Context, script string, keys []string, args ...interface{}) (interface{}, error) {
 	val, err := r.client.Eval(ctx, script, keys, args...).Result()
diff --git a/src/go/internal/scheduler/ack_listener.go b/src/go/internal/scheduler/ack_listener.go
deleted file mode 100644
index 1702180..0000000
--- a/src/go/internal/scheduler/ack_listener.go
+++ /dev/null
@@ -1,200 +0,0 @@
-/**
- * @file ack_listener.go
- * @brief 任务结果确认监听器
- */
-package scheduler
-
-import (
-	"context"
-	"encoding/json"
-	"log/slog"
-	"os"
-	"time"
-
-	"github.com/d1guo/deep_oj/internal/repository"
-	"github.com/d1guo/deep_oj/pkg/common"
-	"github.com/redis/go-redis/v9"
-)
-
-// StartAckListener 启动 ACK 监听
-func StartAckListener(ctx context.Context, redisClient *repository.RedisClient, db *repository.PostgresDB) {
-	slog.Info("启动结果流 ACK 监听器...")
-
-	pendingCount := getEnvInt("ACK_PENDING_COUNT", 20)
-	pendingBlock := time.Duration(getEnvInt("ACK_PENDING_BLOCK_MS", 1000)) * time.Millisecond
-	newCount := getEnvInt("ACK_NEW_COUNT", 10)
-	newBlock := time.Duration(getEnvInt("ACK_NEW_BLOCK_MS", 5000)) * time.Millisecond
-
-	// 创建消费组（幂等）
-	if err := redisClient.XGroupCreateMkStream(ctx, common.ResultStream, common.ResultStreamGroup, "0"); err != nil {
-		slog.Error("创建流消费组失败", "error", err)
-	}
-	consumer := os.Getenv("SCHEDULER_ID")
-	if consumer == "" {
-		consumer = common.ResultStreamConsumer
-	}
-
-	// 尝试处理遗留的 Pending 消息
-	if streams, err := redisClient.XReadGroup(ctx, &redis.XReadGroupArgs{
-		Group:    common.ResultStreamGroup,
-		Consumer: consumer,
-		Streams:  []string{common.ResultStream, "0"},
-		Count:    int64(pendingCount),
-		Block:    pendingBlock,
-	}); err == nil {
-		for _, s := range streams {
-			for _, msg := range s.Messages {
-				jobID, _ := msg.Values["job_id"].(string)
-				resultJSON, _ := msg.Values["result"].(string)
-				if jobID == "" {
-					_ = redisClient.XAck(ctx, common.ResultStream, common.ResultStreamGroup, msg.ID)
-					continue
-				}
-				if handleTaskCompletion(ctx, jobID, resultJSON, redisClient, db) {
-					_ = redisClient.XAck(ctx, common.ResultStream, common.ResultStreamGroup, msg.ID)
-				} else {
-					slog.Warn("结果处理失败，保留在待确认队列", "job_id", jobID, "id", msg.ID)
-				}
-			}
-		}
-	}
-
-	for {
-		select {
-		case <-ctx.Done():
-			return
-		default:
-		}
-
-		streams, err := redisClient.XReadGroup(ctx, &redis.XReadGroupArgs{
-			Group:    common.ResultStreamGroup,
-			Consumer: consumer,
-			Streams:  []string{common.ResultStream, ">"},
-			Count:    int64(newCount),
-			Block:    newBlock,
-		})
-		if err != nil {
-			slog.Error("XReadGroup 失败", "error", err)
-			continue
-		}
-		if len(streams) == 0 {
-			continue
-		}
-
-		for _, s := range streams {
-			for _, msg := range s.Messages {
-				jobID, _ := msg.Values["job_id"].(string)
-				resultJSON, _ := msg.Values["result"].(string)
-				if jobID == "" {
-					slog.Warn("流消息缺少 job_id", "id", msg.ID)
-					_ = redisClient.XAck(ctx, common.ResultStream, common.ResultStreamGroup, msg.ID)
-					continue
-				}
-				slog.Debug("收到任务结果", "job_id", jobID)
-
-				if handleTaskCompletion(ctx, jobID, resultJSON, redisClient, db) {
-					if err := redisClient.XAck(ctx, common.ResultStream, common.ResultStreamGroup, msg.ID); err != nil {
-						slog.Error("XAck 失败", "job_id", jobID, "error", err)
-					}
-				} else {
-					slog.Warn("结果处理失败，保留在待确认队列", "job_id", jobID, "id", msg.ID)
-				}
-			}
-		}
-	}
-}
-
-func handleTaskCompletion(ctx context.Context, jobID, resultJSON string, redisClient *repository.RedisClient, db *repository.PostgresDB) bool {
-	payloadKey := common.TaskPayloadPrefix + jobID
-	rawItem, err := redisClient.Get(ctx, payloadKey)
-	if err != nil {
-		rawItem = ""
-	}
-
-	// 1. 处理结果 (同步到 DB + 更新指标)
-	if !processResult(ctx, jobID, resultJSON, redisClient, db) {
-		return false
-	}
-
-	// 2. 从队列移除
-	if rawItem != "" {
-		if err := redisClient.LRem(ctx, common.QueueProcessing, 1, rawItem); err != nil {
-			slog.Error("从 processing 队列移除任务失败", "job_id", jobID, "error", err)
-		} else {
-			slog.Debug("已从 processing 队列移除任务", "job_id", jobID)
-		}
-	}
-
-	// 3. 清理辅助键并回收 inflight
-	assignmentKey := common.TaskAssignmentPrefix + jobID
-	workerID, _ := redisClient.Get(ctx, assignmentKey)
-	if workerID != "" {
-		_, _ = redisClient.Decr(ctx, common.WorkerInflightPrefix+workerID)
-	}
-	_ = redisClient.Del(ctx,
-		payloadKey,
-		assignmentKey,
-		common.TaskProcessingStartPrefix+jobID,
-	)
-	_ = redisClient.ZRem(ctx, common.TaskProcessingZSet, jobID)
-	return true
-}
-
-func processResult(ctx context.Context, jobID string, resultJSON string, redisClient *repository.RedisClient, db *repository.PostgresDB) bool {
-	// 1. 从 Redis 读取结果（当流中 result 为空时回退）。
-	if resultJSON == "" {
-		resultKey := common.ResultKeyPrefix + jobID
-		val, err := redisClient.Get(ctx, resultKey)
-		if err != nil || val == "" {
-			slog.Error("在 Redis 中未找到结果", "job_id", jobID, "error", err)
-			return false
-		}
-		resultJSON = val
-	}
-
-	// 2. 解析结果
-	var result map[string]interface{}
-	if err := json.Unmarshal([]byte(resultJSON), &result); err != nil {
-		slog.Error("解析结果 JSON 失败", "job_id", jobID, "error", err)
-		return false
-	}
-
-	status, _ := result["status"].(string)     // 例如 "Accepted"、"Wrong Answer"
-	language, _ := result["language"].(string) // 从工作节点结果读取语言
-	if language == "" {
-		language = "未知"
-	}
-
-	traceID, _ := result["trace_id"].(string)
-	cacheKey, _ := result["cache_key"].(string)
-
-	// 更新业务指标
-	submissionResultTotal.WithLabelValues(status, language).Inc()
-
-	// 3. 更新 PostgreSQL (幂等: 仅在未完成时更新)
-	latency, updated, err := db.UpdateSubmissionResultIfNotDone(ctx, jobID, status, result)
-	if err != nil {
-		slog.Error("更新数据库提交结果失败", "job_id", jobID, "error", err)
-		return false
-	}
-	if !updated {
-		slog.Warn("重复结果已忽略（已完成）", "job_id", jobID)
-		return true
-	}
-
-	// 记录耗时指标
-	schedulerJobLatency.Observe(latency)
-
-	slog.Info("任务完成并已同步到数据库",
-		"job_id", jobID,
-		"status", status,
-		"lang", language,
-		"latency_s", latency,
-		"trace_id", traceID,
-	)
-
-	if cacheKey != "" {
-		_ = redisClient.Del(ctx, common.InFlightKeyPrefix+cacheKey)
-	}
-	return true
-}
diff --git a/src/go/internal/scheduler/dispatch.go b/src/go/internal/scheduler/dispatch.go
deleted file mode 100644
index 81b309c..0000000
--- a/src/go/internal/scheduler/dispatch.go
+++ /dev/null
@@ -1,132 +0,0 @@
-package scheduler
-
-import (
-	"context"
-	"crypto/tls"
-	"crypto/x509"
-	"fmt"
-	"log/slog"
-	"os"
-	"strings"
-	"time"
-
-	"github.com/d1guo/deep_oj/internal/repository"
-	"github.com/d1guo/deep_oj/pkg/common"
-	pb "github.com/d1guo/deep_oj/pkg/proto" // Keep pkg/proto for gRPC request construction
-	"google.golang.org/grpc"
-	"google.golang.org/grpc/credentials"
-	"google.golang.org/grpc/credentials/insecure"
-	"google.golang.org/grpc/metadata"
-	"google.golang.org/protobuf/proto"
-)
-
-// DispatchTask 将任务分发给 Worker (支持重试)
-func DispatchTask(ctx context.Context, workerAddr string, taskData []byte, redis *repository.RedisClient) error {
-	// 1. 解析任务 (Protobuf)
-	task := &pb.TaskRequest{}
-	if err := proto.Unmarshal(taskData, task); err != nil {
-		// 无法解析的数据，永远无法成功 -> NonRetryable
-		return fmt.Errorf("unmarshal task: %w (%w)", err, common.ErrNonRetryable)
-	}
-
-	logger := slog.With("job_id", task.JobId, "worker", workerAddr, "trace_id", task.TraceId)
-	logger.InfoContext(ctx, "Dispatching task")
-	workerAuthToken := strings.TrimSpace(os.Getenv("WORKER_AUTH_TOKEN"))
-	allowInsecure := getEnvBool("ALLOW_INSECURE_WORKER_GRPC", true)
-	if workerAuthToken == "" {
-		return fmt.Errorf("WORKER_AUTH_TOKEN is required for scheduler->worker dispatch (%w)", common.ErrNonRetryable)
-	}
-
-	// 2. 建立 gRPC 连接 (带超时)
-	connTimeoutMs := getEnvInt("DISPATCH_CONN_TIMEOUT_MS", 3000)
-	connCtx, cancelConn := context.WithTimeout(ctx, time.Duration(connTimeoutMs)*time.Millisecond)
-	defer cancelConn()
-
-	creds, err := loadClientCreds()
-	if err != nil {
-		return fmt.Errorf("load tls creds: %w (%w)", err, common.ErrRetryable)
-	}
-	if creds == nil {
-		if !allowInsecure {
-			return fmt.Errorf("TLS credentials missing and insecure mode disabled (%w)", common.ErrNonRetryable)
-		}
-		creds = insecure.NewCredentials()
-	}
-	conn, err := grpc.DialContext(connCtx, workerAddr,
-		grpc.WithTransportCredentials(creds),
-		grpc.WithBlock(),
-	)
-	if err != nil {
-		// 连接失败 -> Retryable
-		return fmt.Errorf("grpc dial %s: %w (%w)", workerAddr, err, common.ErrRetryable)
-	}
-	defer conn.Close()
-
-	client := pb.NewJudgeServiceClient(conn)
-
-	// 3. 构造请求 (直接复用 task)
-	req := task
-
-	// 4. 重试策略 (指数退避)
-	maxRetries := getEnvInt("DISPATCH_MAX_RETRIES", 3)
-	backoffBaseMs := getEnvInt("DISPATCH_BACKOFF_BASE_MS", 100)
-	var lastErr error
-
-	for i := 0; i < maxRetries; i++ {
-		// RPC 超时
-		rpcTimeoutMs := getEnvInt("DISPATCH_RPC_TIMEOUT_MS", 5000)
-		rpcCtx, cancelRPC := context.WithTimeout(ctx, time.Duration(rpcTimeoutMs)*time.Millisecond) // 发送请求不需要很久，执行是异步的
-		if workerAuthToken != "" {
-			rpcCtx = metadata.AppendToOutgoingContext(rpcCtx, "x-worker-auth-token", workerAuthToken)
-		}
-
-		_, err := client.ExecuteTask(rpcCtx, req)
-		cancelRPC()
-
-		if err == nil {
-			logger.InfoContext(ctx, "Task accepted by worker")
-			// 成功分发，不需要在这里等待结果，由 ACK Listener 处理
-			return nil
-		}
-
-		lastErr = err
-		logger.WarnContext(ctx, "Dispatch failed", "attempt", i+1, "max_retries", maxRetries, "error", err)
-
-		// 指数退避: base, base*2, base*4...
-		backoff := time.Duration(backoffBaseMs*(1<<i)) * time.Millisecond
-		select {
-		case <-ctx.Done():
-			return fmt.Errorf("context cancelled during retry: %w", ctx.Err())
-		case <-time.After(backoff):
-			// continue
-		}
-	}
-
-	return fmt.Errorf("dispatch failed after %d retries: %w (%w)", maxRetries, lastErr, common.ErrRetryable)
-}
-
-func loadClientCreds() (credentials.TransportCredentials, error) {
-	certFile := os.Getenv("GRPC_TLS_CERT")
-	keyFile := os.Getenv("GRPC_TLS_KEY")
-	caFile := os.Getenv("GRPC_TLS_CA")
-	if certFile == "" || keyFile == "" || caFile == "" {
-		return nil, nil
-	}
-	cert, err := tls.LoadX509KeyPair(certFile, keyFile)
-	if err != nil {
-		return nil, err
-	}
-	caData, err := os.ReadFile(caFile)
-	if err != nil {
-		return nil, err
-	}
-	certPool := x509.NewCertPool()
-	certPool.AppendCertsFromPEM(caData)
-
-	tlsConfig := &tls.Config{
-		Certificates: []tls.Certificate{cert},
-		RootCAs:      certPool,
-		MinVersion:   tls.VersionTLS12,
-	}
-	return credentials.NewTLS(tlsConfig), nil
-}
diff --git a/src/go/internal/scheduler/metrics.go b/src/go/internal/scheduler/metrics.go
index c375e30..dbce3c9 100644
--- a/src/go/internal/scheduler/metrics.go
+++ b/src/go/internal/scheduler/metrics.go
@@ -2,12 +2,9 @@ package scheduler
 
 import (
 	"context"
-	"log/slog"
 	"os"
 	"time"
 
-	"github.com/d1guo/deep_oj/internal/repository"
-	"github.com/d1guo/deep_oj/pkg/common"
 	"github.com/prometheus/client_golang/prometheus"
 )
 
@@ -26,15 +23,6 @@ func metricLabels() prometheus.Labels {
 var reg = prometheus.WrapRegistererWith(metricLabels(), prometheus.DefaultRegisterer)
 
 var (
-	// System Metrics
-	schedulerQueueDepth = prometheus.NewGaugeVec(
-		prometheus.GaugeOpts{
-			Name: "scheduler_queue_depth",
-			Help: "Redis 队列当前任务深度",
-		},
-		[]string{"queue"},
-	)
-
 	schedulerActiveWorkers = prometheus.NewGauge(
 		prometheus.GaugeOpts{
 			Name: "scheduler_active_workers",
@@ -42,43 +30,23 @@ var (
 		},
 	)
 
-	// Business Metrics
-	submissionResultTotal = prometheus.NewCounterVec(
-		prometheus.CounterOpts{
-			Name: "submission_result_total",
-			Help: "按状态和语言统计的已处理提交总数",
-		},
-		[]string{"status", "language"},
-	)
-
-	schedulerJobLatency = prometheus.NewHistogram(
-		prometheus.HistogramOpts{
-			Name:    "job_latency_seconds",
-			Help:    "任务端到端处理耗时 (seconds)",
-			Buckets: prometheus.DefBuckets, // default buckets are fine for now
-		},
-	)
-
 	controlPlaneOnlyGauge = prometheus.NewGauge(
 		prometheus.GaugeOpts{
 			Name: "control_plane_only",
-			Help: "Whether scheduler runs in control-plane-only mode (1=true, 0=false)",
+			Help: "调度器是否处于仅控制面模式（1=是，0=否）",
 		},
 	)
 
 	legacyLoopsStartedGauge = prometheus.NewGauge(
 		prometheus.GaugeOpts{
 			Name: "legacy_loops_started",
-			Help: "Number of legacy scheduler data-plane loops started",
+			Help: "legacy 数据面循环启动次数",
 		},
 	)
 )
 
 func init() {
-	reg.MustRegister(schedulerQueueDepth)
 	reg.MustRegister(schedulerActiveWorkers)
-	reg.MustRegister(submissionResultTotal)
-	reg.MustRegister(schedulerJobLatency)
 	reg.MustRegister(controlPlaneOnlyGauge)
 	reg.MustRegister(legacyLoopsStartedGauge)
 }
@@ -98,8 +66,8 @@ func SetLegacyLoopsStarted(count int) {
 	legacyLoopsStartedGauge.Set(float64(count))
 }
 
-// StartMetricsPoller starts a background loop to update Gauge metrics
-func StartMetricsPoller(ctx context.Context, redis *repository.RedisClient, discovery *WorkerDiscovery) {
+// StartMetricsPoller 周期刷新调度器指标。
+func StartMetricsPoller(ctx context.Context, discovery *WorkerDiscovery) {
 	pollMs := getEnvInt("SCHEDULER_METRICS_POLL_INTERVAL_MS", 1000)
 	ticker := time.NewTicker(time.Duration(pollMs) * time.Millisecond)
 	defer ticker.Stop()
@@ -109,31 +77,12 @@ func StartMetricsPoller(ctx context.Context, redis *repository.RedisClient, disc
 		case <-ctx.Done():
 			return
 		case <-ticker.C:
-			updateQueueMetrics(ctx, redis)
-			updateWorkerMetrics(ctx, discovery)
+			updateWorkerMetrics(discovery)
 		}
 	}
 }
 
-func updateQueueMetrics(ctx context.Context, redis *repository.RedisClient) {
-	// 1. Pending Queue
-	pendingCount, err := redis.LLen(ctx, common.QueuePending)
-	if err != nil {
-		slog.Error("获取待处理队列长度失败", "error", err)
-	} else {
-		schedulerQueueDepth.WithLabelValues("pending").Set(float64(pendingCount))
-	}
-
-	// 2. Processing Queue
-	processingCount, err := redis.LLen(ctx, common.QueueProcessing)
-	if err != nil {
-		slog.Error("获取处理中队列长度失败", "error", err)
-	} else {
-		schedulerQueueDepth.WithLabelValues("processing").Set(float64(processingCount))
-	}
-}
-
-func updateWorkerMetrics(ctx context.Context, discovery *WorkerDiscovery) {
+func updateWorkerMetrics(discovery *WorkerDiscovery) {
 	count := discovery.GetWorkerCount()
 	schedulerActiveWorkers.Set(float64(count))
 }
diff --git a/src/go/internal/scheduler/retry.go b/src/go/internal/scheduler/retry.go
deleted file mode 100644
index 1331fdf..0000000
--- a/src/go/internal/scheduler/retry.go
+++ /dev/null
@@ -1,46 +0,0 @@
-package scheduler
-
-import (
-	"context"
-	"os"
-	"strconv"
-	"time"
-
-	"github.com/d1guo/deep_oj/internal/repository"
-	"github.com/d1guo/deep_oj/pkg/common"
-)
-
-func maxRetry() int64 {
-	if v := os.Getenv("MAX_RETRY"); v != "" {
-		if n, err := strconv.Atoi(v); err == nil && n > 0 {
-			return int64(n)
-		}
-	}
-	return 3
-}
-
-// HandleRetry either requeues or moves to DLQ when retry exceeds max.
-func HandleRetry(ctx context.Context, redis *repository.RedisClient, db *repository.PostgresDB, jobID, taskData, reason string) error {
-	retryKey := common.TaskRetryPrefix + jobID
-	retryCount, _ := redis.Incr(ctx, retryKey)
-	retryTTLSec := getEnvInt("RETRY_TTL_SEC", 86400)
-	_, _ = redis.Expire(ctx, retryKey, time.Duration(retryTTLSec)*time.Second)
-
-	if retryCount > maxRetry() {
-		_ = redis.LPush(ctx, common.QueueDead, taskData)
-		if db != nil {
-			_ = db.UpdateSubmissionState(ctx, jobID, "failed")
-			_, _, _ = db.UpdateSubmissionResultIfNotDone(ctx, jobID, "System Error", map[string]any{
-				"status":        "System Error",
-				"error_message": reason,
-				"retry":         retryCount,
-			})
-		}
-		return nil
-	}
-
-	if db != nil {
-		_ = db.UpdateSubmissionState(ctx, jobID, "retry")
-	}
-	return redis.RequeueTask(ctx, common.QueueProcessing, common.QueuePending, taskData)
-}
diff --git a/src/go/internal/scheduler/slow_path.go b/src/go/internal/scheduler/slow_path.go
deleted file mode 100644
index e6c5a64..0000000
--- a/src/go/internal/scheduler/slow_path.go
+++ /dev/null
@@ -1,133 +0,0 @@
-package scheduler
-
-import (
-	"context"
-	"crypto/sha256"
-	"encoding/hex"
-	"fmt"
-	"log/slog"
-	"time"
-
-	"github.com/d1guo/deep_oj/internal/model"
-	"github.com/d1guo/deep_oj/internal/repository"
-	"github.com/d1guo/deep_oj/pkg/common"
-	pb "github.com/d1guo/deep_oj/pkg/proto"
-	"github.com/redis/go-redis/v9"
-	"google.golang.org/protobuf/proto"
-)
-
-// StartSlowPath 启动慢路径兜底（处理超时任务）。
-func StartSlowPath(ctx context.Context, redisClient *repository.RedisClient, db *repository.PostgresDB) {
-	slog.Info("启动慢路径监控（保障双写一致性）...")
-	tickSec := getEnvInt("SLOW_PATH_TICK_SEC", 60)
-	ticker := time.NewTicker(time.Duration(tickSec) * time.Second) // 数据库扫描成本较高，频率不宜过高。
-	defer ticker.Stop()
-
-	for {
-		select {
-		case <-ctx.Done():
-			return
-		case <-ticker.C:
-			// 1. 核心：扫描数据库，防止任务丢失（双写一致性）。
-			checkLostTasks(ctx, redisClient, db)
-
-			// 2. 辅助：扫描 Redis，防止处理超时（工作节点崩溃）。
-			// 注意：看门狗已覆盖该场景，这里作为第二道防线。
-			checkTimeoutTasks(ctx, redisClient, db)
-		}
-	}
-}
-
-// checkLostTasks 扫描数据库中长期 Pending 的任务，并确认是否仍在 Redis 中。
-func checkLostTasks(ctx context.Context, redisClient *repository.RedisClient, db *repository.PostgresDB) {
-	pendingStaleSec := getEnvInt("PENDING_STALE_SEC", 60)
-	before := time.Now().Add(-time.Duration(pendingStaleSec) * time.Second)
-	dbScanLimit := getEnvInt("SLOW_PATH_DB_SCAN_LIMIT", 100)
-	submissions, err := db.GetPendingSubmissions(ctx, before, dbScanLimit)
-	if err != nil {
-		slog.Error("慢路径：数据库扫描失败", "error", err)
-		return
-	}
-
-	if len(submissions) == 0 {
-		return
-	}
-
-	slog.Info("慢路径：在数据库发现待处理任务", "count", len(submissions))
-
-	for _, sub := range submissions {
-		// 检查 Redis 中是否存在 (Pending 或 Processing)
-		// 注意：该检查开销较大，慢路径频率不能太高。
-		// 目前策略：直接构造 TaskRequest 并重新入队，依赖工作节点幂等与结果缓存兜底。
-
-		slog.Info("慢路径：正在恢复任务", "job_id", sub.JobID)
-
-		cacheKey := buildRecoveredCacheKey(sub)
-
-		task := &pb.TaskRequest{
-			JobId:       sub.JobID,
-			Code:        []byte(sub.Code),
-			Language:    pb.Language(sub.Language),
-			TimeLimit:   int32(sub.TimeLimit),
-			MemoryLimit: int32(sub.MemoryLimit),
-			CacheKey:    cacheKey,
-			ProblemId:   uint32(sub.ProblemID),
-			SubmitTime:  sub.CreatedAt.UnixMilli(),
-			TraceId:     sub.JobID,
-		}
-
-		taskData, _ := proto.Marshal(task)
-
-		// 重新推入 Redis Pending 队列
-		if err := redisClient.LPush(ctx, common.QueuePending, string(taskData)); err != nil {
-			slog.Error("慢路径：重新入队失败", "job_id", sub.JobID, "error", err)
-		} else {
-			slog.Info("慢路径：任务已重新入队", "job_id", sub.JobID)
-		}
-	}
-}
-
-func buildRecoveredCacheKey(sub *model.Submission) string {
-	sum := sha256.Sum256([]byte(fmt.Sprintf("%s|%d|%d|%d|%d", sub.Code, sub.Language, sub.TimeLimit, sub.MemoryLimit, sub.ProblemID)))
-	return common.CacheKeyPrefix + hex.EncodeToString(sum[:])
-}
-
-func checkTimeoutTasks(ctx context.Context, redisClient *repository.RedisClient, db *repository.PostgresDB) {
-	// 1. 获取超时的 processing 任务（基于 processing_start）。
-	cutoffSec := getEnvInt("SLOW_PATH_PROCESSING_CUTOFF_SEC", 30)
-	cutoff := time.Now().Add(-time.Duration(cutoffSec) * time.Second).UnixMilli()
-	jobIDs, err := redisClient.ZRangeByScore(ctx, common.TaskProcessingZSet, &redis.ZRangeBy{
-		Min: "-inf",
-		Max: fmt.Sprintf("%d", cutoff),
-	})
-	if err != nil {
-		slog.Error("慢路径：查询 processing zset 失败", "error", err)
-		return
-	}
-	if len(jobIDs) == 0 {
-		return
-	}
-
-	recoveredCount := 0
-	for _, jobID := range jobIDs {
-		payloadKey := common.TaskPayloadPrefix + jobID
-		taskData, err := redisClient.Get(ctx, payloadKey)
-		if err != nil {
-			continue
-		}
-
-		slog.Warn("慢路径：processing zset 中任务超时", "job_id", jobID)
-		if err := HandleRetry(ctx, redisClient, db, jobID, taskData, "慢路径超时回收"); err != nil {
-			slog.Error("慢路径：超时任务重新入队失败", "job_id", jobID, "error", err)
-			continue
-		}
-
-		_ = redisClient.Del(ctx, common.TaskAssignmentPrefix+jobID, payloadKey, common.TaskProcessingStartPrefix+jobID)
-		_ = redisClient.ZRem(ctx, common.TaskProcessingZSet, jobID)
-		recoveredCount++
-	}
-
-	if recoveredCount > 0 {
-		slog.Info("慢路径：已从 Redis 恢复超时任务", "count", recoveredCount)
-	}
-}
diff --git a/src/go/internal/scheduler/watchdog.go b/src/go/internal/scheduler/watchdog.go
deleted file mode 100644
index 6edc09e..0000000
--- a/src/go/internal/scheduler/watchdog.go
+++ /dev/null
@@ -1,118 +0,0 @@
-package scheduler
-
-import (
-	"context"
-	"fmt"
-	"log/slog"
-	"time"
-
-	"github.com/d1guo/deep_oj/internal/repository"
-	"github.com/d1guo/deep_oj/pkg/common"
-	"github.com/redis/go-redis/v9"
-)
-
-const (
-	// TaskAssignmentPrefix 表示手动任务分配关系键：task:worker:{jobID} -> workerID
-	TaskAssignmentPrefix = common.TaskAssignmentPrefix
-	// TaskProcessingQueue 表示处理中任务的 Redis 列表
-	TaskProcessingQueue = common.QueueProcessing
-	// TaskPendingQueue 表示待处理任务的 Redis 列表
-	TaskPendingQueue = common.QueuePending
-)
-
-// Watchdog 负责扫描处理中队列，回收僵尸任务。
-type Watchdog struct {
-	redisClient *repository.RedisClient
-	discovery   *WorkerDiscovery
-	db          *repository.PostgresDB
-	interval    time.Duration
-}
-
-// NewWatchdog 创建看门狗实例。
-func NewWatchdog(redisClient *repository.RedisClient, discovery *WorkerDiscovery, db *repository.PostgresDB, interval time.Duration) *Watchdog {
-	return &Watchdog{
-		redisClient: redisClient,
-		discovery:   discovery,
-		db:          db,
-		interval:    interval,
-	}
-}
-
-// Start 启动看门狗循环。
-func (w *Watchdog) Start(ctx context.Context) {
-	slog.Info("看门狗启动", "interval", w.interval)
-	ticker := time.NewTicker(w.interval)
-	defer ticker.Stop()
-
-	for {
-		select {
-		case <-ctx.Done():
-			slog.Info("看门狗停止")
-			return
-		case <-ticker.C:
-			if err := w.scanAndReclaim(ctx); err != nil {
-				slog.Error("看门狗扫描失败", "error", err)
-			}
-		}
-	}
-}
-
-// scanAndReclaim 扫描处理中队列并回收需要重试的任务。
-func (w *Watchdog) scanAndReclaim(ctx context.Context) error {
-	// 1. 从 processing ZSET 读取超时任务。
-	cutoff := time.Now().Add(-w.interval).UnixMilli()
-	tasks, err := w.redisClient.ZRangeByScore(ctx, common.TaskProcessingZSet, &redis.ZRangeBy{
-		Min: "-inf",
-		Max: fmt.Sprintf("%d", cutoff),
-	})
-	if err != nil {
-		return fmt.Errorf("读取 processing 任务列表失败: %w", err)
-	}
-
-	if len(tasks) == 0 {
-		return nil
-	}
-
-	for _, jobID := range tasks {
-		// 2. 检查任务分配关系。
-		assignmentKey := TaskAssignmentPrefix + jobID
-		workerID, err := w.redisClient.Get(ctx, assignmentKey)
-
-		if err != nil {
-			// 分配键缺失：先跳过，避免短暂不一致导致误判。
-			continue
-		}
-
-		// 3. 检查该工作节点是否仍在线。
-		if !w.discovery.IsWorkerActive(workerID) {
-			slog.Warn("看门狗：工作节点离线，任务重新入队", "worker_id", workerID, "job_id", jobID)
-
-			err := w.requeueTask(ctx, jobID, assignmentKey)
-			if err != nil {
-				slog.Error("看门狗：任务重新入队失败", "job_id", jobID, "error", err)
-			} else {
-				slog.Info("看门狗：任务已重新入队", "job_id", jobID)
-			}
-		}
-	}
-
-	return nil
-}
-
-// requeueTask 将任务重新入队并清理关联键。
-func (w *Watchdog) requeueTask(ctx context.Context, jobID, assignmentKey string) error {
-	payloadKey := common.TaskPayloadPrefix + jobID
-	taskData, err := w.redisClient.Get(ctx, payloadKey)
-	if err != nil {
-		return err
-	}
-	if err := HandleRetry(ctx, w.redisClient, w.db, jobID, taskData, "看门狗重新入队"); err != nil {
-		return err
-	}
-
-	// 清理分配键与相关状态，避免下次扫描重复处理。
-	if err := w.redisClient.Del(ctx, assignmentKey, payloadKey, common.TaskProcessingStartPrefix+jobID); err != nil {
-		return err
-	}
-	return w.redisClient.ZRem(ctx, common.TaskProcessingZSet, jobID)
-}
diff --git a/src/go/internal/survey/validator.go b/src/go/internal/survey/validator.go
index bd27325..479aab8 100644
--- a/src/go/internal/survey/validator.go
+++ b/src/go/internal/survey/validator.go
@@ -16,13 +16,16 @@ var RequiredRepoSurveySections = []string{
 
 // RequiredRepoSurveyTokens ensures the survey keeps concrete keys, commands and code paths.
 var RequiredRepoSurveyTokens = []string{
-	"queue:pending",
-	"queue:processing",
-	"stream:results",
+	"deepoj:jobs",
+	"outbox_events",
 	"sql/migrations/001_init.sql",
+	"sql/migrations/007_add_outbox_events.sql",
 	"src/go/internal/repository/postgres.go",
+	"src/go/internal/repository/postgres_outbox.go",
 	"src/go/cmd/scheduler/main.go",
-	"src/go/internal/scheduler/ack_listener.go",
+	"src/go/internal/scheduler/metrics.go",
+	"src/go/internal/api/outbox_dispatcher.go",
+	"src/go/internal/worker/stream_consumer.go",
 	"src/go/internal/worker/judge.go",
 	"src/go/internal/api/metrics.go",
 	"scripts/repo_survey_probe.sh",
diff --git a/src/go/internal/worker/config.go b/src/go/internal/worker/config.go
index 4cd2df5..87dd69c 100644
--- a/src/go/internal/worker/config.go
+++ b/src/go/internal/worker/config.go
@@ -14,7 +14,7 @@ import (
 
 type Config struct {
 	WorkerID              string
-	WorkerAddr            string // gRPC listen addr
+	WorkerAddr            string
 	RedisURL              string
 	DatabaseURL           string
 	MinIOEndpoint         string
diff --git a/src/go/internal/worker/judge.go b/src/go/internal/worker/judge.go
index b73b994..636c0f9 100644
--- a/src/go/internal/worker/judge.go
+++ b/src/go/internal/worker/judge.go
@@ -23,7 +23,6 @@ import (
 )
 
 type JudgeService struct {
-	pb.UnimplementedJudgeServiceServer
 	config   *Config
 	executor *Executor
 	tcMgr    *TestCaseManager
@@ -262,63 +261,16 @@ func (s *JudgeService) reportResult(ctx context.Context, jobID string, result ma
 	jsonBytes, _ := json.Marshal(result)
 
 	resultKey := common.ResultKeyPrefix + jobID
-	setOK := false
-	if ok, err := s.redis.SetNX(ctx, resultKey, string(jsonBytes), getResultTTL()).Result(); err != nil {
+	if _, err := s.redis.SetNX(ctx, resultKey, string(jsonBytes), getResultTTL()).Result(); err != nil {
 		slog.Error("Redis SetNX 失败", "job_id", jobID, "error", err)
 		return err
-	} else {
-		setOK = ok
-		if !setOK {
-			slog.Warn("检测到重复结果，仍写入流", "job_id", jobID)
-		}
-	}
-
-	maxRetries := getEnvInt("RESULT_STREAM_MAX_RETRIES", 3)
-	if maxRetries <= 0 {
-		maxRetries = 3
-	}
-	backoffBaseMs := getEnvInt("RESULT_STREAM_BACKOFF_MS", 100)
-	if backoffBaseMs <= 0 {
-		backoffBaseMs = 100
-	}
-	var lastErr error
-	for i := 0; i < maxRetries; i++ {
-		if err := s.redis.XAdd(ctx, &redis.XAddArgs{
-			Stream: common.ResultStream,
-			Values: map[string]interface{}{
-				"job_id": jobID,
-				"result": string(jsonBytes),
-			},
-		}).Err(); err != nil {
-			lastErr = err
-			time.Sleep(time.Duration(backoffBaseMs*(1<<i)) * time.Millisecond)
-			continue
-		}
-		lastErr = nil
-		break
-	}
-	if lastErr != nil {
-		slog.Error("Redis Stream 写入失败", "job_id", jobID, "error", lastErr)
-		if setOK {
-			_ = s.redis.Del(ctx, resultKey).Err()
-		}
-		return lastErr
-	}
-
-	// Optional: keep PubSub for debugging/legacy
-	if err := s.redis.Publish(ctx, "job_done", jobID).Err(); err != nil {
-		slog.Error("Redis 发布失败", "job_id", jobID, "error", err)
 	}
 
 	traceID, _ := result["trace_id"].(string)
-	slog.Info("已上报结果", "job_id", jobID, "trace_id", traceID, "app_status", result["status"])
+	slog.Info("已写入判题结果缓存", "job_id", jobID, "trace_id", traceID, "app_status", result["status"])
 	return nil
 }
 
-func (s *JudgeService) UpdateStatus(ctx context.Context, req *pb.TaskResult) (*pb.Ack, error) {
-	return &pb.Ack{}, nil
-}
-
 // 辅助函数
 
 func UnzipWithLimits(src, dest string, maxTotalBytes, maxFileBytes int64, maxFiles int) error {
diff --git a/src/go/pkg/common/consts.go b/src/go/pkg/common/consts.go
index fe1ea8b..a589648 100644
--- a/src/go/pkg/common/consts.go
+++ b/src/go/pkg/common/consts.go
@@ -1,26 +1,13 @@
 package common
 
 const (
-	// Redis Keys
-	QueuePending    = "queue:pending"
-	QueueProcessing = "queue:processing"
-	QueueDead       = "queue:dead"
-
 	// Cache Keys Prefix
 	CacheKeyPrefix    = "oj:cache:"
 	ResultKeyPrefix   = "result:"
 	InFlightKeyPrefix = "oj:inflight:"
 
-	// Task Assignment Prefix
-	TaskAssignmentPrefix      = "task:worker:"
+	// Streams-only 任务载荷键前缀
 	TaskPayloadPrefix         = "task:payload:"
-	TaskProcessingStartPrefix = "task:processing_start:"
-	TaskProcessingZSet        = "task:processing:zset"
-	TaskRetryPrefix           = "task:retry:"
-	ResultStream              = "stream:results"
-	ResultStreamGroup         = "results-group"
-	ResultStreamConsumer      = "scheduler-1"
-	WorkerInflightPrefix      = "worker:inflight:"
 
 	// Job Status
 	StatusPending    = "pending"
diff --git a/src/go/pkg/proto/judge.pb.go b/src/go/pkg/proto/judge.pb.go
index e0e8eab..979e008 100644
--- a/src/go/pkg/proto/judge.pb.go
+++ b/src/go/pkg/proto/judge.pb.go
@@ -2,7 +2,7 @@
 // versions:
 // 	protoc-gen-go v1.36.11
 // 	protoc        v3.21.12
-// source: proto/judge.proto
+// source: judge.proto
 
 package proto
 
@@ -60,11 +60,11 @@ func (x Language) String() string {
 }
 
 func (Language) Descriptor() protoreflect.EnumDescriptor {
-	return file_proto_judge_proto_enumTypes[0].Descriptor()
+	return file_judge_proto_enumTypes[0].Descriptor()
 }
 
 func (Language) Type() protoreflect.EnumType {
-	return &file_proto_judge_proto_enumTypes[0]
+	return &file_judge_proto_enumTypes[0]
 }
 
 func (x Language) Number() protoreflect.EnumNumber {
@@ -73,7 +73,7 @@ func (x Language) Number() protoreflect.EnumNumber {
 
 // Deprecated: Use Language.Descriptor instead.
 func (Language) EnumDescriptor() ([]byte, []int) {
-	return file_proto_judge_proto_rawDescGZIP(), []int{0}
+	return file_judge_proto_rawDescGZIP(), []int{0}
 }
 
 type JudgeStatus int32
@@ -124,11 +124,11 @@ func (x JudgeStatus) String() string {
 }
 
 func (JudgeStatus) Descriptor() protoreflect.EnumDescriptor {
-	return file_proto_judge_proto_enumTypes[1].Descriptor()
+	return file_judge_proto_enumTypes[1].Descriptor()
 }
 
 func (JudgeStatus) Type() protoreflect.EnumType {
-	return &file_proto_judge_proto_enumTypes[1]
+	return &file_judge_proto_enumTypes[1]
 }
 
 func (x JudgeStatus) Number() protoreflect.EnumNumber {
@@ -137,13 +137,14 @@ func (x JudgeStatus) Number() protoreflect.EnumNumber {
 
 // Deprecated: Use JudgeStatus.Descriptor instead.
 func (JudgeStatus) EnumDescriptor() ([]byte, []int) {
-	return file_proto_judge_proto_rawDescGZIP(), []int{1}
+	return file_judge_proto_rawDescGZIP(), []int{1}
 }
 
-// 任务请求 (对应 Redis 里的 queue:pending 内容)
+// 任务请求（由 Worker Stream 消费链路反序列化）
 type TaskRequest struct {
 	state protoimpl.MessageState `protogen:"open.v1"`
-	JobId string                 `protobuf:"bytes,1,opt,name=job_id,json=jobId,proto3" json:"job_id,omitempty"`
+	// 必须匹配 ^[A-Za-z0-9_-]{1,64}$，禁止路径分隔符。
+	JobId string `protobuf:"bytes,1,opt,name=job_id,json=jobId,proto3" json:"job_id,omitempty"`
 	// 【关键修改】这里改成 bytes！
 	// 避免 string 导致的自动转义 (backslash bug)
 	// 接收端收到后：std::string code(req.code().begin(), req.code().end());
@@ -161,7 +162,7 @@ type TaskRequest struct {
 
 func (x *TaskRequest) Reset() {
 	*x = TaskRequest{}
-	mi := &file_proto_judge_proto_msgTypes[0]
+	mi := &file_judge_proto_msgTypes[0]
 	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
 	ms.StoreMessageInfo(mi)
 }
@@ -173,7 +174,7 @@ func (x *TaskRequest) String() string {
 func (*TaskRequest) ProtoMessage() {}
 
 func (x *TaskRequest) ProtoReflect() protoreflect.Message {
-	mi := &file_proto_judge_proto_msgTypes[0]
+	mi := &file_judge_proto_msgTypes[0]
 	if x != nil {
 		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
 		if ms.LoadMessageInfo() == nil {
@@ -186,7 +187,7 @@ func (x *TaskRequest) ProtoReflect() protoreflect.Message {
 
 // Deprecated: Use TaskRequest.ProtoReflect.Descriptor instead.
 func (*TaskRequest) Descriptor() ([]byte, []int) {
-	return file_proto_judge_proto_rawDescGZIP(), []int{0}
+	return file_judge_proto_rawDescGZIP(), []int{0}
 }
 
 func (x *TaskRequest) GetJobId() string {
@@ -252,99 +253,6 @@ func (x *TaskRequest) GetTraceId() string {
 	return ""
 }
 
-// 判题结果 (Worker 汇报给 API 的)
-type TaskResult struct {
-	state         protoimpl.MessageState `protogen:"open.v1"`
-	JobId         string                 `protobuf:"bytes,1,opt,name=job_id,json=jobId,proto3" json:"job_id,omitempty"`
-	Status        JudgeStatus            `protobuf:"varint,2,opt,name=status,proto3,enum=deep_oj.JudgeStatus" json:"status,omitempty"`
-	ErrorMessage  string                 `protobuf:"bytes,3,opt,name=error_message,json=errorMessage,proto3" json:"error_message,omitempty"` // CE 的报错信息
-	TimeUsed      int32                  `protobuf:"varint,4,opt,name=time_used,json=timeUsed,proto3" json:"time_used,omitempty"`            // ms
-	MemoryUsed    int64                  `protobuf:"varint,5,opt,name=memory_used,json=memoryUsed,proto3" json:"memory_used,omitempty"`      // KB
-	TraceId       string                 `protobuf:"bytes,6,opt,name=trace_id,json=traceId,proto3" json:"trace_id,omitempty"`                // [New] Return for logging
-	Language      Language               `protobuf:"varint,7,opt,name=language,proto3,enum=deep_oj.Language" json:"language,omitempty"`      // [New] For metrics
-	unknownFields protoimpl.UnknownFields
-	sizeCache     protoimpl.SizeCache
-}
-
-func (x *TaskResult) Reset() {
-	*x = TaskResult{}
-	mi := &file_proto_judge_proto_msgTypes[1]
-	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
-	ms.StoreMessageInfo(mi)
-}
-
-func (x *TaskResult) String() string {
-	return protoimpl.X.MessageStringOf(x)
-}
-
-func (*TaskResult) ProtoMessage() {}
-
-func (x *TaskResult) ProtoReflect() protoreflect.Message {
-	mi := &file_proto_judge_proto_msgTypes[1]
-	if x != nil {
-		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
-		if ms.LoadMessageInfo() == nil {
-			ms.StoreMessageInfo(mi)
-		}
-		return ms
-	}
-	return mi.MessageOf(x)
-}
-
-// Deprecated: Use TaskResult.ProtoReflect.Descriptor instead.
-func (*TaskResult) Descriptor() ([]byte, []int) {
-	return file_proto_judge_proto_rawDescGZIP(), []int{1}
-}
-
-func (x *TaskResult) GetJobId() string {
-	if x != nil {
-		return x.JobId
-	}
-	return ""
-}
-
-func (x *TaskResult) GetStatus() JudgeStatus {
-	if x != nil {
-		return x.Status
-	}
-	return JudgeStatus_STATUS_UNSPECIFIED
-}
-
-func (x *TaskResult) GetErrorMessage() string {
-	if x != nil {
-		return x.ErrorMessage
-	}
-	return ""
-}
-
-func (x *TaskResult) GetTimeUsed() int32 {
-	if x != nil {
-		return x.TimeUsed
-	}
-	return 0
-}
-
-func (x *TaskResult) GetMemoryUsed() int64 {
-	if x != nil {
-		return x.MemoryUsed
-	}
-	return 0
-}
-
-func (x *TaskResult) GetTraceId() string {
-	if x != nil {
-		return x.TraceId
-	}
-	return ""
-}
-
-func (x *TaskResult) GetLanguage() Language {
-	if x != nil {
-		return x.Language
-	}
-	return Language_LANGUAGE_UNSPECIFIED
-}
-
 // 空响应 (占位符)
 type TaskResponse struct {
 	state         protoimpl.MessageState `protogen:"open.v1"`
@@ -355,7 +263,7 @@ type TaskResponse struct {
 
 func (x *TaskResponse) Reset() {
 	*x = TaskResponse{}
-	mi := &file_proto_judge_proto_msgTypes[2]
+	mi := &file_judge_proto_msgTypes[1]
 	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
 	ms.StoreMessageInfo(mi)
 }
@@ -367,7 +275,7 @@ func (x *TaskResponse) String() string {
 func (*TaskResponse) ProtoMessage() {}
 
 func (x *TaskResponse) ProtoReflect() protoreflect.Message {
-	mi := &file_proto_judge_proto_msgTypes[2]
+	mi := &file_judge_proto_msgTypes[1]
 	if x != nil {
 		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
 		if ms.LoadMessageInfo() == nil {
@@ -380,7 +288,7 @@ func (x *TaskResponse) ProtoReflect() protoreflect.Message {
 
 // Deprecated: Use TaskResponse.ProtoReflect.Descriptor instead.
 func (*TaskResponse) Descriptor() ([]byte, []int) {
-	return file_proto_judge_proto_rawDescGZIP(), []int{2}
+	return file_judge_proto_rawDescGZIP(), []int{1}
 }
 
 func (x *TaskResponse) GetMessage() string {
@@ -390,47 +298,11 @@ func (x *TaskResponse) GetMessage() string {
 	return ""
 }
 
-type Ack struct {
-	state         protoimpl.MessageState `protogen:"open.v1"`
-	unknownFields protoimpl.UnknownFields
-	sizeCache     protoimpl.SizeCache
-}
+var File_judge_proto protoreflect.FileDescriptor
 
-func (x *Ack) Reset() {
-	*x = Ack{}
-	mi := &file_proto_judge_proto_msgTypes[3]
-	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
-	ms.StoreMessageInfo(mi)
-}
-
-func (x *Ack) String() string {
-	return protoimpl.X.MessageStringOf(x)
-}
-
-func (*Ack) ProtoMessage() {}
-
-func (x *Ack) ProtoReflect() protoreflect.Message {
-	mi := &file_proto_judge_proto_msgTypes[3]
-	if x != nil {
-		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
-		if ms.LoadMessageInfo() == nil {
-			ms.StoreMessageInfo(mi)
-		}
-		return ms
-	}
-	return mi.MessageOf(x)
-}
-
-// Deprecated: Use Ack.ProtoReflect.Descriptor instead.
-func (*Ack) Descriptor() ([]byte, []int) {
-	return file_proto_judge_proto_rawDescGZIP(), []int{3}
-}
-
-var File_proto_judge_proto protoreflect.FileDescriptor
-
-const file_proto_judge_proto_rawDesc = "" +
+const file_judge_proto_rawDesc = "" +
 	"\n" +
-	"\x11proto/judge.proto\x12\adeep_oj\"\xa1\x02\n" +
+	"\vjudge.proto\x12\adeep_oj\"\xa1\x02\n" +
 	"\vTaskRequest\x12\x15\n" +
 	"\x06job_id\x18\x01 \x01(\tR\x05jobId\x12\x12\n" +
 	"\x04code\x18\x02 \x01(\fR\x04code\x12-\n" +
@@ -443,20 +315,9 @@ const file_proto_judge_proto_rawDesc = "" +
 	"submitTime\x12\x1d\n" +
 	"\n" +
 	"problem_id\x18\b \x01(\rR\tproblemId\x12\x19\n" +
-	"\btrace_id\x18\t \x01(\tR\atraceId\"\xfe\x01\n" +
-	"\n" +
-	"TaskResult\x12\x15\n" +
-	"\x06job_id\x18\x01 \x01(\tR\x05jobId\x12,\n" +
-	"\x06status\x18\x02 \x01(\x0e2\x14.deep_oj.JudgeStatusR\x06status\x12#\n" +
-	"\rerror_message\x18\x03 \x01(\tR\ferrorMessage\x12\x1b\n" +
-	"\ttime_used\x18\x04 \x01(\x05R\btimeUsed\x12\x1f\n" +
-	"\vmemory_used\x18\x05 \x01(\x03R\n" +
-	"memoryUsed\x12\x19\n" +
-	"\btrace_id\x18\x06 \x01(\tR\atraceId\x12-\n" +
-	"\blanguage\x18\a \x01(\x0e2\x11.deep_oj.LanguageR\blanguage\"(\n" +
+	"\btrace_id\x18\t \x01(\tR\atraceId\"(\n" +
 	"\fTaskResponse\x12\x18\n" +
-	"\amessage\x18\x01 \x01(\tR\amessage\"\x05\n" +
-	"\x03Ack*K\n" +
+	"\amessage\x18\x01 \x01(\tR\amessage*K\n" +
 	"\bLanguage\x12\x18\n" +
 	"\x14LANGUAGE_UNSPECIFIED\x10\x00\x12\a\n" +
 	"\x03CPP\x10\x01\x12\b\n" +
@@ -472,69 +333,58 @@ const file_proto_judge_proto_rawDesc = "" +
 	"\x13TIME_LIMIT_EXCEEDED\x10\x04\x12\x19\n" +
 	"\x15MEMORY_LIMIT_EXCEEDED\x10\x05\x12\x11\n" +
 	"\rRUNTIME_ERROR\x10\x06\x12\x10\n" +
-	"\fSYSTEM_ERROR\x10\a2}\n" +
-	"\fJudgeService\x12:\n" +
-	"\vExecuteTask\x12\x14.deep_oj.TaskRequest\x1a\x15.deep_oj.TaskResponse\x121\n" +
-	"\fUpdateStatus\x12\x13.deep_oj.TaskResult\x1a\f.deep_oj.AckB$Z\"github.com/d1guo/deep_oj/pkg/protob\x06proto3"
+	"\fSYSTEM_ERROR\x10\aB$Z\"github.com/d1guo/deep_oj/pkg/protob\x06proto3"
 
 var (
-	file_proto_judge_proto_rawDescOnce sync.Once
-	file_proto_judge_proto_rawDescData []byte
+	file_judge_proto_rawDescOnce sync.Once
+	file_judge_proto_rawDescData []byte
 )
 
-func file_proto_judge_proto_rawDescGZIP() []byte {
-	file_proto_judge_proto_rawDescOnce.Do(func() {
-		file_proto_judge_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_proto_judge_proto_rawDesc), len(file_proto_judge_proto_rawDesc)))
+func file_judge_proto_rawDescGZIP() []byte {
+	file_judge_proto_rawDescOnce.Do(func() {
+		file_judge_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_judge_proto_rawDesc), len(file_judge_proto_rawDesc)))
 	})
-	return file_proto_judge_proto_rawDescData
+	return file_judge_proto_rawDescData
 }
 
-var file_proto_judge_proto_enumTypes = make([]protoimpl.EnumInfo, 2)
-var file_proto_judge_proto_msgTypes = make([]protoimpl.MessageInfo, 4)
-var file_proto_judge_proto_goTypes = []any{
+var file_judge_proto_enumTypes = make([]protoimpl.EnumInfo, 2)
+var file_judge_proto_msgTypes = make([]protoimpl.MessageInfo, 2)
+var file_judge_proto_goTypes = []any{
 	(Language)(0),        // 0: deep_oj.Language
 	(JudgeStatus)(0),     // 1: deep_oj.JudgeStatus
 	(*TaskRequest)(nil),  // 2: deep_oj.TaskRequest
-	(*TaskResult)(nil),   // 3: deep_oj.TaskResult
-	(*TaskResponse)(nil), // 4: deep_oj.TaskResponse
-	(*Ack)(nil),          // 5: deep_oj.Ack
+	(*TaskResponse)(nil), // 3: deep_oj.TaskResponse
 }
-var file_proto_judge_proto_depIdxs = []int32{
+var file_judge_proto_depIdxs = []int32{
 	0, // 0: deep_oj.TaskRequest.language:type_name -> deep_oj.Language
-	1, // 1: deep_oj.TaskResult.status:type_name -> deep_oj.JudgeStatus
-	0, // 2: deep_oj.TaskResult.language:type_name -> deep_oj.Language
-	2, // 3: deep_oj.JudgeService.ExecuteTask:input_type -> deep_oj.TaskRequest
-	3, // 4: deep_oj.JudgeService.UpdateStatus:input_type -> deep_oj.TaskResult
-	4, // 5: deep_oj.JudgeService.ExecuteTask:output_type -> deep_oj.TaskResponse
-	5, // 6: deep_oj.JudgeService.UpdateStatus:output_type -> deep_oj.Ack
-	5, // [5:7] is the sub-list for method output_type
-	3, // [3:5] is the sub-list for method input_type
-	3, // [3:3] is the sub-list for extension type_name
-	3, // [3:3] is the sub-list for extension extendee
-	0, // [0:3] is the sub-list for field type_name
-}
-
-func init() { file_proto_judge_proto_init() }
-func file_proto_judge_proto_init() {
-	if File_proto_judge_proto != nil {
+	1, // [1:1] is the sub-list for method output_type
+	1, // [1:1] is the sub-list for method input_type
+	1, // [1:1] is the sub-list for extension type_name
+	1, // [1:1] is the sub-list for extension extendee
+	0, // [0:1] is the sub-list for field type_name
+}
+
+func init() { file_judge_proto_init() }
+func file_judge_proto_init() {
+	if File_judge_proto != nil {
 		return
 	}
 	type x struct{}
 	out := protoimpl.TypeBuilder{
 		File: protoimpl.DescBuilder{
 			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
-			RawDescriptor: unsafe.Slice(unsafe.StringData(file_proto_judge_proto_rawDesc), len(file_proto_judge_proto_rawDesc)),
+			RawDescriptor: unsafe.Slice(unsafe.StringData(file_judge_proto_rawDesc), len(file_judge_proto_rawDesc)),
 			NumEnums:      2,
-			NumMessages:   4,
+			NumMessages:   2,
 			NumExtensions: 0,
-			NumServices:   1,
+			NumServices:   0,
 		},
-		GoTypes:           file_proto_judge_proto_goTypes,
-		DependencyIndexes: file_proto_judge_proto_depIdxs,
-		EnumInfos:         file_proto_judge_proto_enumTypes,
-		MessageInfos:      file_proto_judge_proto_msgTypes,
+		GoTypes:           file_judge_proto_goTypes,
+		DependencyIndexes: file_judge_proto_depIdxs,
+		EnumInfos:         file_judge_proto_enumTypes,
+		MessageInfos:      file_judge_proto_msgTypes,
 	}.Build()
-	File_proto_judge_proto = out.File
-	file_proto_judge_proto_goTypes = nil
-	file_proto_judge_proto_depIdxs = nil
+	File_judge_proto = out.File
+	file_judge_proto_goTypes = nil
+	file_judge_proto_depIdxs = nil
 }
diff --git a/src/go/pkg/proto/judge_grpc.pb.go b/src/go/pkg/proto/judge_grpc.pb.go
deleted file mode 100644
index ca73706..0000000
--- a/src/go/pkg/proto/judge_grpc.pb.go
+++ /dev/null
@@ -1,175 +0,0 @@
-// Code generated by protoc-gen-go-grpc. DO NOT EDIT.
-// versions:
-// - protoc-gen-go-grpc v1.6.1
-// - protoc             v3.21.12
-// source: proto/judge.proto
-
-package proto
-
-import (
-	context "context"
-	grpc "google.golang.org/grpc"
-	codes "google.golang.org/grpc/codes"
-	status "google.golang.org/grpc/status"
-)
-
-// This is a compile-time assertion to ensure that this generated file
-// is compatible with the grpc package it is being compiled against.
-// Requires gRPC-Go v1.64.0 or later.
-const _ = grpc.SupportPackageIsVersion9
-
-const (
-	JudgeService_ExecuteTask_FullMethodName  = "/deep_oj.JudgeService/ExecuteTask"
-	JudgeService_UpdateStatus_FullMethodName = "/deep_oj.JudgeService/UpdateStatus"
-)
-
-// JudgeServiceClient is the client API for JudgeService service.
-//
-// For semantics around ctx use and closing/ending streaming RPCs, please refer to https://pkg.go.dev/google.golang.org/grpc/?tab=doc#ClientConn.NewStream.
-type JudgeServiceClient interface {
-	// -----------------------------------------------------------
-	// 动作 1: Scheduler -> Worker
-	// 调度器把任务派发给 Worker。Worker 收到后只回个 "收到"，然后异步去跑。
-	// -----------------------------------------------------------
-	ExecuteTask(ctx context.Context, in *TaskRequest, opts ...grpc.CallOption) (*TaskResponse, error)
-	// -----------------------------------------------------------
-	// 动作 2: Worker -> API
-	// Worker 跑完后，主动调用 API 的这个接口汇报结果。
-	// -----------------------------------------------------------
-	UpdateStatus(ctx context.Context, in *TaskResult, opts ...grpc.CallOption) (*Ack, error)
-}
-
-type judgeServiceClient struct {
-	cc grpc.ClientConnInterface
-}
-
-func NewJudgeServiceClient(cc grpc.ClientConnInterface) JudgeServiceClient {
-	return &judgeServiceClient{cc}
-}
-
-func (c *judgeServiceClient) ExecuteTask(ctx context.Context, in *TaskRequest, opts ...grpc.CallOption) (*TaskResponse, error) {
-	cOpts := append([]grpc.CallOption{grpc.StaticMethod()}, opts...)
-	out := new(TaskResponse)
-	err := c.cc.Invoke(ctx, JudgeService_ExecuteTask_FullMethodName, in, out, cOpts...)
-	if err != nil {
-		return nil, err
-	}
-	return out, nil
-}
-
-func (c *judgeServiceClient) UpdateStatus(ctx context.Context, in *TaskResult, opts ...grpc.CallOption) (*Ack, error) {
-	cOpts := append([]grpc.CallOption{grpc.StaticMethod()}, opts...)
-	out := new(Ack)
-	err := c.cc.Invoke(ctx, JudgeService_UpdateStatus_FullMethodName, in, out, cOpts...)
-	if err != nil {
-		return nil, err
-	}
-	return out, nil
-}
-
-// JudgeServiceServer is the server API for JudgeService service.
-// All implementations must embed UnimplementedJudgeServiceServer
-// for forward compatibility.
-type JudgeServiceServer interface {
-	// -----------------------------------------------------------
-	// 动作 1: Scheduler -> Worker
-	// 调度器把任务派发给 Worker。Worker 收到后只回个 "收到"，然后异步去跑。
-	// -----------------------------------------------------------
-	ExecuteTask(context.Context, *TaskRequest) (*TaskResponse, error)
-	// -----------------------------------------------------------
-	// 动作 2: Worker -> API
-	// Worker 跑完后，主动调用 API 的这个接口汇报结果。
-	// -----------------------------------------------------------
-	UpdateStatus(context.Context, *TaskResult) (*Ack, error)
-	mustEmbedUnimplementedJudgeServiceServer()
-}
-
-// UnimplementedJudgeServiceServer must be embedded to have
-// forward compatible implementations.
-//
-// NOTE: this should be embedded by value instead of pointer to avoid a nil
-// pointer dereference when methods are called.
-type UnimplementedJudgeServiceServer struct{}
-
-func (UnimplementedJudgeServiceServer) ExecuteTask(context.Context, *TaskRequest) (*TaskResponse, error) {
-	return nil, status.Error(codes.Unimplemented, "method ExecuteTask not implemented")
-}
-func (UnimplementedJudgeServiceServer) UpdateStatus(context.Context, *TaskResult) (*Ack, error) {
-	return nil, status.Error(codes.Unimplemented, "method UpdateStatus not implemented")
-}
-func (UnimplementedJudgeServiceServer) mustEmbedUnimplementedJudgeServiceServer() {}
-func (UnimplementedJudgeServiceServer) testEmbeddedByValue()                      {}
-
-// UnsafeJudgeServiceServer may be embedded to opt out of forward compatibility for this service.
-// Use of this interface is not recommended, as added methods to JudgeServiceServer will
-// result in compilation errors.
-type UnsafeJudgeServiceServer interface {
-	mustEmbedUnimplementedJudgeServiceServer()
-}
-
-func RegisterJudgeServiceServer(s grpc.ServiceRegistrar, srv JudgeServiceServer) {
-	// If the following call panics, it indicates UnimplementedJudgeServiceServer was
-	// embedded by pointer and is nil.  This will cause panics if an
-	// unimplemented method is ever invoked, so we test this at initialization
-	// time to prevent it from happening at runtime later due to I/O.
-	if t, ok := srv.(interface{ testEmbeddedByValue() }); ok {
-		t.testEmbeddedByValue()
-	}
-	s.RegisterService(&JudgeService_ServiceDesc, srv)
-}
-
-func _JudgeService_ExecuteTask_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
-	in := new(TaskRequest)
-	if err := dec(in); err != nil {
-		return nil, err
-	}
-	if interceptor == nil {
-		return srv.(JudgeServiceServer).ExecuteTask(ctx, in)
-	}
-	info := &grpc.UnaryServerInfo{
-		Server:     srv,
-		FullMethod: JudgeService_ExecuteTask_FullMethodName,
-	}
-	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
-		return srv.(JudgeServiceServer).ExecuteTask(ctx, req.(*TaskRequest))
-	}
-	return interceptor(ctx, in, info, handler)
-}
-
-func _JudgeService_UpdateStatus_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
-	in := new(TaskResult)
-	if err := dec(in); err != nil {
-		return nil, err
-	}
-	if interceptor == nil {
-		return srv.(JudgeServiceServer).UpdateStatus(ctx, in)
-	}
-	info := &grpc.UnaryServerInfo{
-		Server:     srv,
-		FullMethod: JudgeService_UpdateStatus_FullMethodName,
-	}
-	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
-		return srv.(JudgeServiceServer).UpdateStatus(ctx, req.(*TaskResult))
-	}
-	return interceptor(ctx, in, info, handler)
-}
-
-// JudgeService_ServiceDesc is the grpc.ServiceDesc for JudgeService service.
-// It's only intended for direct use with grpc.RegisterService,
-// and not to be introspected or modified (even as a copy)
-var JudgeService_ServiceDesc = grpc.ServiceDesc{
-	ServiceName: "deep_oj.JudgeService",
-	HandlerType: (*JudgeServiceServer)(nil),
-	Methods: []grpc.MethodDesc{
-		{
-			MethodName: "ExecuteTask",
-			Handler:    _JudgeService_ExecuteTask_Handler,
-		},
-		{
-			MethodName: "UpdateStatus",
-			Handler:    _JudgeService_UpdateStatus_Handler,
-		},
-	},
-	Streams:  []grpc.StreamDesc{},
-	Metadata: "proto/judge.proto",
-}
diff --git a/scripts/verify_b2_no_legacy_dataplane.sh b/scripts/verify_b2_no_legacy_dataplane.sh
new file mode 100755
index 0000000..4187885
--- /dev/null
+++ b/scripts/verify_b2_no_legacy_dataplane.sh
@@ -0,0 +1,170 @@
+#!/usr/bin/env bash
+set -euo pipefail
+
+ROOT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
+cd "$ROOT_DIR"
+
+KEEP_TMP="${KEEP_TMP:-0}"
+TMP_DIR="$(mktemp -d)"
+
+cleanup() {
+  if [[ "$KEEP_TMP" == "1" ]]; then
+    echo "KEEP_TMP=1, 保留临时目录: $TMP_DIR" >&2
+    return
+  fi
+  rm -rf "$TMP_DIR"
+}
+trap cleanup EXIT
+
+compose_service_exists() {
+  local service="$1"
+  local svc
+  while IFS= read -r svc; do
+    [[ "$svc" == "$service" ]] && return 0
+  done < <(docker compose config --services 2>/dev/null || true)
+  return 1
+}
+
+emit_diagnostics() {
+  echo >&2
+  echo "========== 诊断信息 ==========" >&2
+  echo "[docker compose ps]" >&2
+  docker compose ps >&2 || true
+
+  for service in api worker scheduler; do
+    if compose_service_exists "$service"; then
+      echo "[docker compose logs --tail=200 $service]" >&2
+      docker compose logs --tail=200 "$service" >&2 || true
+    fi
+  done
+  echo "================================" >&2
+}
+
+fatal() {
+  local msg="$1"
+  echo "ERROR: $msg" >&2
+  emit_diagnostics
+  exit 1
+}
+
+require_cmd() {
+  local cmd="$1"
+  if ! command -v "$cmd" >/dev/null 2>&1; then
+    fatal "缺少命令: $cmd"
+  fi
+}
+
+assert_no_legacy_scheduler_files() {
+  local removed_files=(
+    "src/go/internal/scheduler/dispatch.go"
+    "src/go/internal/scheduler/ack_listener.go"
+    "src/go/internal/scheduler/retry.go"
+    "src/go/internal/scheduler/slow_path.go"
+    "src/go/internal/scheduler/watchdog.go"
+    "src/go/pkg/proto/judge_grpc.pb.go"
+  )
+  local f
+  for f in "${removed_files[@]}"; do
+    if [[ -e "$f" ]]; then
+      fatal "仍存在 legacy 文件: $f"
+    fi
+  done
+}
+
+assert_no_legacy_keywords() {
+  local keywords=(
+    "QueuePending"
+    "QueueProcessing"
+    "TaskProcessingZSet"
+    "BRPopLPush"
+    "BRPOPLPUSH"
+    "SubmitJob"
+    "DispatchTask"
+    "legacy_grpc_push"
+    "dispatch_enabled"
+    "queue:pending"
+    "queue:processing"
+    "stream:results"
+    "results-group"
+    "WORKER_AUTH_TOKEN"
+    "ALLOW_INSECURE_WORKER_GRPC"
+    "50052"
+  )
+
+  local kw
+  for kw in "${keywords[@]}"; do
+    if rg -n --fixed-strings \
+      --glob '!scripts/verify_b2_no_legacy_dataplane.sh' \
+      --glob '!*.diff' \
+      -- "$kw" . >/dev/null; then
+      echo "命中 legacy 关键字: $kw" >&2
+      rg -n --fixed-strings \
+        --glob '!scripts/verify_b2_no_legacy_dataplane.sh' \
+        --glob '!*.diff' \
+        -- "$kw" . >&2 || true
+      fatal "存在 legacy 数据面关键字残留"
+    fi
+  done
+}
+
+run_script_step() {
+  local label="$1"
+  local script="$2"
+  echo "$label"
+  if [[ ! -f "$script" ]]; then
+    fatal "脚本不存在: $script"
+  fi
+  if ! bash "$script"; then
+    fatal "脚本执行失败: $script"
+  fi
+}
+
+run_optional_g1() {
+  if [[ ! -f scripts/verify_g1_kill_all.sh ]]; then
+    echo "[7/8] 跳过 G1：未找到 scripts/verify_g1_kill_all.sh"
+    return 0
+  fi
+
+  if [[ -f scripts/verify_g1_prereq.sh ]]; then
+    echo "[7/8] 检查 G1 运行前置条件"
+    if ! bash scripts/verify_g1_prereq.sh; then
+      echo "[7/8] 跳过 G1：当前环境不满足前置条件" >&2
+      return 0
+    fi
+  else
+    echo "[7/8] 未找到 verify_g1_prereq.sh，按可运行处理并直接执行 G1"
+  fi
+
+  echo "[7/8] 执行 scripts/verify_g1_kill_all.sh"
+  if ! bash scripts/verify_g1_kill_all.sh; then
+    fatal "脚本执行失败: scripts/verify_g1_kill_all.sh"
+  fi
+}
+
+require_cmd docker
+require_cmd bash
+require_cmd rg
+
+export JWT_SECRET="${JWT_SECRET:-dev_jwt_secret_change_me}"
+export ADMIN_USERS="${ADMIN_USERS:-admin}"
+export REDIS_PASSWORD="${REDIS_PASSWORD:-deepoj_redis_change_me}"
+export POSTGRES_PASSWORD="${POSTGRES_PASSWORD:-deepoj_pg_password_change_me}"
+export MINIO_ROOT_USER="${MINIO_ROOT_USER:-deepoj_minio_user}"
+export MINIO_ROOT_PASSWORD="${MINIO_ROOT_PASSWORD:-deepoj_minio_password_change_me}"
+
+echo "[1/8] docker compose up -d --build"
+if ! docker compose up -d --build; then
+  fatal "docker compose up 失败"
+fi
+
+echo "[2/8] 断言无 legacy 数据面残留"
+assert_no_legacy_scheduler_files
+assert_no_legacy_keywords
+
+run_script_step "[3/8] 执行 scripts/verify_mvp2_e2e.sh" "scripts/verify_mvp2_e2e.sh"
+run_script_step "[4/8] 执行 scripts/verify_mvp3_crash_recover.sh" "scripts/verify_mvp3_crash_recover.sh"
+run_script_step "[5/8] 执行 scripts/verify_mvp4_observability.sh" "scripts/verify_mvp4_observability.sh"
+run_script_step "[6/8] 执行 scripts/verify_ci.sh" "scripts/verify_ci.sh"
+run_optional_g1
+
+echo "[8/8] 无 legacy 数据面验证通过"
