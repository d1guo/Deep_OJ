diff --git a/FAILURE_MODES.md b/FAILURE_MODES.md
index 7e98bf5..f9090b7 100644
--- a/FAILURE_MODES.md
+++ b/FAILURE_MODES.md
@@ -14,7 +14,7 @@
 - 触发点：`src/go/cmd/scheduler/main.go` `selectWorker` 失败
 - 现象：任务被 `RequeueTask(queue:processing -> queue:pending)` 循环重试
 - 定位：日志关键字 `No workers available`
-- 恢复：恢复 worker/etcd 注册，观察 pending 回落
+- 恢复：恢复 worker 可达（`WORKER_ADDR/WORKER_ADDRS`），观察 pending 回落
 
 ## 3. Worker 执行完成但结果未入 Stream
 
@@ -43,4 +43,3 @@
 - 现象：当前 SQL 条件 `state != 'done'` 下被忽略
 - 定位：日志关键字 `Duplicate result ignored (already done)`
 - 恢复：无需人工处理；后续需升级到 attempt_id fencing（C5）
-
diff --git a/RUNBOOK.md b/RUNBOOK.md
index 3ab4ac8..dac3378 100644
--- a/RUNBOOK.md
+++ b/RUNBOOK.md
@@ -24,9 +24,17 @@ python3 tests/integration/test_e2e.py
 
 预期：
 
-- `docker compose ps` 中 `api/scheduler/worker/redis/postgres/minio/etcd` 为 `Up`
+- `docker compose ps` 中 `api/scheduler/worker/redis/postgres/minio` 为 `Up`
 - `test_e2e.py` 输出 `Submitted, Job ID:`，并在轮询后出现 `Result: Accepted`（或可预期判题状态）
 
+### 0.1 无 etcd 启动验证（B1）
+
+系统已移除 etcd 运行时依赖。请使用以下脚本执行完整验收：
+
+```bash
+bash scripts/verify_b1_no_etcd.sh
+```
+
 ## 1. 环境与依赖
 
 最低要求（生产建议高于此基线）：
@@ -46,7 +54,6 @@ python3 tests/integration/test_e2e.py
 - Redis（兼容 compose 中 `redis:alpine`）
 - PostgreSQL 15（compose 使用 `postgres:15-alpine`）
 - MinIO（项目使用对象存储下载测试数据；非可选于完整判题链路）
-- Etcd 3.5（Worker 服务发现）
 
 ### 1.1 本地运行 judge 并查看 JSON 输出
 
@@ -187,7 +194,7 @@ done
 | Redis 不可达，API 报 `QUEUE_ERROR` | `REDIS_PASSWORD` 不一致、Redis 容器未启动、网络不可达 | `docker compose ps redis`; `docker exec -it oj-redis redis-cli -a "$REDIS_PASSWORD" PING`; `docker compose logs --tail=200 api` | 统一 `REDIS_PASSWORD`，重启 `redis/api/scheduler/worker`，必要时 `docker compose up -d` |
 | DB 连接池耗尽，接口变慢/5xx | `PG_MAX_CONNS` 太小、长事务、慢 SQL | `docker compose logs --tail=300 api | rg -i "database|timeout|too many"`; `docker exec -it oj-postgres psql -U deep_oj -d deep_oj -c "select * from pg_stat_activity;"` | 增大 `PG_MAX_CONNS`，排查慢查询，缩短事务并重启 API |
 | 队列堆积（`queue:pending` 持续增长） | Worker 不可用、Scheduler 调度失败、下游执行慢 | `docker exec -it oj-redis redis-cli -a "$REDIS_PASSWORD" LLEN queue:pending`; `docker compose logs --tail=200 scheduler worker` | 先恢复 worker 可用性，再根据 CPU/内存扩容 worker 或降低提交速率 |
-| Worker 不消费任务 | Etcd 注册失败、gRPC 不可达、worker 启动失败 | `docker compose logs --tail=200 worker`; `docker compose logs --tail=200 scheduler | rg -i "No workers|dispatch"` | 修复 `ETCD_ENDPOINTS/WORKER_ADDR`，确认 `oj-etcd` 可达，重启 worker/scheduler |
+| Worker 不消费任务 | gRPC 不可达、worker 启动失败、`WORKER_ADDR` 配置错误 | `docker compose logs --tail=200 worker`; `docker compose logs --tail=200 scheduler | rg -i "No workers|dispatch"` | 修复 `WORKER_ADDR/WORKER_ADDRS`，确认 worker 可达，重启 worker/scheduler |
 | Judge 出现超时/TLE 异常增多 | 用户代码死循环、时间限制过低、机器负载过高 | `docker compose logs --tail=300 worker | rg -i "Time Limit|timed out|command timed out"` | 调整题目时限、检查资源压力、必要时横向扩容 worker |
 | Judge 出现 OOM/MLE 异常增多 | 代码内存占用过大、内存限制过低、容器内存紧张 | `docker compose logs --tail=300 worker | rg -i "Memory Limit|OOM|killed"` | 调整内存上限，检查宿主机内存，限制异常任务并重试 |
 | 出现僵尸进程或残留判题进程 | kill/cleanup 未完整执行、worker 异常中断 | `docker exec -it oj-worker ps -eo pid,ppid,stat,cmd | rg "judge_engine| Z "` | 重启 worker 清理现场；后续按任务 G1/G2 加强进程组 kill 与幂等清理 |
diff --git a/config.yaml b/config.yaml
index 10b7deb..90353ff 100644
--- a/config.yaml
+++ b/config.yaml
@@ -96,8 +96,6 @@ api:
     job_payload_ttl_sec: 86400
 
 scheduler:
-  etcd_endpoints:
-    - "localhost:2379"
   redis_url: "localhost:6379"
   database_url: "postgres://deep_oj:change_me@localhost:5432/deep_oj?sslmode=disable"
   worker_capacity: 4
@@ -108,7 +106,6 @@ scheduler:
   dispatch_enabled: false       # legacy_grpc_push requires true
   metrics_port: 9091
   metrics_poll_ms: 1000
-  etcd_dial_timeout_ms: 5000
   queue:
     brpop_timeout_sec: 5
     no_worker_sleep_ms: 1000
@@ -145,10 +142,6 @@ worker:
   id: ""
   addr: ""
   port: 50051
-  etcd_endpoints:
-    - "localhost:2379"
-  etcd_dial_timeout_ms: 5000
-  etcd_lease_ttl_sec: 10
   redis_url: "localhost:6379"
   database_url: "postgres://deep_oj:change_me@localhost:5432/deep_oj?sslmode=disable"
   stream:
diff --git a/docker-compose.yml b/docker-compose.yml
index 87ef737..ea1e314 100644
--- a/docker-compose.yml
+++ b/docker-compose.yml
@@ -37,7 +37,6 @@ services:
       - MINIO_ACCESS_KEY=${MINIO_ROOT_USER:-deepoj_minio_user}
       - MINIO_SECRET_KEY=${MINIO_ROOT_PASSWORD:-deepoj_minio_password_change_me}
       - MINIO_BUCKET=deep-oj-problems
-      - ETCD_ENDPOINTS=oj-etcd:2379
       - WORKER_ADDR=oj-worker:50051
       - JOB_STREAM_KEY=deepoj:jobs
       - JOB_STREAM_GROUP=deepoj:workers
@@ -70,7 +69,6 @@ services:
       - REDIS_URL=oj-redis:6379
       - REDIS_PASSWORD=${REDIS_PASSWORD:-deepoj_redis_change_me}
       - WORKER_ADDR=oj-worker:50051
-      - ETCD_ENDPOINTS=oj-etcd:2379 # Need Etcd service
       - PGPASSWORD=${POSTGRES_PASSWORD:-deepoj_pg_password_change_me}
       - DATABASE_URL=postgres://deep_oj:${POSTGRES_PASSWORD:-deepoj_pg_password_change_me}@oj-postgres:5432/deep_oj?sslmode=disable
       - WORKER_AUTH_TOKEN=${WORKER_AUTH_TOKEN:-deepoj_worker_token_change_me}
@@ -103,6 +101,8 @@ services:
       - MINIO_ACCESS_KEY=${MINIO_ROOT_USER:-deepoj_minio_user}
       - MINIO_SECRET_KEY=${MINIO_ROOT_PASSWORD:-deepoj_minio_password_change_me}
       - MINIO_BUCKET=deep-oj-problems
+      - JWT_SECRET=${JWT_SECRET:-dev_jwt_secret_change_me}
+      - ADMIN_USERS=${ADMIN_USERS:-admin}
       - METRICS_TOKEN=${METRICS_TOKEN:-}
     networks:
       - deep-oj-net
@@ -143,16 +143,6 @@ services:
     networks:
       - deep-oj-net
 
-  # 7. Etcd (Service Discovery)
-  etcd:
-    image: registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.0-0
-    container_name: oj-etcd
-    command: /usr/local/bin/etcd --listen-client-urls http://0.0.0.0:2379 --advertise-client-urls http://oj-etcd:2379
-    expose:
-      - "2379"
-    networks:
-      - deep-oj-net
-
 networks:
   deep-oj-net:
     driver: bridge
diff --git a/scripts/check_deps.go b/scripts/check_deps.go
index d29f56a..afbb953 100644
--- a/scripts/check_deps.go
+++ b/scripts/check_deps.go
@@ -10,7 +10,6 @@ import (
 func main() {
 	services := map[string]string{
 		"Redis":    "127.0.0.1:6379",
-		"Etcd":     "127.0.0.1:2379",
 		"Postgres": "127.0.0.1:5432",
 	}
 
diff --git a/scripts/start_docker.sh b/scripts/start_docker.sh
index c8db489..7524b25 100755
--- a/scripts/start_docker.sh
+++ b/scripts/start_docker.sh
@@ -19,16 +19,7 @@ if [ ! "$(docker ps -q -f name=oj-minio)" ]; then
         minio/minio:RELEASE.2024-01-18T22-51-28Z server /data --console-address ":9001"
 fi
 
-# 2. Etcd
-echo "Starting Etcd..."
-docker rm -f oj-etcd || true
-docker run -d --name oj-etcd \
-    --network deep-oj-net \
-    -p 2381:2379 \
-    registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.0-0 \
-    /usr/local/bin/etcd --listen-client-urls http://0.0.0.0:2379 --advertise-client-urls http://oj-etcd:2379
-
-# 3. Redis
+# 2. Redis
 echo "Starting Redis..."
 docker rm -f oj-redis || true
 docker run -d --name oj-redis \
@@ -36,7 +27,7 @@ docker run -d --name oj-redis \
     -p 6380:6379 \
     redis:alpine
 
-# 4. Postgres
+# 3. Postgres
 echo "Starting Postgres..."
 docker rm -f oj-postgres || true
 docker run -d --name oj-postgres \
@@ -52,7 +43,7 @@ docker run -d --name oj-postgres \
 echo "Waiting for Postgres..."
 sleep 5
 
-# 5. Worker
+# 4. Worker
 echo "Starting Worker..."
 docker rm -f oj-worker || true
 docker run -d --name oj-worker \
@@ -66,13 +57,12 @@ docker run -d --name oj-worker \
     -e MINIO_ACCESS_KEY=minioadmin \
     -e MINIO_SECRET_KEY=minioadmin \
     -e MINIO_BUCKET=deep-oj-problems \
-    -e ETCD_ENDPOINTS=oj-etcd:2379 \
     -e WORKER_ADDR=oj-worker:50051 \
     -e JUDGER_BIN=/app/judge_engine \
     -e WORKSPACE=/data/workspace \
     deep-oj:v3 /app/oj_worker
 
-# 6. Scheduler
+# 5. Scheduler
 echo "Starting Scheduler..."
 docker rm -f oj-scheduler || true
 docker run -d --name oj-scheduler \
@@ -80,12 +70,11 @@ docker run -d --name oj-scheduler \
     -p 50052:50052 \
     -e REDIS_URL=oj-redis:6379 \
     -e WORKER_ADDR=oj-worker:50051 \
-    -e ETCD_ENDPOINTS=oj-etcd:2379 \
     -e PGPASSWORD=secret \
     -e DATABASE_URL=postgres://deep_oj:secret@oj-postgres:5432/deep_oj?sslmode=disable \
     deep-oj:v3 /app/oj_scheduler
 
-# 7. API
+# 6. API
 echo "Starting API..."
 docker rm -f oj-api || true
 docker run -d --name oj-api \
diff --git a/scripts/start_integration.sh b/scripts/start_integration.sh
index def4dd3..d939881 100755
--- a/scripts/start_integration.sh
+++ b/scripts/start_integration.sh
@@ -8,7 +8,6 @@ NC='\033[0m'
 
 cleanup() {
     echo -e "\n${RED}停止服务中...${NC}"
-    pkill -f "etcd"
     pkill -f "bin/api"
     pkill -f "bin/scheduler"
     echo "qsrlys67" | sudo -S pkill -f "oj_worker"
@@ -18,7 +17,7 @@ trap cleanup EXIT
 # 1. 检查依赖
 echo -e "${GREEN}检查依赖项...${NC}"
 MISSING=0
-for cmd in etcd psql python3 go; do
+for cmd in psql python3 go; do
     if ! command -v $cmd &> /dev/null; then
         echo -e "${RED}缺失依赖: $cmd${NC}"
         MISSING=1
@@ -33,12 +32,6 @@ fi
 # 2. 启动基础设施
 echo -e "${GREEN}启动基础设施...${NC}"
 
-# 启动 Etcd
-if ! pgrep etcd > /dev/null; then
-    nohup etcd > /dev/null 2>&1 &
-    sleep 2
-fi
-
 # 检查 Redis
 if ! redis-cli ping > /dev/null 2>&1; then
     echo -e "${RED}Redis 未运行! 请启动 Redis.${NC}"
@@ -121,8 +114,7 @@ cd ..
 
 # 启动 C++ Worker (需要 root 权限配置 Cgroup)
 # 使用 sudo -b 后台运行，并且把日志重定向
-# 必须设置 ETCD_ENDPOINTS 才能进行服务注册
-echo "qsrlys67" | sudo -S -E -b ETCD_ENDPOINTS=localhost:2379 WORKER_ADDR=localhost:50051 ./build/oj_worker > worker.log 2>&1
+echo "qsrlys67" | sudo -S -E -b WORKER_ADDR=localhost:50051 ./build/oj_worker > worker.log 2>&1
 echo "Worker 已启动 (sudo)"
 
 # 等待服务就绪
diff --git a/scripts/verify_d3_backpressure.sh b/scripts/verify_d3_backpressure.sh
index ff12d4b..837e762 100755
--- a/scripts/verify_d3_backpressure.sh
+++ b/scripts/verify_d3_backpressure.sh
@@ -111,7 +111,7 @@ emit_diagnostics() {
 
 cleanup_conflicting_containers() {
   local name
-  for name in oj-api oj-worker oj-scheduler oj-redis oj-postgres oj-minio oj-etcd; do
+  for name in oj-api oj-worker oj-scheduler oj-redis oj-postgres oj-minio; do
     if docker ps -a --format '{{.Names}}' | grep -qx "$name"; then
       docker rm -f "$name" >/dev/null 2>&1 || true
     fi
@@ -242,9 +242,9 @@ ensure_stream_group() {
 
 export JWT_SECRET ADMIN_USERS REDIS_PASSWORD STREAM_KEY STREAM_GROUP
 
-echo "[1/8] start core services (api/scheduler/redis/postgres/minio/etcd)"
+echo "[1/8] start core services (api/scheduler/redis/postgres/minio)"
 cleanup_conflicting_containers
-if ! docker compose up -d --build api scheduler redis postgres minio etcd; then
+if ! docker compose up -d --build api scheduler redis postgres minio; then
   fatal "docker compose up failed"
 fi
 
diff --git a/scripts/verify_mvp3_crash_recover.sh b/scripts/verify_mvp3_crash_recover.sh
index c4a68b3..a4ba7a4 100755
--- a/scripts/verify_mvp3_crash_recover.sh
+++ b/scripts/verify_mvp3_crash_recover.sh
@@ -34,6 +34,13 @@ READINESS_HEALTH_STATUS="<none>"
 READINESS_HEALTH_BODY_FILE=""
 READINESS_LAST_SOURCE="<none>"
 HAS_RG=0
+COMPOSE_FILES=(-f docker-compose.yml)
+if [[ -f docker-compose.verify-g1.yml ]]; then
+  COMPOSE_FILES+=(-f docker-compose.verify-g1.yml)
+fi
+if [[ -f docker-compose.override.yml ]]; then
+  COMPOSE_FILES+=(-f docker-compose.override.yml)
+fi
 
 cleanup() {
   if [[ "$KEEP_TMP" == "1" ]]; then
@@ -81,12 +88,16 @@ rg_or_grep() {
   fi
 }
 
+compose_cmd() {
+  docker compose "${COMPOSE_FILES[@]}" "$@"
+}
+
 compose_service_exists() {
   local service="$1"
   local svc
   while IFS= read -r svc; do
     [[ "$svc" == "$service" ]] && return 0
-  done < <(docker compose config --services 2>/dev/null || true)
+  done < <(compose_cmd config --services 2>/dev/null || true)
   return 1
 }
 
@@ -95,7 +106,7 @@ service_running() {
   local svc
   while IFS= read -r svc; do
     [[ "$svc" == "$service" ]] && return 0
-  done < <(docker compose ps --status running --services 2>/dev/null || true)
+  done < <(compose_cmd ps --status running --services 2>/dev/null || true)
   return 1
 }
 
@@ -124,7 +135,7 @@ emit_diagnostics() {
   echo
   echo "========== diagnostics ==========" >&2
   echo "[docker compose ps]" >&2
-  docker compose ps >&2 || true
+  compose_cmd ps >&2 || true
   echo "[api readiness snapshot]" >&2
   echo "  source=${READINESS_LAST_SOURCE}" >&2
   echo "  health_status=${READINESS_HEALTH_STATUS}" >&2
@@ -134,14 +145,14 @@ emit_diagnostics() {
   fi
 
   local services
-  services="$(docker compose config --services 2>/dev/null || true)"
+  services="$(compose_cmd config --services 2>/dev/null || true)"
   if [[ -z "$services" ]]; then
     echo "[docker compose config --services] <none>" >&2
   else
     while IFS= read -r service; do
       [[ -z "$service" ]] && continue
       echo "[docker compose logs --tail=200 ${service}]" >&2
-      docker compose logs --tail=200 "$service" >&2 || true
+      compose_cmd logs --tail=200 "$service" >&2 || true
     done <<< "$services"
   fi
   echo "=================================" >&2
@@ -291,7 +302,7 @@ wait_api_ready() {
     fi
 
     if compose_service_exists "api"; then
-      if docker compose logs --tail=200 api 2>&1 | rg_or_grep "API Server starting" >/dev/null 2>&1; then
+      if compose_cmd logs --tail=200 api 2>&1 | rg_or_grep "API Server starting" >/dev/null 2>&1; then
         READINESS_LAST_SOURCE="compose_log"
         return 0
       fi
@@ -388,11 +399,11 @@ fi
 echo "[1/10] start services"
 FAILED_STEP="start services"
 if [[ "$COMPOSE_BUILD" == "1" ]]; then
-  if ! docker compose up -d --build; then
+  if ! compose_cmd up -d --build; then
     fail_verify "$FAILED_STEP" "<none>" "" "docker compose up -d --build failed"
   fi
 else
-  if ! docker compose up -d; then
+  if ! compose_cmd up -d; then
     fail_verify "$FAILED_STEP" "<none>" "" "docker compose up -d failed"
   fi
 fi
@@ -579,7 +590,7 @@ if ! is_int "$XPENDING_BEFORE"; then
 fi
 
 FAILED_STEP="kill worker"
-if ! docker compose kill -s SIGKILL "$WORKER_SERVICE" >/dev/null 2>&1; then
+if ! compose_cmd kill -s SIGKILL "$WORKER_SERVICE" >/dev/null 2>&1; then
   worker_container="$(find_container_by_service "$WORKER_SERVICE")"
   if [[ -z "$worker_container" ]]; then
     fail_verify "$FAILED_STEP" "N/A" "" "failed to find worker container for SIGKILL"
@@ -619,7 +630,7 @@ fi
 
 sleep "$CRASH_WAIT_SEC"
 FAILED_STEP="restart worker"
-if ! docker compose up -d "$WORKER_SERVICE" >/dev/null 2>&1; then
+if ! compose_cmd up -d "$WORKER_SERVICE" >/dev/null 2>&1; then
   fail_verify "$FAILED_STEP" "N/A" "" "docker compose up -d worker failed"
 fi
 if ! wait_service_running "$WORKER_SERVICE" 90; then
@@ -676,7 +687,7 @@ fi
 echo "[8/10] reclaim evidence"
 FAILED_STEP="collect reclaim evidence"
 WORKER_LOGS_SINCE_FILE="$TMP_DIR/worker_logs_since_crash.log"
-docker compose logs --since "$CRASH_TS" --tail=800 "$WORKER_SERVICE" > "$WORKER_LOGS_SINCE_FILE" 2>&1 || true
+compose_cmd logs --since "$CRASH_TS" --tail=800 "$WORKER_SERVICE" > "$WORKER_LOGS_SINCE_FILE" 2>&1 || true
 RECLAIM_MATCHES="$(rg_or_grep "DB reclaim success|reclaim_claimed|XAUTOCLAIM|reclaim" "$WORKER_LOGS_SINCE_FILE" || true)"
 RECLAIM_MATCH_COUNT="$(printf '%s\n' "$RECLAIM_MATCHES" | sed '/^[[:space:]]*$/d' | wc -l | tr -d ' ')"
 
diff --git a/src/go/cmd/scheduler/main.go b/src/go/cmd/scheduler/main.go
index ba66246..1156092 100644
--- a/src/go/cmd/scheduler/main.go
+++ b/src/go/cmd/scheduler/main.go
@@ -1,33 +1,4 @@
-/**
- * @file main.go
- * @brief Go Scheduler 入口
- *
- * 架构定位: 任务调度层
- * 技术选型: Etcd (服务发现) + gRPC (Worker 通信) + Redis (任务队列)
- *
- *
- * 1. Etcd 服务发现 vs 传统配置:
- *    - 传统: 硬编码 Worker 地址，重启才能更新
- *    - Etcd: Worker 动态注册，实时感知变化
- *    - Lease 机制: Worker 定期续约，超时自动注销
- *
- * 2. 负载均衡策略:
- *    - Round-Robin: 简单轮询，适合同构服务
- *    - Weighted: 加权轮询，根据 Worker 能力分配
- *    - Least-Connections: 最少连接优先
- *    - Consistent-Hashing: 一致性哈希，适合缓存场景
- *
- * 3. gRPC 优势:
- *    - HTTP/2: 多路复用，头部压缩
- *    - Protobuf: 紧凑的二进制序列化
- *    - 流式传输: 双向流支持
- *    - 代码生成: 强类型接口
- *
- * 4. 可靠性设计:
- *    - ACK 机制: 任务确认后才从队列移除
- *    - 超时检测: 处理中任务超时后重新入队
- *    - 重试策略: 指数退避 (Exponential Backoff)
- */
+// 调度器入口：不依赖 etcd，使用 WORKER_ADDR/WORKER_ADDRS 进行工作节点发现。
 package main
 
 import (
@@ -38,7 +9,6 @@ import (
 	"os"
 	"os/signal"
 	"strconv"
-	"strings"
 	"sync"
 	"syscall"
 	"time"
@@ -82,7 +52,6 @@ func main() {
 		appconfig.SetEnvIfEmptyInt("PG_MAX_CONN_LIFETIME_MIN", cfg.Postgres.MaxConnLifetimeMin)
 		appconfig.SetEnvIfEmptyInt("PG_MAX_CONN_IDLE_MIN", cfg.Postgres.MaxConnIdleMin)
 
-		appconfig.SetEnvIfEmptySlice("ETCD_ENDPOINTS", cfg.Scheduler.EtcdEndpoints)
 		appconfig.SetEnvIfEmpty("REDIS_URL", cfg.Scheduler.RedisURL)
 		appconfig.SetEnvIfEmpty("DATABASE_URL", cfg.Scheduler.DatabaseURL)
 		appconfig.SetEnvIfEmptyInt("WORKER_CAPACITY", cfg.Scheduler.WorkerCapacity)
@@ -91,7 +60,6 @@ func main() {
 		appconfig.SetEnvIfEmpty("SCHEDULER_ID", cfg.Scheduler.SchedulerID)
 		appconfig.SetEnvIfEmptyInt("SCHEDULER_METRICS_PORT", cfg.Scheduler.MetricsPort)
 		appconfig.SetEnvIfEmptyInt("SCHEDULER_METRICS_POLL_INTERVAL_MS", cfg.Scheduler.MetricsPollMs)
-		appconfig.SetEnvIfEmptyInt("ETCD_DIAL_TIMEOUT_MS", cfg.Scheduler.EtcdDialTimeoutMs)
 		appconfig.SetEnvIfEmptyInt("QUEUE_BRPOP_TIMEOUT_SEC", cfg.Scheduler.Queue.BRPopTimeoutSec)
 		appconfig.SetEnvIfEmptyInt("NO_WORKER_SLEEP_MS", cfg.Scheduler.Queue.NoWorkerSleepMs)
 		appconfig.SetEnvIfEmptyInt("ASSIGNMENT_TTL_SEC", cfg.Scheduler.Queue.AssignmentTTLSec)
@@ -119,11 +87,6 @@ func main() {
 	}
 
 	// 1. 读取配置
-	etcdEndpoints := os.Getenv("ETCD_ENDPOINTS")
-	if etcdEndpoints == "" {
-		etcdEndpoints = "localhost:2379"
-	}
-
 	redisURL := os.Getenv("REDIS_URL")
 	if redisURL == "" {
 		redisURL = "localhost:6379"
@@ -148,17 +111,16 @@ func main() {
 		cancel()
 	}()
 
-	// 3. 初始化 Etcd 服务发现
-	endpoints := strings.Split(etcdEndpoints, ",")
-	discovery, err := scheduler.NewEtcdDiscovery(endpoints)
+	// 3. 初始化工作节点发现（无 etcd 依赖）
+	discovery, err := scheduler.NewWorkerDiscovery()
 	if err != nil {
-		slog.Error("连接 Etcd 失败", "error", err)
+		slog.Error("初始化工作节点发现失败", "error", err)
 		os.Exit(1)
 	}
 	defer discovery.Close()
-	slog.Info("已连接 Etcd")
+	slog.Info("工作节点发现已就绪", "worker_count", discovery.GetWorkerCount())
 
-	// 启动 Worker 监听
+	// 保留调用路径，当前实现会等待 ctx 退出。
 	go discovery.WatchWorkers(ctx)
 
 	// 4. 初始化 Redis 客户端
@@ -188,7 +150,7 @@ func main() {
 
 	// 6. 启动监控（探针与指标）
 
-	// 6.1 启动指标轮询（Redis/Etcd 状态）
+	// 6.1 启动指标轮询（Redis/Worker 状态）
 	go scheduler.StartMetricsPoller(ctx, redisClient, discovery)
 
 	// 6.2 暴露 Prometheus 指标端点
@@ -322,7 +284,7 @@ func main() {
 	}
 }
 
-func selectWorker(ctx context.Context, redisClient *repository.RedisClient, discovery *scheduler.EtcdDiscovery, capacity int) (string, string, bool) {
+func selectWorker(ctx context.Context, redisClient *repository.RedisClient, discovery *scheduler.WorkerDiscovery, capacity int) (string, string, bool) {
 	total := discovery.GetWorkerCount()
 	if total == 0 {
 		return "", "", false
diff --git a/src/go/cmd/worker/main.go b/src/go/cmd/worker/main.go
index 4c53a34..a511d02 100644
--- a/src/go/cmd/worker/main.go
+++ b/src/go/cmd/worker/main.go
@@ -15,7 +15,6 @@ import (
 	"strings"
 	"sync"
 	"syscall"
-	"time"
 
 	"github.com/d1guo/deep_oj/internal/appconfig"
 	"github.com/d1guo/deep_oj/internal/repository"
@@ -23,7 +22,6 @@ import (
 	"github.com/d1guo/deep_oj/pkg/observability"
 	pb "github.com/d1guo/deep_oj/pkg/proto"
 	"github.com/redis/go-redis/v9"
-	clientv3 "go.etcd.io/etcd/client/v3"
 	"google.golang.org/grpc"
 	"google.golang.org/grpc/codes"
 	"google.golang.org/grpc/credentials"
@@ -117,7 +115,6 @@ func main() {
 		appconfig.SetEnvIfEmpty("WORKER_ID", wcfg.ID)
 		appconfig.SetEnvIfEmpty("WORKER_ADDR", wcfg.Addr)
 		appconfig.SetEnvIfEmptyInt("WORKER_PORT", wcfg.Port)
-		appconfig.SetEnvIfEmptySlice("ETCD_ENDPOINTS", wcfg.EtcdEndpoints)
 		appconfig.SetEnvIfEmpty("REDIS_URL", wcfg.RedisURL)
 		appconfig.SetEnvIfEmpty("DATABASE_URL", wcfg.DatabaseURL)
 		appconfig.SetEnvIfEmpty("MINIO_ENDPOINT", wcfg.MinIO.Endpoint)
@@ -143,8 +140,6 @@ func main() {
 		appconfig.SetEnvIfEmptyInt("CLEANUP_TIMEOUT_SEC", wcfg.CleanupTimeoutSec)
 		appconfig.SetEnvIfEmptyInt("RESULT_STREAM_MAX_RETRIES", wcfg.ResultStreamMaxRetries)
 		appconfig.SetEnvIfEmptyInt("RESULT_STREAM_BACKOFF_MS", wcfg.ResultStreamBackoffMs)
-		appconfig.SetEnvIfEmptyInt("ETCD_DIAL_TIMEOUT_MS", wcfg.EtcdDialTimeoutMs)
-		appconfig.SetEnvIfEmptyInt("ETCD_LEASE_TTL_SEC", wcfg.EtcdLeaseTTLSec)
 		appconfig.SetEnvIfEmptyBool("ALLOW_HOST_CHECKER", wcfg.AllowHostChecker)
 		appconfig.SetEnvIfEmptyBool("REQUIRE_CGROUPS_V2", wcfg.RequireCgroupsV2)
 		appconfig.SetEnvIfEmpty("GRPC_TLS_CERT", wcfg.GRPCTLS.Cert)
@@ -254,34 +249,7 @@ func main() {
 		}
 	}()
 
-	// 4. Etcd 注册
-	dialMs := getEnvInt("ETCD_DIAL_TIMEOUT_MS", 5000)
-	cli, err := clientv3.New(clientv3.Config{
-		Endpoints:   cfg.EtcdEndpoints,
-		DialTimeout: time.Duration(dialMs) * time.Millisecond,
-	})
-	if err != nil {
-		slog.Error("连接 etcd 失败", "error", err)
-		os.Exit(1)
-	}
-	defer cli.Close()
-
-	leaseTTL := int64(getEnvInt("ETCD_LEASE_TTL_SEC", 10))
-	key := fmt.Sprintf("/deep-oj/workers/%s", cfg.WorkerID)
-	val := cfg.WorkerAddr
-
-	regCtx, cancelReg := context.WithCancel(context.Background())
-	defer cancelReg()
-	leaseID, keepAliveCh, err := registerWorkerWithLease(regCtx, cli, key, val, leaseTTL)
-	if err != nil {
-		slog.Error("首次注册工作节点失败", "error", err)
-		os.Exit(1)
-	}
-	go maintainWorkerRegistration(regCtx, cli, key, val, leaseTTL, leaseID, keepAliveCh)
-
-	slog.Info("工作节点已注册", "key", key, "lease_id", leaseID)
-
-	// 5. 启动服务
+	// 4. 启动服务
 	go func() {
 		slog.Info("gRPC 服务监听中", "addr", lis.Addr())
 		if err := grpcServer.Serve(lis); err != nil {
@@ -296,11 +264,9 @@ func main() {
 	<-quit
 
 	slog.Info("正在关闭...")
-	cancelReg()
 	cancelStream()
 	streamWG.Wait()
 	grpcServer.GracefulStop()
-	_, _ = cli.Delete(context.Background(), key)
 	slog.Info("工作节点已退出")
 }
 
@@ -330,64 +296,3 @@ func loadServerTLS() (grpc.ServerOption, error) {
 	}
 	return grpc.Creds(credentials.NewTLS(tlsConfig)), nil
 }
-
-func registerWorkerWithLease(
-	ctx context.Context,
-	cli *clientv3.Client,
-	key, val string,
-	leaseTTL int64,
-) (clientv3.LeaseID, <-chan *clientv3.LeaseKeepAliveResponse, error) {
-	lease, err := cli.Grant(ctx, leaseTTL)
-	if err != nil {
-		return 0, nil, err
-	}
-	if _, err := cli.Put(ctx, key, val, clientv3.WithLease(lease.ID)); err != nil {
-		return 0, nil, err
-	}
-	ch, err := cli.KeepAlive(ctx, lease.ID)
-	if err != nil {
-		return 0, nil, err
-	}
-	return lease.ID, ch, nil
-}
-
-func maintainWorkerRegistration(
-	ctx context.Context,
-	cli *clientv3.Client,
-	key, val string,
-	leaseTTL int64,
-	leaseID clientv3.LeaseID,
-	ch <-chan *clientv3.LeaseKeepAliveResponse,
-) {
-	currentLeaseID := leaseID
-	currentCh := ch
-
-	for {
-		select {
-		case <-ctx.Done():
-			return
-		case _, ok := <-currentCh:
-			if ok {
-				continue
-			}
-			slog.Warn("Etcd 保活通道已关闭，尝试重新注册", "lease_id", currentLeaseID)
-			for {
-				select {
-				case <-ctx.Done():
-					return
-				default:
-				}
-				newLeaseID, newCh, err := registerWorkerWithLease(ctx, cli, key, val, leaseTTL)
-				if err != nil {
-					slog.Error("工作节点重新注册失败", "error", err)
-					time.Sleep(time.Second)
-					continue
-				}
-				currentLeaseID = newLeaseID
-				currentCh = newCh
-				slog.Info("工作节点已重新注册到 etcd", "lease_id", currentLeaseID)
-				break
-			}
-		}
-	}
-}
diff --git a/src/go/go.mod b/src/go/go.mod
index b55340d..fa0f060 100644
--- a/src/go/go.mod
+++ b/src/go/go.mod
@@ -10,7 +10,6 @@ require (
 	github.com/minio/minio-go/v7 v7.0.98
 	github.com/prometheus/client_golang v1.23.2
 	github.com/redis/go-redis/v9 v9.18.0
-	go.etcd.io/etcd/client/v3 v3.5.10
 	golang.org/x/crypto v0.48.0
 	golang.org/x/oauth2 v0.35.0
 	golang.org/x/sync v0.19.0
@@ -26,8 +25,6 @@ require (
 	github.com/bytedance/sonic v1.9.1 // indirect
 	github.com/cespare/xxhash/v2 v2.3.0 // indirect
 	github.com/chenzhuoyu/base64x v0.0.0-20221115062448-fe3a3abad311 // indirect
-	github.com/coreos/go-semver v0.3.0 // indirect
-	github.com/coreos/go-systemd/v22 v22.3.2 // indirect
 	github.com/dgryski/go-rendezvous v0.0.0-20200823014737-9f7001d12a5f // indirect
 	github.com/dustin/go-humanize v1.0.1 // indirect
 	github.com/gabriel-vasile/mimetype v1.4.2 // indirect
@@ -37,8 +34,6 @@ require (
 	github.com/go-playground/universal-translator v0.18.1 // indirect
 	github.com/go-playground/validator/v10 v10.14.0 // indirect
 	github.com/goccy/go-json v0.10.2 // indirect
-	github.com/gogo/protobuf v1.3.2 // indirect
-	github.com/golang/protobuf v1.5.4 // indirect
 	github.com/jackc/pgpassfile v1.0.0 // indirect
 	github.com/jackc/pgservicefile v0.0.0-20240606120523-5a60cdf6a761 // indirect
 	github.com/jackc/puddle/v2 v2.2.2 // indirect
@@ -63,17 +58,12 @@ require (
 	github.com/tinylib/msgp v1.6.1 // indirect
 	github.com/twitchyliquid64/golang-asm v0.15.1 // indirect
 	github.com/ugorji/go/codec v1.2.11 // indirect
-	go.etcd.io/etcd/api/v3 v3.5.10 // indirect
-	go.etcd.io/etcd/client/pkg/v3 v3.5.10 // indirect
 	go.uber.org/atomic v1.11.0 // indirect
-	go.uber.org/multierr v1.6.0 // indirect
-	go.uber.org/zap v1.17.0 // indirect
 	go.yaml.in/yaml/v2 v2.4.2 // indirect
 	go.yaml.in/yaml/v3 v3.0.4 // indirect
 	golang.org/x/arch v0.3.0 // indirect
 	golang.org/x/net v0.50.0 // indirect
 	golang.org/x/sys v0.41.0 // indirect
 	golang.org/x/text v0.34.0 // indirect
-	google.golang.org/genproto/googleapis/api v0.0.0-20251029180050-ab9386a59fda // indirect
 	google.golang.org/genproto/googleapis/rpc v0.0.0-20260209200024-4cfbd4190f57 // indirect
 )
diff --git a/src/go/go.sum b/src/go/go.sum
index 22e8446..4a7f959 100644
--- a/src/go/go.sum
+++ b/src/go/go.sum
@@ -12,10 +12,6 @@ github.com/cespare/xxhash/v2 v2.3.0/go.mod h1:VGX0DQ3Q6kWi7AoAeZDth3/j3BFtOZR5XL
 github.com/chenzhuoyu/base64x v0.0.0-20211019084208-fb5309c8db06/go.mod h1:DH46F32mSOjUmXrMHnKwZdA8wcEefY7UVqBKYGjpdQY=
 github.com/chenzhuoyu/base64x v0.0.0-20221115062448-fe3a3abad311 h1:qSGYFH7+jGhDF8vLC+iwCD4WpbV1EBDSzWkJODFLams=
 github.com/chenzhuoyu/base64x v0.0.0-20221115062448-fe3a3abad311/go.mod h1:b583jCggY9gE99b6G5LEC39OIiVsWj+R97kbl5odCEk=
-github.com/coreos/go-semver v0.3.0 h1:wkHLiw0WNATZnSG7epLsujiMCgPAc9xhjJ4tgnAxmfM=
-github.com/coreos/go-semver v0.3.0/go.mod h1:nnelYz7RCh+5ahJtPPxZlU+153eP4D4r3EedlOD2RNk=
-github.com/coreos/go-systemd/v22 v22.3.2 h1:D9/bQk5vlXQFZ6Kwuu6zaiXJ9oTPe68++AzAJc1DzSI=
-github.com/coreos/go-systemd/v22 v22.3.2/go.mod h1:Y58oyj3AT4RCenI/lSvhwexgC+NSVTIJ3seZv2GcEnc=
 github.com/creack/pty v1.1.9/go.mod h1:oKZEueFk5CKHvIhNR5MUki03XCEU+Q6VDXinZuGJ33E=
 github.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
 github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=
@@ -46,9 +42,6 @@ github.com/go-playground/validator/v10 v10.14.0 h1:vgvQWe3XCz3gIeFDm/HnTIbj6UGmg
 github.com/go-playground/validator/v10 v10.14.0/go.mod h1:9iXMNT7sEkjXb0I+enO7QXmzG6QCsPWY4zveKFVRSyU=
 github.com/goccy/go-json v0.10.2 h1:CrxCmQqYDkv1z7lO7Wbh2HN93uovUHgrECaO5ZrCXAU=
 github.com/goccy/go-json v0.10.2/go.mod h1:6MelG93GURQebXPDq3khkgXZkazVtN9CRI+MGFi0w8I=
-github.com/godbus/dbus/v5 v5.0.4/go.mod h1:xhWf0FNVPg57R7Z0UbKHbJfkEywrmjJnf7w5xrFpKfA=
-github.com/gogo/protobuf v1.3.2 h1:Ov1cvc58UF3b5XjBnZv7+opcTcQFZebYjWzi34vdm4Q=
-github.com/gogo/protobuf v1.3.2/go.mod h1:P1XiOD3dCwIKUDQYPy72D8LYyHL2YPYrpS2s69NZV8Q=
 github.com/golang-jwt/jwt/v5 v5.3.1 h1:kYf81DTWFe7t+1VvL7eS+jKFVWaUnK9cB1qbwn63YCY=
 github.com/golang-jwt/jwt/v5 v5.3.1/go.mod h1:fxCRLWMO43lRc8nhHWY6LGqRcf+1gQWArsqaEUEa5bE=
 github.com/golang/protobuf v1.5.4 h1:i7eJL8qZTpSEXOPTxNKhASYpMn+8e5Q6AdndVa1dWek=
@@ -68,8 +61,6 @@ github.com/jackc/puddle/v2 v2.2.2 h1:PR8nw+E/1w0GLuRFSmiioY6UooMp6KJv0/61nB7icHo
 github.com/jackc/puddle/v2 v2.2.2/go.mod h1:vriiEXHvEE654aYKXXjOvZM39qJ0q+azkZFrfEOc3H4=
 github.com/json-iterator/go v1.1.12 h1:PV8peI4a0ysnczrg+LtxykD8LfKY9ML6u2jnxaEnrnM=
 github.com/json-iterator/go v1.1.12/go.mod h1:e30LSqwooZae/UwlEbR2852Gd8hjQvJoHmT4TnhNGBo=
-github.com/kisielk/errcheck v1.5.0/go.mod h1:pFxgyoBC7bSaBwPgfKdkLd5X25qrDl4LWUI2bnpBCr8=
-github.com/kisielk/gotool v1.0.0/go.mod h1:XhKaO+MFFWcvkIS/tQcRk01m1F5IRFswLeQ+oQHNcck=
 github.com/klauspost/compress v1.18.2 h1:iiPHWW0YrcFgpBYhsA6D1+fqHssJscY/Tm/y2Uqnapk=
 github.com/klauspost/compress v1.18.2/go.mod h1:R0h/fSBs8DE4ENlcrlib3PsXS61voFxhIs2DeRhCvJ4=
 github.com/klauspost/cpuid/v2 v2.0.1/go.mod h1:FInQzS24/EEf25PyTYn52gqo7WaD8xa0213Md/qVLRg=
@@ -105,8 +96,6 @@ github.com/pelletier/go-toml/v2 v2.0.8 h1:0ctb6s9mE31h0/lhu+J6OPmVeDxJn+kYnJc2jZ
 github.com/pelletier/go-toml/v2 v2.0.8/go.mod h1:vuYfssBdrU2XDZ9bYydBu6t+6a6PYNcZljzZR9VXg+4=
 github.com/philhofer/fwd v1.2.0 h1:e6DnBTl7vGY+Gz322/ASL4Gyp1FspeMvx1RNDoToZuM=
 github.com/philhofer/fwd v1.2.0/go.mod h1:RqIHx9QI14HlwKwm98g9Re5prTQ6LdeRQn+gXJFxsJM=
-github.com/pkg/errors v0.8.1 h1:iURUrRGxPUNPdy5/HRSm+Yj6okJ6UtLINN0Q9M4+h3I=
-github.com/pkg/errors v0.8.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=
 github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=
 github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
 github.com/prometheus/client_golang v1.23.2 h1:Je96obch5RDVy3FDMndoUsjAhG5Edi49h0RJWRi/o0o=
@@ -141,16 +130,8 @@ github.com/twitchyliquid64/golang-asm v0.15.1 h1:SU5vSMR7hnwNxj24w34ZyCi/FmDZTkS
 github.com/twitchyliquid64/golang-asm v0.15.1/go.mod h1:a1lVb/DtPvCB8fslRZhAngC2+aY1QWCk3Cedj/Gdt08=
 github.com/ugorji/go/codec v1.2.11 h1:BMaWp1Bb6fHwEtbplGBGJ498wD+LKlNSl25MjdZY4dU=
 github.com/ugorji/go/codec v1.2.11/go.mod h1:UNopzCgEMSXjBc6AOMqYvWC1ktqTAfzJZUZgYf6w6lg=
-github.com/yuin/goldmark v1.1.27/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=
-github.com/yuin/goldmark v1.2.1/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=
 github.com/zeebo/xxh3 v1.0.2 h1:xZmwmqxHZA8AI603jOQ0tMqmBr9lPeFwGg6d+xy9DC0=
 github.com/zeebo/xxh3 v1.0.2/go.mod h1:5NWz9Sef7zIDm2JHfFlcQvNekmcEl9ekUZQQKCYaDcA=
-go.etcd.io/etcd/api/v3 v3.5.10 h1:szRajuUUbLyppkhs9K6BRtjY37l66XQQmw7oZRANE4k=
-go.etcd.io/etcd/api/v3 v3.5.10/go.mod h1:TidfmT4Uycad3NM/o25fG3J07odo4GBB9hoxaodFCtI=
-go.etcd.io/etcd/client/pkg/v3 v3.5.10 h1:kfYIdQftBnbAq8pUWFXfpuuxFSKzlmM5cSn76JByiT0=
-go.etcd.io/etcd/client/pkg/v3 v3.5.10/go.mod h1:DYivfIviIuQ8+/lCq4vcxuseg2P2XbHygkKwFo9fc8U=
-go.etcd.io/etcd/client/v3 v3.5.10 h1:W9TXNZ+oB3MCd/8UjxHTWK5J9Nquw9fQBLJd5ne5/Ao=
-go.etcd.io/etcd/client/v3 v3.5.10/go.mod h1:RVeBnDz2PUEZqTpgqwAtUd8nAPf5kjyFyND7P1VkOKc=
 go.opentelemetry.io/auto/sdk v1.2.1 h1:jXsnJ4Lmnqd11kwkBV2LgLoFMZKizbCi5fNZ/ipaZ64=
 go.opentelemetry.io/auto/sdk v1.2.1/go.mod h1:KRTj+aOaElaLi+wW1kO/DZRXwkF4C5xPbEe3ZiIhN7Y=
 go.opentelemetry.io/otel v1.38.0 h1:RkfdswUDRimDg0m2Az18RKOsnI8UDzppJAtj01/Ymk8=
@@ -163,15 +144,10 @@ go.opentelemetry.io/otel/sdk/metric v1.38.0 h1:aSH66iL0aZqo//xXzQLYozmWrXxyFkBJ6
 go.opentelemetry.io/otel/sdk/metric v1.38.0/go.mod h1:dg9PBnW9XdQ1Hd6ZnRz689CbtrUp0wMMs9iPcgT9EZA=
 go.opentelemetry.io/otel/trace v1.38.0 h1:Fxk5bKrDZJUH+AMyyIXGcFAPah0oRcT+LuNtJrmcNLE=
 go.opentelemetry.io/otel/trace v1.38.0/go.mod h1:j1P9ivuFsTceSWe1oY+EeW3sc+Pp42sO++GHkg4wwhs=
-go.uber.org/atomic v1.7.0/go.mod h1:fEN4uk6kAWBTFdckzkM89CLk9XfWZrxpCo0nPH17wJc=
 go.uber.org/atomic v1.11.0 h1:ZvwS0R+56ePWxUNi+Atn9dWONBPp/AUETXlHW0DxSjE=
 go.uber.org/atomic v1.11.0/go.mod h1:LUxbIzbOniOlMKjJjyPfpl4v+PKK2cNJn91OQbhoJI0=
 go.uber.org/goleak v1.3.0 h1:2K3zAYmnTNqV73imy9J1T3WC+gmCePx2hEGkimedGto=
 go.uber.org/goleak v1.3.0/go.mod h1:CoHD4mav9JJNrW/WLlf7HGZPjdw8EucARQHekz1X6bE=
-go.uber.org/multierr v1.6.0 h1:y6IPFStTAIT5Ytl7/XYmHvzXQ7S3g/IeZW9hyZ5thw4=
-go.uber.org/multierr v1.6.0/go.mod h1:cdWPpRnG4AhwMwsgIHip0KRBQjJy5kYEpYjJxpXp9iU=
-go.uber.org/zap v1.17.0 h1:MTjgFu6ZLKvY6Pvaqk97GlxNBuMpV4Hy/3P6tRGlI2U=
-go.uber.org/zap v1.17.0/go.mod h1:MXVU+bhUf/A7Xi2HNOnopQOrmycQ5Ih87HtOu4q5SSo=
 go.yaml.in/yaml/v2 v2.4.2 h1:DzmwEr2rDGHl7lsFgAHxmNz/1NlQ7xLIrlN2h5d1eGI=
 go.yaml.in/yaml/v2 v2.4.2/go.mod h1:081UH+NErpNdqlCXm3TtEran0rJZGxAYx9hb/ELlsPU=
 go.yaml.in/yaml/v3 v3.0.4 h1:tfq32ie2Jv2UxXFdLJdh3jXuOzWiL1fo0bu/FbuKpbc=
@@ -179,48 +155,21 @@ go.yaml.in/yaml/v3 v3.0.4/go.mod h1:DhzuOOF2ATzADvBadXxruRBLzYTpT36CKvDb3+aBEFg=
 golang.org/x/arch v0.0.0-20210923205945-b76863e36670/go.mod h1:5om86z9Hs0C8fWVUuoMHwpExlXzs5Tkyp9hOrfG7pp8=
 golang.org/x/arch v0.3.0 h1:02VY4/ZcO/gBOH6PUaoiptASxtXU10jazRCP865E97k=
 golang.org/x/arch v0.3.0/go.mod h1:5om86z9Hs0C8fWVUuoMHwpExlXzs5Tkyp9hOrfG7pp8=
-golang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=
-golang.org/x/crypto v0.0.0-20191011191535-87dc89f01550/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=
-golang.org/x/crypto v0.0.0-20200622213623-75b288015ac9/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=
 golang.org/x/crypto v0.48.0 h1:/VRzVqiRSggnhY7gNRxPauEQ5Drw9haKdM0jqfcCFts=
 golang.org/x/crypto v0.48.0/go.mod h1:r0kV5h3qnFPlQnBSrULhlsRfryS2pmewsg+XfMgkVos=
-golang.org/x/mod v0.2.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=
-golang.org/x/mod v0.3.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=
-golang.org/x/net v0.0.0-20190404232315-eb5bcb51f2a3/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=
-golang.org/x/net v0.0.0-20190620200207-3b0461eec859/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
-golang.org/x/net v0.0.0-20200226121028-0de0cce0169b/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
-golang.org/x/net v0.0.0-20201021035429-f5854403a974/go.mod h1:sp8m0HH+o8qH0wwXwYZr8TS3Oi6o0r6Gce1SSxlDquU=
 golang.org/x/net v0.50.0 h1:ucWh9eiCGyDR3vtzso0WMQinm2Dnt8cFMuQa9K33J60=
 golang.org/x/net v0.50.0/go.mod h1:UgoSli3F/pBgdJBHCTc+tp3gmrU4XswgGRgtnwWTfyM=
 golang.org/x/oauth2 v0.35.0 h1:Mv2mzuHuZuY2+bkyWXIHMfhNdJAdwW3FuWeCPYN5GVQ=
 golang.org/x/oauth2 v0.35.0/go.mod h1:lzm5WQJQwKZ3nwavOZ3IS5Aulzxi68dUSgRHujetwEA=
-golang.org/x/sync v0.0.0-20190423024810-112230192c58/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
-golang.org/x/sync v0.0.0-20190911185100-cd5d95a43a6e/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
-golang.org/x/sync v0.0.0-20201020160332-67f06af15bc9/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
 golang.org/x/sync v0.19.0 h1:vV+1eWNmZ5geRlYjzm2adRgW2/mcpevXNg50YZtPCE4=
 golang.org/x/sync v0.19.0/go.mod h1:9KTHXmSnoGruLpwFjVSX0lNNA75CykiMECbovNTZqGI=
-golang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=
-golang.org/x/sys v0.0.0-20190412213103-97732733099d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
-golang.org/x/sys v0.0.0-20200930185726-fdedc70b468f/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
 golang.org/x/sys v0.6.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
 golang.org/x/sys v0.41.0 h1:Ivj+2Cp/ylzLiEU89QhWblYnOE9zerudt9Ftecq2C6k=
 golang.org/x/sys v0.41.0/go.mod h1:OgkHotnGiDImocRcuBABYBEXf8A9a87e/uXjp9XT3ks=
-golang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=
-golang.org/x/text v0.3.3/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=
 golang.org/x/text v0.34.0 h1:oL/Qq0Kdaqxa1KbNeMKwQq0reLCCaFtqu2eNuSeNHbk=
 golang.org/x/text v0.34.0/go.mod h1:homfLqTYRFyVYemLBFl5GgL/DWEiH5wcsQ5gSh1yziA=
-golang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=
-golang.org/x/tools v0.0.0-20191119224855-298f0cb1881e/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=
-golang.org/x/tools v0.0.0-20200619180055-7c47624df98f/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=
-golang.org/x/tools v0.0.0-20210106214847-113979e3529a/go.mod h1:emZCQorbCU4vsT4fOWvOPXz4eW1wZW4PmDk9uLelYpA=
-golang.org/x/xerrors v0.0.0-20190717185122-a985d3407aa7/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
-golang.org/x/xerrors v0.0.0-20191011141410-1b5146add898/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
-golang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
-golang.org/x/xerrors v0.0.0-20200804184101-5ec99f83aff1/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
 gonum.org/v1/gonum v0.16.0 h1:5+ul4Swaf3ESvrOnidPp4GZbzf0mxVQpDCYUQE7OJfk=
 gonum.org/v1/gonum v0.16.0/go.mod h1:fef3am4MQ93R2HHpKnLk4/Tbh/s0+wqD5nfa6Pnwy4E=
-google.golang.org/genproto/googleapis/api v0.0.0-20251029180050-ab9386a59fda h1:+2XxjfsAu6vqFxwGBRcHiMaDCuZiqXGDUDVWVtrFAnE=
-google.golang.org/genproto/googleapis/api v0.0.0-20251029180050-ab9386a59fda/go.mod h1:fDMmzKV90WSg1NbozdqrE64fkuTv6mlq2zxo9ad+3yo=
 google.golang.org/genproto/googleapis/rpc v0.0.0-20260209200024-4cfbd4190f57 h1:mWPCjDEyshlQYzBpMNHaEof6UX1PmHcaUODUywQ0uac=
 google.golang.org/genproto/googleapis/rpc v0.0.0-20260209200024-4cfbd4190f57/go.mod h1:j9x/tPzZkyxcgEFkiKEEGxfvyumM01BEtsW8xzOahRQ=
 google.golang.org/grpc v1.78.0 h1:K1XZG/yGDJnzMdd/uZHAkVqJE+xIDOcmdSFZkBUicNc=
@@ -230,11 +179,7 @@ google.golang.org/protobuf v1.36.11/go.mod h1:HTf+CrKn2C3g5S8VImy6tdcUvCska2kB7j
 gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
 gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c h1:Hei/4ADfdWqJk1ZMxUNpqntNwaWcugrBjAiHlqqRiVk=
 gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c/go.mod h1:JHkPIbrfpd72SG/EVd6muEfDQjcINNoR0C8j2r3qZ4Q=
-gopkg.in/yaml.v2 v2.2.8/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=
-gopkg.in/yaml.v2 v2.4.0 h1:D8xgwECY7CYvx+Y2n4sBz93Jn9JRvxdiyyo8CTfuKaY=
-gopkg.in/yaml.v2 v2.4.0/go.mod h1:RDklbk79AGWmwhnvt/jBztapEOGDOx6ZbXqjP6csGnQ=
 gopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
-gopkg.in/yaml.v3 v3.0.0-20210107192922-496545a6307b/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
 gopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=
 gopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
 rsc.io/pdf v0.1.1/go.mod h1:n8OzWcQ6Sp37PL01nO98y4iUCRdTGarVfzxY20ICaU4=
diff --git a/src/go/internal/appconfig/config.go b/src/go/internal/appconfig/config.go
index 6bdbb57..45726bd 100644
--- a/src/go/internal/appconfig/config.go
+++ b/src/go/internal/appconfig/config.go
@@ -105,23 +105,21 @@ type ProblemDefaults struct {
 }
 
 type SchedulerConfig struct {
-	EtcdEndpoints     []string          `yaml:"etcd_endpoints"`
-	RedisURL          string            `yaml:"redis_url"`
-	DatabaseURL       string            `yaml:"database_url"`
-	WorkerCapacity    int               `yaml:"worker_capacity"`
-	MaxRetry          int               `yaml:"max_retry"`
-	RetryTTLSec       int               `yaml:"retry_ttl_sec"`
-	SchedulerID       string            `yaml:"scheduler_id"`
-	GRPCTLS           TLSConfig         `yaml:"grpc_tls"`
-	Metrics           Metrics           `yaml:"metrics"`
-	MetricsPort       int               `yaml:"metrics_port"`
-	MetricsPollMs     int               `yaml:"metrics_poll_ms"`
-	EtcdDialTimeoutMs int               `yaml:"etcd_dial_timeout_ms"`
-	Queue             SchedulerQueue    `yaml:"queue"`
-	AckListener       AckListenerConfig `yaml:"ack_listener"`
-	SlowPath          SlowPathConfig    `yaml:"slow_path"`
-	Watchdog          WatchdogConfig    `yaml:"watchdog"`
-	Dispatch          DispatchConfig    `yaml:"dispatch"`
+	RedisURL       string            `yaml:"redis_url"`
+	DatabaseURL    string            `yaml:"database_url"`
+	WorkerCapacity int               `yaml:"worker_capacity"`
+	MaxRetry       int               `yaml:"max_retry"`
+	RetryTTLSec    int               `yaml:"retry_ttl_sec"`
+	SchedulerID    string            `yaml:"scheduler_id"`
+	GRPCTLS        TLSConfig         `yaml:"grpc_tls"`
+	Metrics        Metrics           `yaml:"metrics"`
+	MetricsPort    int               `yaml:"metrics_port"`
+	MetricsPollMs  int               `yaml:"metrics_poll_ms"`
+	Queue          SchedulerQueue    `yaml:"queue"`
+	AckListener    AckListenerConfig `yaml:"ack_listener"`
+	SlowPath       SlowPathConfig    `yaml:"slow_path"`
+	Watchdog       WatchdogConfig    `yaml:"watchdog"`
+	Dispatch       DispatchConfig    `yaml:"dispatch"`
 }
 
 type SchedulerQueue struct {
@@ -162,9 +160,6 @@ type WorkerConfig struct {
 	ID                     string             `yaml:"id"`
 	Addr                   string             `yaml:"addr"`
 	Port                   int                `yaml:"port"`
-	EtcdEndpoints          []string           `yaml:"etcd_endpoints"`
-	EtcdDialTimeoutMs      int                `yaml:"etcd_dial_timeout_ms"`
-	EtcdLeaseTTLSec        int                `yaml:"etcd_lease_ttl_sec"`
 	RedisURL               string             `yaml:"redis_url"`
 	DatabaseURL            string             `yaml:"database_url"`
 	Stream                 WorkerStreamConfig `yaml:"stream"`
diff --git a/src/go/internal/scheduler/discovery.go b/src/go/internal/scheduler/discovery.go
index cea3854..e8493c4 100644
--- a/src/go/internal/scheduler/discovery.go
+++ b/src/go/internal/scheduler/discovery.go
@@ -2,125 +2,150 @@ package scheduler
 
 import (
 	"context"
+	"fmt"
 	"log/slog"
-	"sync"
+	"net"
+	"net/url"
+	"os"
+	"strings"
 	"sync/atomic"
 	"time"
-
-	clientv3 "go.etcd.io/etcd/client/v3"
 )
 
-const (
-	// WorkerPrefix Etcd 中 Worker 注册的前缀
-	WorkerPrefix = "/deep-oj/workers/"
-)
+type workerEndpoint struct {
+	id   string
+	addr string
+}
 
-// EtcdDiscovery Etcd 服务发现
-type EtcdDiscovery struct {
-	client  *clientv3.Client
-	workers sync.Map // map[workerID]workerAddress
-	rrIndex uint64   // Round-Robin 计数器
+// WorkerDiscovery 提供无 etcd 的工作节点发现。
+// 支持两种环境变量：
+// - WORKER_ADDR=host:port
+// - WORKER_ADDRS=id1=host1:port1,id2=host2:port2 或 host1:port1,host2:port2
+type WorkerDiscovery struct {
+	workers      []workerEndpoint
+	rrIndex      uint64
+	probeTimeout time.Duration
 }
 
-// NewEtcdDiscovery 创建 Etcd 服务发现实例
-func NewEtcdDiscovery(endpoints []string) (*EtcdDiscovery, error) {
-	dialMs := getEnvInt("ETCD_DIAL_TIMEOUT_MS", 5000)
-	client, err := clientv3.New(clientv3.Config{
-		Endpoints:   endpoints,
-		DialTimeout: time.Duration(dialMs) * time.Millisecond,
-	})
-	if err != nil {
-		return nil, err
+func NewWorkerDiscovery() (*WorkerDiscovery, error) {
+	rawWorkers := strings.TrimSpace(os.Getenv("WORKER_ADDRS"))
+	if rawWorkers == "" {
+		rawWorkers = strings.TrimSpace(os.Getenv("WORKER_ADDR"))
 	}
-
-	return &EtcdDiscovery{
-		client: client,
+	workers := parseWorkerEndpoints(rawWorkers)
+	if len(workers) == 0 {
+		slog.Warn("未配置可用工作节点，调度器将等待 worker 上线", "env", "WORKER_ADDR/WORKER_ADDRS")
+	}
+	probeMs := getEnvInt("WORKER_PROBE_TIMEOUT_MS", 1000)
+	if probeMs <= 0 {
+		probeMs = 1000
+	}
+	return &WorkerDiscovery{
+		workers:      workers,
+		probeTimeout: time.Duration(probeMs) * time.Millisecond,
 	}, nil
 }
 
-// Close 关闭 Etcd 连接
-func (d *EtcdDiscovery) Close() error {
-	return d.client.Close()
-}
-
-// WatchWorkers 监听 Worker 注册/注销事件
-func (d *EtcdDiscovery) WatchWorkers(ctx context.Context) {
-	slog.Info("正在监听工作节点", "prefix", WorkerPrefix)
-
-	// 1. 首先获取现有的 Workers
-	resp, err := d.client.Get(ctx, WorkerPrefix, clientv3.WithPrefix())
-	if err != nil {
-		slog.Error("获取初始工作节点失败", "error", err)
-	} else {
-		for _, kv := range resp.Kvs {
-			workerID := string(kv.Key)[len(WorkerPrefix):]
-			workerAddr := string(kv.Value)
-			d.workers.Store(workerID, workerAddr)
-			slog.Info("发现工作节点", "worker_id", workerID, "addr", workerAddr)
-		}
+func parseWorkerEndpoints(raw string) []workerEndpoint {
+	if strings.TrimSpace(raw) == "" {
+		return nil
 	}
-
-	// 2. 开始监听变化
-	watchChan := d.client.Watch(ctx, WorkerPrefix, clientv3.WithPrefix())
-
-	for resp := range watchChan {
-		for _, ev := range resp.Events {
-			workerID := string(ev.Kv.Key)[len(WorkerPrefix):]
-
-			switch ev.Type {
-			case clientv3.EventTypePut:
-				// Worker 注册或更新
-				workerAddr := string(ev.Kv.Value)
-				d.workers.Store(workerID, workerAddr)
-				slog.Info("工作节点已注册", "worker_id", workerID, "addr", workerAddr)
-
-			case clientv3.EventTypeDelete:
-				// Worker 注销 (Lease 过期或主动注销)
-				d.workers.Delete(workerID)
-				slog.Info("工作节点已注销", "worker_id", workerID)
-			}
+	parts := strings.Split(raw, ",")
+	workers := make([]workerEndpoint, 0, len(parts))
+	seenIDs := make(map[string]struct{}, len(parts))
+	for idx, part := range parts {
+		part = strings.TrimSpace(part)
+		if part == "" {
+			continue
 		}
+		id := ""
+		addr := part
+		if strings.Contains(part, "=") {
+			pair := strings.SplitN(part, "=", 2)
+			id = strings.TrimSpace(pair[0])
+			addr = strings.TrimSpace(pair[1])
+		}
+		if addr == "" {
+			continue
+		}
+		if id == "" {
+			id = fmt.Sprintf("worker-%d", idx+1)
+		}
+		if _, exists := seenIDs[id]; exists {
+			id = fmt.Sprintf("%s-%d", id, idx+1)
+		}
+		seenIDs[id] = struct{}{}
+		workers = append(workers, workerEndpoint{
+			id:   id,
+			addr: addr,
+		})
 	}
+	return workers
 }
 
-// GetNextWorker 使用 Round-Robin 获取下一个 Worker
-func (d *EtcdDiscovery) GetNextWorker() (string, string, bool) {
-	// 收集所有 Worker 信息
-	type workerInfo struct {
-		id   string
-		addr string
-	}
-	var workers []workerInfo
-	d.workers.Range(func(key, value interface{}) bool {
-		workers = append(workers, workerInfo{
-			id:   key.(string),
-			addr: value.(string),
-		})
-		return true
-	})
+// Close 为接口兼容保留，当前无外部连接需要关闭。
+func (d *WorkerDiscovery) Close() error {
+	return nil
+}
 
-	if len(workers) == 0 {
+// WatchWorkers 在无 etcd 模式下仅阻塞等待退出信号，避免调用方改动。
+func (d *WorkerDiscovery) WatchWorkers(ctx context.Context) {
+	<-ctx.Done()
+}
+
+// GetNextWorker 使用 Round-Robin 获取下一个 worker。
+func (d *WorkerDiscovery) GetNextWorker() (string, string, bool) {
+	if len(d.workers) == 0 {
 		return "", "", false
 	}
-
-	// 原子递增计数器，取模得到索引
 	idx := atomic.AddUint64(&d.rrIndex, 1)
-	w := workers[idx%uint64(len(workers))]
+	w := d.workers[(idx-1)%uint64(len(d.workers))]
 	return w.id, w.addr, true
 }
 
-// GetWorkerCount 获取当前 Worker 数量
-func (d *EtcdDiscovery) GetWorkerCount() int {
-	count := 0
-	d.workers.Range(func(key, value interface{}) bool {
-		count++
-		return true
-	})
-	return count
+func (d *WorkerDiscovery) GetWorkerCount() int {
+	return len(d.workers)
+}
+
+// IsWorkerActive 通过 TCP 探活判断 worker 是否在线。
+func (d *WorkerDiscovery) IsWorkerActive(workerID string) bool {
+	for _, w := range d.workers {
+		if w.id == workerID {
+			return probeTCPAddress(w.addr, d.probeTimeout)
+		}
+	}
+	return false
 }
 
-// IsWorkerActive 检查 Worker 是否在线
-func (d *EtcdDiscovery) IsWorkerActive(workerID string) bool {
-	_, ok := d.workers.Load(workerID)
-	return ok
+func probeTCPAddress(rawAddr string, timeout time.Duration) bool {
+	addr := normalizeDialAddress(rawAddr)
+	if addr == "" {
+		return false
+	}
+	conn, err := net.DialTimeout("tcp", addr, timeout)
+	if err != nil {
+		return false
+	}
+	if err := conn.Close(); err != nil {
+		slog.Debug("关闭探活连接失败", "addr", addr, "error", err)
+	}
+	return true
+}
+
+func normalizeDialAddress(raw string) string {
+	addr := strings.TrimSpace(raw)
+	if addr == "" {
+		return ""
+	}
+	if strings.Contains(addr, "://") {
+		u, err := url.Parse(addr)
+		if err != nil {
+			return ""
+		}
+		if u.Host != "" {
+			return strings.TrimSpace(u.Host)
+		}
+		return ""
+	}
+	return addr
 }
diff --git a/src/go/internal/scheduler/discovery_test.go b/src/go/internal/scheduler/discovery_test.go
index e77af12..53b12b0 100644
--- a/src/go/internal/scheduler/discovery_test.go
+++ b/src/go/internal/scheduler/discovery_test.go
@@ -5,36 +5,26 @@ import (
 )
 
 func TestGetNextWorker_RoundRobin(t *testing.T) {
-	// Initialize EtcdDiscovery
-	d := &EtcdDiscovery{}
-
-	// Populate sync.Map
-	d.workers.Store("worker-1", "127.0.0.1:5001")
-	d.workers.Store("worker-2", "127.0.0.1:5002")
-
-	// Note: sync.Map Range order is not guaranteed to be consistent or sorted.
-	// However, usually for small set it might be stable enough for simple test,
-	// OR GetNextWorker implementation collects them.
-	// The implementation collects them into a slice. The order of accumulation depends on Range.
-	// sync.Map Range order is random-ish.
-	// So Round-Robin test is tricky if order changes.
-	// But Round-Robin just means "next index".
-	// If the slice content changes order, RR index might preserve "fairness" but not strict "1 then 2".
-	// Let's verify that we get *A* worker, then *Another* worker (if we only have 2 and call 2 times? No).
-	// Actually, if Range order changes, RR is unpredictable.
-	// But let's assume for this test we trigger it enough times to cover.
+	d := &WorkerDiscovery{
+		workers: []workerEndpoint{
+			{id: "worker-1", addr: "127.0.0.1:5001"},
+			{id: "worker-2", addr: "127.0.0.1:5002"},
+		},
+	}
 
-	// Let's just check that it returns valid workers.
-	id1, _, _ := d.GetNextWorker()
-	id2, _, _ := d.GetNextWorker()
+	id1, _, ok1 := d.GetNextWorker()
+	id2, _, ok2 := d.GetNextWorker()
 
-	if id1 == "" || id2 == "" {
-		t.Error("Returned empty worker ID")
+	if !ok1 || !ok2 {
+		t.Fatalf("expected round robin to return available workers")
+	}
+	if id1 != "worker-1" || id2 != "worker-2" {
+		t.Fatalf("unexpected round robin order: got %s then %s", id1, id2)
 	}
 }
 
 func TestGetNextWorker_Empty(t *testing.T) {
-	d := &EtcdDiscovery{}
+	d := &WorkerDiscovery{}
 
 	_, _, ok := d.GetNextWorker()
 	if ok {
diff --git a/src/go/internal/scheduler/metrics.go b/src/go/internal/scheduler/metrics.go
index b996b29..c375e30 100644
--- a/src/go/internal/scheduler/metrics.go
+++ b/src/go/internal/scheduler/metrics.go
@@ -38,7 +38,7 @@ var (
 	schedulerActiveWorkers = prometheus.NewGauge(
 		prometheus.GaugeOpts{
 			Name: "scheduler_active_workers",
-			Help: "Etcd 中活跃的工作节点数",
+			Help: "当前发现到的活跃工作节点数",
 		},
 	)
 
@@ -99,7 +99,7 @@ func SetLegacyLoopsStarted(count int) {
 }
 
 // StartMetricsPoller starts a background loop to update Gauge metrics
-func StartMetricsPoller(ctx context.Context, redis *repository.RedisClient, discovery *EtcdDiscovery) {
+func StartMetricsPoller(ctx context.Context, redis *repository.RedisClient, discovery *WorkerDiscovery) {
 	pollMs := getEnvInt("SCHEDULER_METRICS_POLL_INTERVAL_MS", 1000)
 	ticker := time.NewTicker(time.Duration(pollMs) * time.Millisecond)
 	defer ticker.Stop()
@@ -133,7 +133,7 @@ func updateQueueMetrics(ctx context.Context, redis *repository.RedisClient) {
 	}
 }
 
-func updateWorkerMetrics(ctx context.Context, discovery *EtcdDiscovery) {
+func updateWorkerMetrics(ctx context.Context, discovery *WorkerDiscovery) {
 	count := discovery.GetWorkerCount()
 	schedulerActiveWorkers.Set(float64(count))
 }
diff --git a/src/go/internal/scheduler/watchdog.go b/src/go/internal/scheduler/watchdog.go
index 2827335..6edc09e 100644
--- a/src/go/internal/scheduler/watchdog.go
+++ b/src/go/internal/scheduler/watchdog.go
@@ -23,13 +23,13 @@ const (
 // Watchdog 负责扫描处理中队列，回收僵尸任务。
 type Watchdog struct {
 	redisClient *repository.RedisClient
-	discovery   *EtcdDiscovery
+	discovery   *WorkerDiscovery
 	db          *repository.PostgresDB
 	interval    time.Duration
 }
 
 // NewWatchdog 创建看门狗实例。
-func NewWatchdog(redisClient *repository.RedisClient, discovery *EtcdDiscovery, db *repository.PostgresDB, interval time.Duration) *Watchdog {
+func NewWatchdog(redisClient *repository.RedisClient, discovery *WorkerDiscovery, db *repository.PostgresDB, interval time.Duration) *Watchdog {
 	return &Watchdog{
 		redisClient: redisClient,
 		discovery:   discovery,
diff --git a/src/go/internal/worker/config.go b/src/go/internal/worker/config.go
index 87926d4..4cd2df5 100644
--- a/src/go/internal/worker/config.go
+++ b/src/go/internal/worker/config.go
@@ -15,7 +15,6 @@ import (
 type Config struct {
 	WorkerID              string
 	WorkerAddr            string // gRPC listen addr
-	EtcdEndpoints         []string
 	RedisURL              string
 	DatabaseURL           string
 	MinIOEndpoint         string
@@ -163,20 +162,6 @@ func LoadConfig() *Config {
 	cfg.UnzipMaxFileBytes = getEnvInt64("UNZIP_MAX_FILE_BYTES", 64*1024*1024)
 	cfg.AllowHostChecker = getEnvBool("ALLOW_HOST_CHECKER", false)
 
-	if endpoints := getEnv("ETCD_ENDPOINTS", "localhost:2379"); endpoints != "" {
-		parts := strings.Split(endpoints, ",")
-		for _, p := range parts {
-			p = strings.TrimSpace(p)
-			if p == "" {
-				continue
-			}
-			cfg.EtcdEndpoints = append(cfg.EtcdEndpoints, p)
-		}
-		if len(cfg.EtcdEndpoints) == 0 {
-			cfg.EtcdEndpoints = []string{"localhost:2379"}
-		}
-	}
-
 	return cfg
 }
 
diff --git a/scripts/verify_b1_no_etcd.sh b/scripts/verify_b1_no_etcd.sh
new file mode 100755
index 0000000..9722026
--- /dev/null
+++ b/scripts/verify_b1_no_etcd.sh
@@ -0,0 +1,133 @@
+#!/usr/bin/env bash
+set -euo pipefail
+
+ROOT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
+cd "$ROOT_DIR"
+
+KEEP_TMP="${KEEP_TMP:-0}"
+TMP_DIR="$(mktemp -d)"
+
+cleanup() {
+  if [[ "$KEEP_TMP" == "1" ]]; then
+    echo "KEEP_TMP=1, 保留临时目录: $TMP_DIR" >&2
+    return
+  fi
+  rm -rf "$TMP_DIR"
+}
+trap cleanup EXIT
+
+compose_service_exists() {
+  local service="$1"
+  local svc
+  while IFS= read -r svc; do
+    [[ "$svc" == "$service" ]] && return 0
+  done < <(docker compose config --services 2>/dev/null || true)
+  return 1
+}
+
+emit_diagnostics() {
+  echo >&2
+  echo "========== diagnostics ==========" >&2
+  echo "[docker compose ps]" >&2
+  docker compose ps >&2 || true
+
+  for service in api worker scheduler; do
+    if compose_service_exists "$service"; then
+      echo "[docker compose logs --tail=200 $service]" >&2
+      docker compose logs --tail=200 "$service" >&2 || true
+    fi
+  done
+  echo "=================================" >&2
+}
+
+fatal() {
+  local msg="$1"
+  echo "ERROR: $msg" >&2
+  emit_diagnostics
+  exit 1
+}
+
+require_cmd() {
+  local cmd="$1"
+  if ! command -v "$cmd" >/dev/null 2>&1; then
+    fatal "缺少命令: $cmd"
+  fi
+}
+
+assert_no_etcd_service_defined() {
+  if docker compose config --services 2>/dev/null | grep -qx 'etcd'; then
+    fatal "compose 配置中仍存在 etcd 服务"
+  fi
+}
+
+assert_no_etcd_service_running() {
+  if docker compose ps --services 2>/dev/null | grep -qx 'etcd'; then
+    fatal "compose 运行列表中存在 etcd 服务"
+  fi
+  if docker compose ps 2>/dev/null | grep -Eiq '(^|[[:space:]])(etcd|oj-etcd)([[:space:]]|$)'; then
+    fatal "compose ps 输出中检测到 etcd/oj-etcd"
+  fi
+}
+
+run_script_step() {
+  local label="$1"
+  local script="$2"
+  echo "$label"
+  if [[ ! -f "$script" ]]; then
+    fatal "脚本不存在: $script"
+  fi
+  if ! bash "$script"; then
+    fatal "脚本执行失败: $script"
+  fi
+}
+
+run_optional_g1() {
+  if [[ ! -f scripts/verify_g1_kill_all.sh ]]; then
+    echo "[7/8] 跳过 G1：未找到 scripts/verify_g1_kill_all.sh"
+    return 0
+  fi
+
+  if [[ -f scripts/verify_g1_prereq.sh ]]; then
+    echo "[7/8] 检查 G1 运行前置条件"
+    if ! bash scripts/verify_g1_prereq.sh; then
+      echo "[7/8] 跳过 G1：当前环境不满足前置条件" >&2
+      return 0
+    fi
+  else
+    echo "[7/8] 未找到 verify_g1_prereq.sh，按可运行处理并直接执行 G1"
+  fi
+
+  echo "[7/8] 执行 scripts/verify_g1_kill_all.sh"
+  if ! bash scripts/verify_g1_kill_all.sh; then
+    fatal "脚本执行失败: scripts/verify_g1_kill_all.sh"
+  fi
+}
+
+require_cmd docker
+require_cmd bash
+
+export JWT_SECRET="${JWT_SECRET:-dev_jwt_secret_change_me}"
+export ADMIN_USERS="${ADMIN_USERS:-admin}"
+export REDIS_PASSWORD="${REDIS_PASSWORD:-deepoj_redis_change_me}"
+export POSTGRES_PASSWORD="${POSTGRES_PASSWORD:-deepoj_pg_password_change_me}"
+export MINIO_ROOT_USER="${MINIO_ROOT_USER:-deepoj_minio_user}"
+export MINIO_ROOT_PASSWORD="${MINIO_ROOT_PASSWORD:-deepoj_minio_password_change_me}"
+export WORKER_AUTH_TOKEN="${WORKER_AUTH_TOKEN:-deepoj_worker_token_change_me}"
+
+echo "[1/8] docker compose up -d --build"
+assert_no_etcd_service_defined
+if ! docker compose up -d --build; then
+  fatal "docker compose up 失败"
+fi
+
+echo "[2/8] docker compose ps 断言不存在 etcd"
+assert_no_etcd_service_defined
+assert_no_etcd_service_running
+
+run_script_step "[3/8] 执行 scripts/verify_mvp2_e2e.sh" "scripts/verify_mvp2_e2e.sh"
+run_script_step "[4/8] 执行 scripts/verify_mvp3_crash_recover.sh" "scripts/verify_mvp3_crash_recover.sh"
+run_script_step "[5/8] 执行 scripts/verify_mvp4_observability.sh" "scripts/verify_mvp4_observability.sh"
+run_script_step "[6/8] 执行 scripts/verify_ci.sh" "scripts/verify_ci.sh"
+run_optional_g1
+
+echo "[8/8] 无 etcd 启动验证通过"
diff --git a/scripts/verify_ci.sh b/scripts/verify_ci.sh
new file mode 100755
index 0000000..ed7ebd6
--- /dev/null
+++ b/scripts/verify_ci.sh
@@ -0,0 +1,33 @@
+#!/usr/bin/env bash
+set -euo pipefail
+
+ROOT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
+cd "$ROOT_DIR"
+
+if ! command -v go >/dev/null 2>&1; then
+  echo "ERROR: 未找到 go 命令" >&2
+  exit 2
+fi
+if ! command -v bash >/dev/null 2>&1; then
+  echo "ERROR: 未找到 bash 命令" >&2
+  exit 2
+fi
+
+echo "[1/3] go test ./..."
+(
+  cd src/go
+  go test ./...
+)
+
+echo "[2/3] go vet ./..."
+(
+  cd src/go
+  go vet ./...
+)
+
+echo "[3/3] 脚本语法检查（bash -n）"
+while IFS= read -r script; do
+  bash -n "$script"
+done < <(find scripts -maxdepth 1 -type f -name '*.sh' | sort)
+
+echo "verify_ci: PASS"
